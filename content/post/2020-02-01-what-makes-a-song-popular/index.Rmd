---
title: The Map of Spotify Songs
author: Muhammad Chenariyan Nakhaee
date: '2020-02-01'
slug: []
categories:
  - R
  - XAI
tags:
  - R
  - XAI
  - Visualization
subtitle: ''
summary: ''
authors: []
lastmod: '2020-02-01T17:12:06+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

In the [4th week of the Tidy Tuesday project](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md), a very interesting and fun dataset was proposed to the data science community. The dataset contains information about thousands of songs on Spotify's platform and along with their metadata and audio features. You can download the dataset can using the following piece of code.

[4th week of the Tidy Tuesday project](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md)

```{r message=FALSE, warning=FALSE}
spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')
head(spotify_songs)
```

For this week's tidy Tuesday, I decided to use a rather different approach from my previous submission. Instead of  focusing entirely on the visualization aspect of my submission, I tried to use other tools from tidy model universe for machine learning model development.

Each song has around 12 columns representing audio features. The Github's page for this dataset describes these features as follows: 

| variable         | class  | description                                                  |
| ---------------- | ------ | ------------------------------------------------------------ |
| danceability     | double | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |
| energy           | double | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |
| key              | double | The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. |
| loudness         | double | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db. |
| mode             | double | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. |
| speechiness      | double | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| acousticness     | double | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. |
| instrumentalness | double | Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |
| liveness         | double | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |
| valence          | double | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). |
| tempo            | double | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |
| duration_ms      | double | Duration of song in milliseconds                             |



It would be very helpful to compare songs based on the combination of their audio features. However, It is not possible to plot all these features at the same time on a 2 or 3 dimensional space. 



For this reason, I tried to use unsupervised machine learning for the purpose of visualizing songs on a 2D space by transforming their high-dimensional audio features into a more compressed form.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(workflows)
library(gghighlight)
library(hrbrthemes)
library(ggthemes)
library(lubridate)
library(reticulate)
library(ggrepel)

theme_update(legend.position = 'top',
      legend.text   = element_text(size = 24,color = 'gray75' ),
      legend.key = element_rect(fill = "black", color = "black"),
      plot.title = element_text(family = 'Montserrat', face = "bold", size = 60,hjust = 0.5,vjust = 0.5,color = '#FFE66D',margin = ggplot2::margin(40,0,0,0)),
      plot.subtitle = element_text(
      family = 'Montserrat', size = 30, hjust = 0.5),
      strip.background = element_blank(),
      plot.background = element_rect(fill = "black", color = "black"),
      panel.background = element_rect(fill = "black", color = "black"),
      panel.grid.major.x =element_blank(),
      panel.grid.major.y =element_blank(),
      panel.grid.minor =element_blank(),
      axis.text.x.bottom  = element_blank(),
      axis.ticks.x = element_blank(), 
      axis.ticks.y = element_blank(),
      axis.text.x = element_blank(),
      axis.text.y.left = element_blank()) 
```

## Dimensionality Reduction and UMAP

My initial idea was to use clustering algorithms to cluster songs based on their audio feature and find songs that are similar to each other. Yet, it was difficult to visualize these clusters in a two dimensional space. Of course you can do that by using hierarchal clustering but even then visualizing a few thousands samples (songs) can not be done easily. So, I decided to to use other unsupervised techniques to compress the high-dimensional audio features and transform them into a 2D space. One main purpose of the UMAP algorithm is to give us a compressed representation of the input data for visualization with the least possible information loss. 



There are a number of dimensionality reduction algorithms such as PCA, t-SNE UMAP. PCA is a linear dimensionality reduction  method while both t-SNE and UMAP are non-linear methods. In this post I will use UMAP and t-SNE algorithms.

### Data Preprocessing 
The songs'  track names in this dataset are too long. Just for the purpose of the final visualization, the first thing that I did was to make the track names shorter by using both python and R. 

```{r}
spotify_songs <- spotify_songs %>%  
  dplyr::rowwise() %>% 
  mutate(shorter_names = unlist(str_split(track_name,'-'))[1]) 
```


```{python include=FALSE}
import pandas as pd
df = r.spotify_songs
df['shorter_names'] = df.shorter_names.apply(lambda x: x.split('(')[0])
```

Note that both UMAP and T-SNE compute the distance between samples and this distance should be meaningful and reasonable. It is necessary to normalize input features before implementing a them, otherwise some features might have higher influence than other features on the computation of distance between samples. 

I created a data preprocessing recipe using the `` recipe`` package and I added a normalization step to scale only the audio features.

Since I implement an unsupervised algorithm, there is no need to split the dataset into a training and testing dataset.


```{r include=FALSE}
spotify_songs <- py$df 
```



```{r}
normalized_features <- spotify_songs %>%
  recipe() %>% 
  step_normalize( danceability,
    energy,
    key,
    loudness,
    mode,
    speechiness,
    acousticness,
    instrumentalness,
    liveness,
    valence,
    tempo,
    duration_ms) %>% 
  prep() %>% 
  juice()

head(normalized_features)
```

Both UMAP and T-SNE have several hyper-parameters that can influence the resulting embedding output.
For the sake of simplicity, I did not change the default values for these hyper-parameters.

```{r eval=FALSE, include=TRUE}
library(Rtsne)
tsne_embedding <- normalized_features %>%
  select(c(12:23)) %>%
  Rtsne(check_duplicates = FALSE)

```
```{r include=FALSE}
#embeddings %>%  write_csv('spotify_emb.csv')
embeddings <- read_csv('spotify_emb.csv')
```

Just like t-SNE, UMAP is  a dimensionality reduction algorithm but it is much more computationally efficient and faster that t-SNE. The UMAP algorithm was [originally implemented in Python](https://github.com/lmcinnes/umap). But there are also several libraries in R such as [umapr](https://github.com/ropenscilabs/umapr), [umap](https://cran.r-project.org/web/packages/umap/vignettes/umap.html) and [uwot](https://github.com/jlmelville/uwot) that also provide an implementation of the UMAP algorithm. [``umapr``](https://github.com/ropenscilabs/umapr) and [`umap`](https://cran.r-project.org/web/packages/umap/vignettes/umap.html) use the [``reticulate``](https://cran.r-project.org/web/packages/reticulate/index.html) package and provide a wrapper function around the original ``umap-learn`` python library.  Also,  ``umap`` and ``uwot`` library have their own R implementation of the algorithms and they do not require the python package to be installed first. For my experiment, I used the ``uwot`` library.

```{r eval=FALSE, include=TRUE}
library(uwot)
umap_embedding <- normalized_features %>%
  select(c(12:23)) %>%
  umap(random_state = 123)
```

Now that I had the learned embeddings for both t-SNE and UMAP, it was time to put the side by side.

```{r eval=FALSE, warning=FALSE, include=TRUE}
embeddings <- 
  spotify_songs %>% 
  select(-c(12:22)) %>% 
  bind_cols(tsne_embedding$Y %>% as_tibble()) %>% 
  dplyr::rename(tsne_1 = V1, tsne_2 = V2) %>% 
  bind_cols(umap_embedding$layout %>% as_tibble() ) %>% 
  dplyr::rename(umap_1 = V1, umap_2 = V2) 
```

Even though I managed to transform a high dimensional dataset into a 2D space, still it was very challenging to visualize every song and every artists all at once. So, I just selected a few artists that I have heard their names and I decided to plot thir most popular songs.

```{r}
selected_artists <- c('Queen','Drake','Rihanna','Taylor Swift','Eminem','Snoop Dogg','Katy Perry','The Beatles')

```

```{r eval=FALSE, warning=FALSE, include=TRUE}
embeddings <- %>% embeddings
  mutate(
    selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), ""),
    point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
    track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
    genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
    popular_tracks_selected_artist = if_else(
      track_artist %in% selected_artists & track_popularity > 70,shorter_names, NULL )) %>%
  distinct(track_name, .keep_all = TRUE)

```
```{r}
head(embeddings)
```

Finally, it was time to plot the results of both the t-SNE and UMAP embeddings using ``ggplot`` and [``gghighlight``](https://github.com/yutannihilation/gghighlight) libraries. First I visualize the embeddings for t-SNE.


```{r eval=TRUE, fig.height=35, fig.width=35, warning=FALSE, include=TRUE}
embeddings %>%
  ggplot(aes(x = tsne_1, y = tsne_2 ,color = selected_artist )) +
  geom_point(aes(size = point_size_selected_artist)) +
  gghighlight(selected_artist != "",unhighlighted_params = list(alpha = 0.2, color = '#FFE66D')) +
  scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+
  guides(size = FALSE,
    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',
    point.padding = 2.2,
    box.padding = .5,
    force = 1,
    min.segment.length = 0.1) +
  labs(x = "", y = "" ,
       title = 'The Map of Spotify Songs\n',
       subtitle = 'Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n',
       color = '') 
```


```{r eval=TRUE, fig.height=35, fig.width=35, warning=FALSE, include=TRUE}
embeddings %>%
  ggplot(aes(x = umap_1, y = umap_2 ,color = selected_artist )) +
  geom_point(aes(size = point_size_selected_artist)) +
  gghighlight(selected_artist != "",unhighlighted_params = list(alpha = 0.2, color = '#FFE66D')) +
  scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+
  guides(size = FALSE,
    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',
    point.padding = 2.2,
    box.padding = .5,
    force = 1,
    min.segment.length = 0.1) +
  labs(x = "", y = "" ,
       title = 'The Map of Spotify Songs\n',
       subtitle = 'Using the UMAP algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n',
       color = '') 
```
For the most part both t-SNE and UMAP place songs from the same artists or similar songs close to each other. The UMAP embedding is somehow similar to a real map.  For instance, the upper part looks like the map of the US. In the UMAP representation of the songs, we can see isolated clusters of songs. However, in t-SNE representation, no clear and separate cluster of points can be seen.

```{r eval=FALSE, include=FALSE}
#install.packages("uwot")
library(uwot)
audio_feature =  normalized_features %>%
  select(c(12:23))  
supervised_umap_embedding <- uwot::umap(audio_feature,y = normalized_features$playlist_genre)

```


## Supervised UMAP

UMAP is an unsupervised dimensionality reduction algorithm but we can also feed target labels to UMAP and make it a [supervised algorithm](https://umap-learn.readthedocs.io/en/latest/supervised.html).


```{r eval=FALSE, warning=FALSE, include=TRUE}
supervised_umap_embedding_df <- 
  spotify_songs %>% 
  select(-c(12:22)) %>% 
  bind_cols(supervised_umap_embedding %>% as_tibble()) %>% 
  dplyr::rename(umap_1 = V1, umap_2 = V2) %>% 
  mutate(
    selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), ""),
    point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
    track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
    genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
    popular_tracks_selected_artist = if_else(
      track_artist %in% selected_artists & track_popularity > 70,shorter_names, NULL )) %>%
  distinct(track_name, .keep_all = TRUE)
```

```{r warning=FALSE, include=FALSE}
#supervised_umap_embedding_df %>% write_csv('supervised_umap_embedding_df.csv')

supervised_umap_embedding_df <- read_csv('supervised_umap_embedding_df.csv')
```


```{r eval=TRUE, fig.height=35, fig.width=35, include=TRUE}
supervised_umap_embedding_df %>%
  ggplot(aes(x = umap_1, y = umap_2 ,color = selected_artist )) +
  geom_point(aes(size = point_size_selected_artist)) +
  gghighlight(selected_artist != "",unhighlighted_params = list(alpha = 0.2, color = '#FFE66D')) +
  scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+
  guides(size = FALSE,
    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',
    point.padding = 2.2,
    box.padding = .5,
    force = 1,
    min.segment.length = 0.1) +
  labs(x = "", y = "" ,
       title = 'The Map of Spotify Songs\n',
       subtitle = 'Using the Supervised UMAP algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n',
       color = '') 
```

It is no surprise that the results of the supervised UMAP are much better separated than the unsupervised one. We just gave additional information to UMAP to transform input data.



 