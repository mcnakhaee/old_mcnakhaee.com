<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Muhammad Chenariyan Nakhaee">

  
  
  
    
  
  <meta name="description" content="IntroductionWhat Kinds of explanation does CIU generate?How does CIIU work?A toy example: predicting breastPermutation feature importanceDecision Tree ClassifierRandom Forest ClassifierGradient Boosting ClassifierExplaining a single observationGenerating Textual ExplanationsDrawbacksIntroductionExplainablity is a very hot topics in the machine learning research community these days. over the past few years, many methods have been introduced to just understand how a machine learning model makes prediction.">

  
  <link rel="alternate" hreflang="en-us" href="/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/">

  


  
  
  
  <meta name="theme-color" content="#328cc1">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.a1e68790d54b399632e80a98649b3d14.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.ed4eb580d51d5203cc5d4b573235472f.css">
  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@m_cnakhaee">
  <meta property="twitter:creator" content="@m_cnakhaee">
  
  <meta property="og:site_name" content="Muhammad Chenariyan Nakhaee">
  <meta property="og:url" content="/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/">
  <meta property="og:title" content="Explaining Machine Learning Models Using Contextual Importance and Contextual Utility | Muhammad Chenariyan Nakhaee">
  <meta property="og:description" content="IntroductionWhat Kinds of explanation does CIU generate?How does CIIU work?A toy example: predicting breastPermutation feature importanceDecision Tree ClassifierRandom Forest ClassifierGradient Boosting ClassifierExplaining a single observationGenerating Textual ExplanationsDrawbacksIntroductionExplainablity is a very hot topics in the machine learning research community these days. over the past few years, many methods have been introduced to just understand how a machine learning model makes prediction."><meta property="og:image" content="/img/avatar.jpg">
  <meta property="twitter:image" content="/img/avatar.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-31T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-18T18:46:22&#43;01:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/"
  },
  "headline": "Explaining Machine Learning Models Using Contextual Importance and Contextual Utility",
  
  "datePublished": "2020-05-31T00:00:00Z",
  "dateModified": "2020-01-18T18:46:22+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Muhammad Chenariyan Nakhaee"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Muhammad Chenariyan Nakhaee",
    "logo": {
      "@type": "ImageObject",
      "url": "/img/icon-512.png"
    }
  },
  "description": "Introduction\rWhat Kinds of explanation does CIU generate?\rHow does CIIU work?\rA toy example: predicting breast\rPermutation feature importance\rDecision Tree Classifier\rRandom Forest Classifier\rGradient Boosting Classifier\rExplaining a single observation\r\rGenerating Textual Explanations\rDrawbacks\r\r\r\rIntroduction\rExplainablity is a very hot topics in the machine learning research community these days. over the past few years, many methods have been introduced to just understand how a machine learning model makes prediction."
}
</script>

  

  


  


  
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
  <link rel="stylesheet" href="/css/codefolding.css" />



  <title>Explaining Machine Learning Models Using Contextual Importance and Contextual Utility | Muhammad Chenariyan Nakhaee</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Muhammad Chenariyan Nakhaee</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/project/"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/contact/"><span>Contact</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Explaining Machine Learning Models Using Contextual Importance and Contextual Utility</h1>

  

  
    

<div id="code-folding-buttons" class="dropdown btn-group pull-right">
  <a class="btn btn-light btn-sm dropdown-toggle" href="#" role="button" id="allCodeToggleButton"
     data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
    Code
  </a>
  <div class="dropdown-menu" aria-labelledby="allCodeToggleButton">
    <a id="rmd-show-all-code" class="dropdown-item small" href="#">Show all</a>
    <a id="rmd-hide-all-code" class="dropdown-item small" href="#">Hide all</a>
  </div>
</div>



    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Jan 18, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/xai/">XAI</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#what-kinds-of-explanation-does-ciu-generate">What Kinds of explanation does CIU generate?</a></li>
<li><a href="#how-does-ciiu-work">How does CIIU work?</a></li>
<li><a href="#a-toy-example-predicting-breast">A toy example: predicting breast</a><ul>
<li><a href="#permutation-feature-importance">Permutation feature importance</a></li>
<li><a href="#decision-tree-classifier">Decision Tree Classifier</a></li>
<li><a href="#random-forest-classifier">Random Forest Classifier</a></li>
<li><a href="#gradient-boosting-classifier">Gradient Boosting Classifier</a></li>
<li><a href="#explaining-a-single-observation">Explaining a single observation</a></li>
</ul></li>
<li><a href="#generating-textual-explanations">Generating Textual Explanations</a><ul>
<li><a href="#drawbacks">Drawbacks</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Explainablity is a very hot topics in the machine learning research community these days. over the past few years, many methods have been introduced to just understand how a machine learning model makes prediction. However, explainablity is not an entirely new concept and it was actually started a few decades ago. In this blogpost, I will introduce you to a rather unknown but simple technique which was introduced almost 20 years ago. This technique is called <a href="https://www.researchgate.net/publication/228897070_Explaining_results_of_neural_networks_by_contextual_importance_and_utility">Contextual Importance and Utility (CIU)</a> for explaining ML models and show you how we can explain any types of machine learning.</p>
</div>
<div id="what-kinds-of-explanation-does-ciu-generate" class="section level2">
<h2>What Kinds of explanation does CIU generate?</h2>
<ol style="list-style-type: decimal">
<li>It is a <a href="https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html"><strong><em>model-agnostic</em></strong></a> methods and it can explain the output of any “black-box” machine learning model.</li>
<li>It produces <a href="https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html"><strong><em>local</em></strong></a> explanations which means that the explanations are generated for individual instances (not the whole model) and they just show which features are more important with respect to an individual observation.</li>
<li>It gives us <a href="https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html"><strong><em>post-hoc</em></strong></a> explanations as it is a method that processes the output of a machine learning model after training.</li>
</ol>
<p>Unlike LIME and many other techniques, CIU does not approximate or transforms what a model predicts but rather directly explain predictions. It can also provide a contrastive explanation. For instance, why did the model predict rainy and not cloudy?</p>
</div>
<div id="how-does-ciiu-work" class="section level2">
<h2>How does CIIU work?</h2>
<p>CIT estimates two values that aim to explain the context in which a machine learning model predicts:</p>
<p><strong>Contextual Importance (CI)</strong> is a measure of how much of change in the range an output values can be attributed to one (or several) input variables. CU is based on the notion that a variable which results in a wider ranger of output values would be more important. Formally, CIU is defined as follows:</p>
<p>CI = (Cmax - Cmin)/(absmax - absmin)</p>
<p>Contextual Utility (CU) indicates how favorable the current value of one (or several) input variables is for a high output value. CU is computed using the following formula.</p>
<p>CU = (out - Cmin)/(Cmax - Cmin)</p>
<p>Cmax and Cmin are the highest and lowest values that the output of an ML model <em>can</em> take by changing the input feature(s). Obtaining Cmax and Cmin is computationally and mathematically is not a trivial task. Also, absmax and absmin indicate the absolute range of values that the output has taken. For example, In classification problems, the absolute minimum and maximum range of values are the predicted probabilities of machine learning models and are between 0 and 1.</p>
<p>CIU is implemented both in <a href="https://github.com/TimKam/py-ciu"><strong>python</strong></a> and <a href="https://github.com/KaryFramling/ciu">R</a>. For simplicity, I will use its python implementation (<em>py-ciu library</em>) in this blogpost.</p>
<p>You can install py-ciu using the pip command:</p>
<pre class="bash"><code>pip install py-ciu</code></pre>
</div>
<div id="a-toy-example-predicting-breast" class="section level2">
<h2>A toy example: predicting breast</h2>
<p>I will use breast cancer dataset in scikit-learn to show how we can use CIU. I will train three different machine learning models including a decision tree, a random forest and a gradient boosting algorithm on this dataset and compute CI and CU values for a single instance from test dataset.</p>
<p>First we need to load necessary libraries and modules.</p>
<pre class="python"><code>from ciu import determine_ciu
from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# for reproducability
np.random.seed(123)</code></pre>
<p>Then we split the dataset into a training and test set. We train our machine learning models on the training dataset and evaluate their performance on the test dataset. Note that for explaining ML models, we also use samples from the test dataset and not the training dataset.</p>
<pre class="python"><code>X = pd.DataFrame(load_breast_cancer()[&#39;data&#39;])
y = load_breast_cancer()[&#39;target&#39;]
X.columns = load_breast_cancer()[&#39;feature_names&#39;]</code></pre>
<pre class="python"><code>X_train,X_test, y_train,y_test = train_test_split(X,y,stratify = y)</code></pre>
<pre class="python"><code>def fit_evaluate_model(clf):
  clf = clf.fit(X_train, y_train)
  print(&#39; Accuracy on test dataset {}&#39;.format(clf.score(X_test,y_test)))
  return clf</code></pre>
<div id="permutation-feature-importance" class="section level3">
<h3>Permutation feature importance</h3>
<p>As I mentioned before, CIU only generates local explanations and doesn’t give us an global overview of how a model makes prediction. To gain a better understanding of the global importance of the model let us compute permutation feature importance scores that is implemented in scikit-learn.</p>
<pre class="python"><code>def print_permutation_importance(model):
  imp_features = []
  pi = permutation_importance(model, X_test, y_test,
                            n_repeats=30,
                           random_state=0)
  for i in pi.importances_mean.argsort()[::-1]:
       if pi.importances_mean[i] - 2 * pi.importances_std[i] &gt; 0:
           print(f&quot;{X_test.columns[i]:&lt;8} &quot;
                 f&quot;{pi.importances_mean[i]:.3f} &quot;
                 f&quot; +/- {pi.importances_std[i]:.3f}&quot;)
           imp_features.append(pi.importances_mean[i])
           if len(imp_features) == 0:
                print(&#39;no important features&#39;)</code></pre>
</div>
<div id="decision-tree-classifier" class="section level3">
<h3>Decision Tree Classifier</h3>
<p>Since it is just a toy example, I won’t be very picky about hyper-parameters of my models and leave them to their default values in sklearn.</p>
<pre class="python"><code>dt = DecisionTreeClassifier()
dt_fit = fit_evaluate_model(dt)</code></pre>
<pre><code>##  Accuracy on test dataset 0.9370629370629371</code></pre>
<pre class="python"><code>print_permutation_importance(dt_fit)</code></pre>
<pre><code>## worst perimeter 0.173  +/- 0.019
## worst concave points 0.145  +/- 0.023
## worst concavity 0.135  +/- 0.017
## worst area 0.063  +/- 0.014
## radius error 0.036  +/- 0.014
## worst smoothness 0.018  +/- 0.008
## mean area 0.017  +/- 0.006</code></pre>
</div>
<div id="random-forest-classifier" class="section level3">
<h3>Random Forest Classifier</h3>
<pre class="python"><code>rf = RandomForestClassifier(
)
rf_fit = fit_evaluate_model(rf)</code></pre>
<pre><code>##  Accuracy on test dataset 0.972027972027972</code></pre>
<pre class="python"><code>print_permutation_importance(rf_fit)</code></pre>
<pre><code>## worst texture 0.023  +/- 0.004
## mean texture 0.013  +/- 0.006
## worst smoothness 0.010  +/- 0.004
## mean concavity 0.010  +/- 0.005
## worst fractal dimension 0.006  +/- 0.003</code></pre>
</div>
<div id="gradient-boosting-classifier" class="section level3">
<h3>Gradient Boosting Classifier</h3>
<pre class="python"><code>gb = GradientBoostingClassifier()
gb_fit = fit_evaluate_model(gb)</code></pre>
<pre><code>##  Accuracy on test dataset 0.9790209790209791</code></pre>
<pre class="python"><code>print_permutation_importance(gb_fit)</code></pre>
<pre><code>## worst concave points 0.024  +/- 0.011
## mean concave points 0.021  +/- 0.010</code></pre>
<p>The random forest and gradient boosting classifiers have exactly the same accuracy score, however their most important features are different.</p>
</div>
<div id="explaining-a-single-observation" class="section level3">
<h3>Explaining a single observation</h3>
<p>Now lets explain how each model makes prediction on a single example (observation) from the test dataset.</p>
<pre class="python"><code>example = X_test.iloc[1,:]
example_prediction = gb.predict(example.values.reshape(1, -1))
example_prediction_prob = gb.predict_proba(example.values.reshape(1, -1))
prediction_index = 0 if example_prediction &gt; 0.5 else 1
print(f&#39;Prediction {example_prediction}; Probability: {example_prediction_prob}&#39;)</code></pre>
<pre><code>## Prediction [1]; Probability: [[0.10952357 0.89047643]]</code></pre>
<p>To obtain CIU score, we need to compute minimum and maximum observed value of each feature in the dataset first.</p>
<pre class="python"><code>def min_max_features(X_train):
  min_max = dict()
  for i in range(len(X_train.columns)):
      min_max[X_train.columns[i]] =[X_train.iloc[:,i].min(),X_train.iloc[:,i].max(),False]
  return min_max
  
min_max = min_max_features(X_train)</code></pre>
<pre class="python"><code>def explain_ciu(example,model):
  ciu = determine_ciu(
      example.to_dict(),
      model.predict_proba,
      min_max,
      1000,
      prediction_index,
  )
  return ciu</code></pre>
<pre class="python"><code>dt_ciu = explain_ciu(example,dt_fit)
rf_ciu = explain_ciu(example,rf_fit)
gb_ciu = explain_ciu(example,gb_fit)</code></pre>
</div>
</div>
<div id="generating-textual-explanations" class="section level2">
<h2>Generating Textual Explanations</h2>
<p>We can obtain a textual explanation of CIU which indicates which feature(s) can be important for our test example</p>
<pre class="python"><code>dt_ciu.text_explain()</code></pre>
<pre><code>## [&#39;The feature &quot;mean radius&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean texture&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean perimeter&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean area&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean smoothness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean compactness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean concavity&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean concave points&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean symmetry&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean fractal dimension&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;radius error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;texture error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;perimeter error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;area error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;smoothness error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;compactness error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;concavity error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;concave points error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;symmetry error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;fractal dimension error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst radius&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst texture&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst perimeter&quot;, which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;worst area&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst smoothness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst compactness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst concavity&quot;, which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;worst concave points&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst symmetry&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst fractal dimension&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;]</code></pre>
<pre class="python"><code>rf_ciu.text_explain()</code></pre>
<pre><code>## [&#39;The feature &quot;mean radius&quot;, which is important (CI=32.26%), is very typical for its class (CU=90.0%).&#39;, &#39;The feature &quot;mean texture&quot;, which is important (CI=35.48%), is unlikely for its class (CU=27.27%).&#39;, &#39;The feature &quot;mean perimeter&quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;mean area&quot;, which is not important (CI=16.13%), is unlikely for its class (CU=40.0%).&#39;, &#39;The feature &quot;mean smoothness&quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;mean compactness&quot;, which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).&#39;, &#39;The feature &quot;mean concavity&quot;, which is not important (CI=16.13%), is not typical for its class (CU=20.0%).&#39;, &#39;The feature &quot;mean concave points&quot;, which is not important (CI=19.35%), is not typical for its class (CU=16.67%).&#39;, &#39;The feature &quot;mean symmetry&quot;, which is important (CI=38.71%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;mean fractal dimension&quot;, which is not important (CI=6.45%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;radius error&quot;, which is not important (CI=22.58%), is typical for its class (CU=71.43%).&#39;, &#39;The feature &quot;texture error&quot;, which is not important (CI=22.58%), is very typical for its class (CU=85.71%).&#39;, &#39;The feature &quot;perimeter error&quot;, which is not important (CI=22.58%), is unlikely for its class (CU=42.86%).&#39;, &#39;The feature &quot;area error&quot;, which is important (CI=38.71%), is unlikely for its class (CU=33.33%).&#39;, &#39;The feature &quot;smoothness error&quot;, which is not important (CI=3.23%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;compactness error&quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;concavity error&quot;, which is not important (CI=6.45%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;concave points error&quot;, which is not important (CI=9.68%), is typical for its class (CU=66.67%).&#39;, &#39;The feature &quot;symmetry error&quot;, which is not important (CI=6.45%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;fractal dimension error&quot;, which is not important (CI=19.35%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;worst radius&quot;, which is very important (CI=51.61%), is very typical for its class (CU=87.5%).&#39;, &#39;The feature &quot;worst texture&quot;, which is very important (CI=67.74%), is unlikely for its class (CU=33.33%).&#39;, &#39;The feature &quot;worst perimeter&quot;, which is very important (CI=70.97%), is typical for its class (CU=63.64%).&#39;, &#39;The feature &quot;worst area&quot;, which is very important (CI=61.29%), is typical for its class (CU=57.89%).&#39;, &#39;The feature &quot;worst smoothness&quot;, which is not important (CI=6.45%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;worst compactness&quot;, which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).&#39;, &#39;The feature &quot;worst concavity&quot;, which is very important (CI=64.52%), is very typical for its class (CU=85.0%).&#39;, &#39;The feature &quot;worst concave points&quot;, which is important (CI=38.71%), is not typical for its class (CU=16.67%).&#39;, &#39;The feature &quot;worst symmetry&quot;, which is important (CI=25.81%), is typical for its class (CU=50.0%).&#39;, &#39;The feature &quot;worst fractal dimension&quot;, which is not important (CI=3.23%), is not typical for its class (CU=0.1%).&#39;]</code></pre>
<pre class="python"><code>gb_ciu.text_explain()</code></pre>
<pre><code>## [&#39;The feature &quot;mean radius&quot;, which is not important (CI=16.49%), is not typical for its class (CU=0.65%).&#39;, &#39;The feature &quot;mean texture&quot;, which is highly important (CI=90.14%), is not typical for its class (CU=3.76%).&#39;, &#39;The feature &quot;mean perimeter&quot;, which is not important (CI=2.63%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean area&quot;, which is not important (CI=3.36%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;mean smoothness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean compactness&quot;, which is important (CI=37.26%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;mean concavity&quot;, which is not important (CI=4.0%), is not typical for its class (CU=8.92%).&#39;, &#39;The feature &quot;mean concave points&quot;, which is important (CI=38.25%), is not typical for its class (CU=3.57%).&#39;, &#39;The feature &quot;mean symmetry&quot;, which is not important (CI=8.91%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;mean fractal dimension&quot;, which is not important (CI=1.54%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;radius error&quot;, which is not important (CI=10.53%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;texture error&quot;, which is not important (CI=6.53%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;perimeter error&quot;, which is not important (CI=1.48%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;area error&quot;, which is very important (CI=57.97%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;smoothness error&quot;, which is not important (CI=16.51%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;compactness error&quot;, which is not important (CI=4.39%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;concavity error&quot;, which is not important (CI=4.03%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;concave points error&quot;, which is not important (CI=5.76%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;symmetry error&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;fractal dimension error&quot;, which is not important (CI=21.47%), is not typical for its class (CU=17.33%).&#39;, &#39;The feature &quot;worst radius&quot;, which is not important (CI=1.27%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;worst texture&quot;, which is very important (CI=60.61%), is not typical for its class (CU=13.75%).&#39;, &#39;The feature &quot;worst perimeter&quot;, which is important (CI=41.37%), is not typical for its class (CU=23.17%).&#39;, &#39;The feature &quot;worst area&quot;, which is not important (CI=19.51%), is typical for its class (CU=67.91%).&#39;, &#39;The feature &quot;worst smoothness&quot;, which is not important (CI=18.24%), is unlikely for its class (CU=48.97%).&#39;, &#39;The feature &quot;worst compactness&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst concavity&quot;, which is not important (CI=10.79%), is very typical for its class (CU=100.0%).&#39;, &#39;The feature &quot;worst concave points&quot;, which is important (CI=42.94%), is not typical for its class (CU=4.32%).&#39;, &#39;The feature &quot;worst symmetry&quot;, which is not important (CI=5.86%), is not typical for its class (CU=0.1%).&#39;, &#39;The feature &quot;worst fractal dimension&quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&#39;]</code></pre>
<div id="drawbacks" class="section level3">
<h3>Drawbacks</h3>
<ol style="list-style-type: decimal">
<li><p>In regression problems, the range of possible values for the target variable can be infinite, which somehow does not make sense when we want to compute CIU. The authors said that they have put a limit on the range of values.</p></li>
<li><p>Computing the range of values can be a little bit misleading especially when we have outliers in the dataset.</p></li>
<li><p>It is not clear how we can get a global explanation for the model using CIU.</p></li>
</ol>
</div>
</div>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/xai/">XAI</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
</div>



    
      








  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hub745bfe23d2f8324d7277352a88ce1a8_77169_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Muhammad Chenariyan Nakhaee</a></h5>
      <h6 class="card-subtitle">PhD Student</h6>
      <p class="card-text">I am Muhammad, currently a PhD student at the University of Twente. However, my journey at UTwente will come to an end by March 31</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/m_cnakhaee" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=vbXQurwAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/mcnakhaee" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://instagram.com/mcnakhaee" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/2020-01-18-optimal-rule-lists/">Optimal Rule Lists</a></li>
          
          <li><a href="/post/2020-02-14-tidymodel-for-scikit-learn-users-and-vise-versa/">Tidymodel for Scikit-Learn Users and Vise Versa</a></li>
          
          <li><a href="/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/">Analyzing the 2020 Democratic Presidential Debates - Part 2</a></li>
          
          <li><a href="/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/">What Makes a Song Popular? An Explainable Machine Learning Approach</a></li>
          
          <li><a href="/post/2020-02-01-what-makes-a-song-popular/">The Map of Spotify Songs</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  
    

  
</section>



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.0bf1e3db85cbb232372ed31d6f10dc70.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/otherwidget/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/otherwidget/terms/">Terms</a>
    
  </p>
  
  
  
   
  <script>
  $(document).ready(function () {
    window.initializeCodeFolding("show" === "hide");
  });
  </script>
  <script src="/js/codefolding.js"></script>



  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
