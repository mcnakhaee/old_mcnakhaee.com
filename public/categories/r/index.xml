<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Muhammad Chenariyan Nakhaee</title>
    <link>/categories/r/</link>
      <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 04 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpg</url>
      <title>R</title>
      <link>/categories/r/</link>
    </image>
    
    <item>
      <title>Covid-19 Trends in the Netherlands</title>
      <link>/post/2020-11-04-covid-19-trends-in-the-netherlands/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-11-04-covid-19-trends-in-the-netherlands/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Two weeks ago, I made a visualization that shows how Covid-19 cases spread in the Netherlands from the beginning of March and how grim the situation looks. However, someone pointed out to the fact that the number of tests has increased significantly. It means that my plot may exaggerate the Covid-19 situation in the Netherlands. Unfortunately, I could not find testing data for each Dutch municipality. Instead, I decided to use hospitalization admission and deceased cases to see if we can indeed see a massive spread in the second wave of Covid-19 cases in the Netherlands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(CoronaWatchNL)
library(sf)
library(gganimate)
library(santoku)
library(lubridate)
library(gghighlight)
library(geofacet)
library(foreign)
theme_set(theme_void())
theme_update(
  #plot.background = element_rect(fill = &amp;#39;#FDF6E3&amp;#39;,color = &amp;#39;#FDF6E3&amp;#39;),
  text = element_text(family = &amp;#39;Poppins Light&amp;#39;),
  plot.subtitle = element_text(
    family = &amp;#39;Poppins Light&amp;#39;,
    size = 10,
    margin = margin(b = 10)
  ),
  plot.title = element_text(
    family = &amp;#39;Poppins Light&amp;#39;,
    size = 12,
    margin = margin(t = 10, b = 10)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I created an R package called CoronaWarchNL that allows you to access a wide range of Covid-19 datasets. Iâ€™ll use this package in this post to get Covid-19 cases, hospital admissions, and deaths for Dutch municipalities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;municipalBoundaries &amp;lt;- st_read(
    &amp;quot;https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&amp;amp;service=WFS&amp;amp;version=2.0.0&amp;amp;typeName=cbs_gemeente_2020_gegeneraliseerd&amp;amp;outputFormat=json&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Reading layer `OGRGeoJSON&amp;#39; from data source `https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&amp;amp;service=WFS&amp;amp;version=2.0.0&amp;amp;typeName=cbs_gemeente_2020_gegeneraliseerd&amp;amp;outputFormat=json&amp;#39; using driver `GeoJSON&amp;#39;
## Simple feature collection with 355 features and 5 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 13565.4 ymin: 306846.2 xmax: 278026.1 ymax: 619352.4
## projected CRS:  Amersfoort / RD New&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;daily_cases_per_municpality &amp;lt;- get_daily_cases_per_municipality()
populatuon_per_region &amp;lt;- get_population_per_region()

daily_cases_per_municpality &amp;lt;- daily_cases_per_municpality %&amp;gt;%
  inner_join(populatuon_per_region, by = c(&amp;#39;Municipality_name&amp;#39; = &amp;#39;Regions&amp;#39;)) %&amp;gt;%
  mutate(
    Date_of_publication = as_date(Date_of_publication),
    avg_daily_total_cases = 100000 * as.numeric(Total_reported) / as.numeric(`Bevolking op 1 januari (aantal)`),
    avg_daily_hospital_admissions = 100000 * as.numeric(Hospital_admission) / as.numeric(`Bevolking op 1 januari (aantal)`),
    avg_daily_deceased = 100000 * as.numeric(Deceased) / as.numeric(`Bevolking op 1 januari (aantal)`)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I compute the weekly average number of Covid-19 cases, hospitalizations, and death for each municipality. The following piece of code shows how I did this using R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly_cases &amp;lt;- daily_cases_per_municpality %&amp;gt;%
  mutate(week = round_date(Date_of_publication , unit = &amp;#39;week&amp;#39;))

weekly_cases_per_municpality &amp;lt;- weekly_cases %&amp;gt;%
  group_by(Municipality_name, week) %&amp;gt;%
  summarise(
    avg_weekly_total_cases = mean(avg_daily_total_cases),
    avg_weekly_hospital_admissions = mean(avg_daily_hospital_admissions),
    avg_weekly_deceased = mean(avg_daily_deceased)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    d_avg_weekly_total_cases = chop(avg_weekly_total_cases, c(0, 0, 0.5, 1, 5, 12, 20, 35, 55, 80, 100)),
    d_avg_hospital_admissions = chop(
      avg_weekly_hospital_admissions,
      c(0, 0, 0.5, 1, 2, 3, 5, 7, 9, 10, 15)),
    d_avg_weekly_deceased = chop(avg_weekly_deceased, c(0, 0, 0.1, 0.5, 1, 1.5, 2, 2.5, 3, 5)))

data_weekly &amp;lt;- municipalBoundaries %&amp;gt;%
  right_join(weekly_cases_per_municpality,
             by = c(statnaam = &amp;quot;Municipality_name&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will create an animation that shows how Covid-19 cases spread in the Netherlands and which municipality were and are hit hardest by the pandemic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_animation &amp;lt;- function(data, var_name, pal, title) {
  var_name &amp;lt;- rlang::enquo(var_name)
  data %&amp;gt;%
    #filter(week &amp;gt; &amp;#39;2020-10-01&amp;#39;) %&amp;gt;%
    ggplot() +
    geom_sf(aes(fill = !!var_name), color = &amp;#39;gray95&amp;#39;) +
    scale_fill_manual(values  = pal) +
    coord_sf(datum = NA) +
    labs(
      title = title,
      subtitle = &amp;#39;Date: {current_frame}&amp;#39;,
      fill = &amp;#39;Counts per 100000&amp;#39;,
      caption = &amp;#39;Source: RIVM&amp;#39;
    ) +
    transition_manual(week, cumulative = T) +
    ease_aes(&amp;quot;sine&amp;quot;) +
    enter_fade(alpha = 0.5) +
    exit_fade(alpha = 0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pal_cases &amp;lt;- c(
      &amp;#39;gray95&amp;#39;,
      &amp;#39;#fee440&amp;#39;,
      &amp;#39;#FFBA08&amp;#39;,
      &amp;#39;#FAA307&amp;#39;,
      &amp;#39;#F48C06&amp;#39;,
      &amp;#39;#E85D04&amp;#39;,
      &amp;#39;#DC2F02&amp;#39;,
      &amp;#39;#D00000&amp;#39;,
      &amp;#39;#9D0208&amp;#39;,
      &amp;#39;#6A040F&amp;#39;,
      &amp;#39;#370617&amp;#39;,
      &amp;#39;#03071e&amp;#39;
    )
make_animation(data_weekly,d_avg_weekly_total_cases,pal_cases,&amp;#39;The Average Weekly Number of Covid-19 Cases\nper 100000 Inhabitants in the Netherlands&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;cases.gif&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pal_patients &amp;lt;- c(
      &amp;#39;gray95&amp;#39;,
      &amp;#39;#caf0f8&amp;#39;,
      &amp;#39;#ade8f4&amp;#39;,
      &amp;#39;#90e0ef&amp;#39;,
      &amp;#39;#48cae4&amp;#39;,
      &amp;#39;#00b4d8&amp;#39;,
      &amp;#39;#0096c7&amp;#39;,
      &amp;#39;#0077b6&amp;#39;,
      &amp;#39;#023e8a&amp;#39;,
      &amp;#39;#03045e&amp;#39;,
      &amp;#39;#03071e&amp;#39;
)
make_animation(data_weekly,d_avg_hospital_admissions,pal_patients,&amp;#39;The Average Weekly Number of Hospital Admissions\nper 100000 Inhabitants in the Netherlands&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;admissions.gif&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pal_deceased &amp;lt;- c(
      &amp;#39;gray95&amp;#39;,
      &amp;#39;#fdc5f5&amp;#39;,
     &amp;#39;#e0aaff&amp;#39;,
      &amp;#39;#c77dff&amp;#39;,
      &amp;#39;#9d4edd&amp;#39;,
      &amp;#39;#7b2cbf&amp;#39;,
      &amp;#39;#5a189a&amp;#39;,
      &amp;#39;#3c096c&amp;#39;,
      &amp;#39;#240046&amp;#39;,
      &amp;#39;#10002b&amp;#39;

)
make_animation(data_weekly,d_avg_weekly_deceased,pal_deceased,&amp;#39;The Average Weekly Number of Deceased Patients\nper 100000 Inhabitants in the Netherlands&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;deceased.gif&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trends</title>
      <link>/post/2020-10-30-trends/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-30-trends/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(trendyy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;trendyy&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtrendsR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;gtrendsR&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tidyverse&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages ---------------------------------------------------- tidyverse 1.3.0 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.2     v purrr   0.3.3
## v tibble  3.0.1     v dplyr   1.0.2
## v tidyr   1.0.2     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gghighlight)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;gghighlight&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ob &amp;lt;- trendy(&amp;quot;obama&amp;quot;)
get_interest_country(ob)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 250 x 5
##    location             hits keyword geo   gprop
##    &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1 Burundi                NA obama   world web  
##  2 Liberia                NA obama   world web  
##  3 U.S. Virgin Islands    NA obama   world web  
##  4 Equatorial Guinea      NA obama   world web  
##  5 Cuba                   NA obama   world web  
##  6 Solomon Islands        NA obama   world web  
##  7 Malawi                 NA obama   world web  
##  8 United States         100 obama   world web  
##  9 Congo - Brazzaville    NA obama   world web  
## 10 Cameroon               NA obama   world web  
## # ... with 240 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search_terms &amp;lt;- c(&amp;quot;Rutte&amp;quot;, &amp;quot;Iran&amp;quot;,&amp;quot;Sex&amp;quot;,&amp;quot;Ajax&amp;quot;,&amp;quot;Toilet Paper&amp;quot;)
search_terms_2 &amp;lt;- c(&amp;quot;Rutte&amp;quot;, &amp;quot;Black Lives Matter&amp;quot;,&amp;quot;Barcelona&amp;quot;,&amp;quot;Vaccine&amp;quot;,&amp;quot;WC Papier&amp;quot;)

gtrends(keyword = search_terms,
        geo = &amp;quot;NL&amp;quot;,
        time = &amp;quot;today 12-m&amp;quot;) -&amp;gt; output_results
search_terms &amp;lt;- c(&amp;quot;Rutte&amp;quot;, &amp;quot;Iran&amp;quot;,&amp;quot;Sex&amp;quot;,&amp;quot;Ajax&amp;quot;,&amp;quot;Toilet Paper&amp;quot;)

gtrendsR::countries %&amp;gt;% View()
output_results$interest_over_time %&amp;gt;% as_tibble() %&amp;gt;% 
  filter(date&amp;gt; &amp;#39;2018-11-25&amp;#39;) %&amp;gt;% 
  ggplot(aes(date,y = hits,group =keyword,color = keyword )) +
  geom_smooth(se = FALSE) +
  gghighlight()+
  theme_void()+
  facet_wrap(~keyword)+
  theme(legend.position = &amp;#39;top&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-30-trends/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(trendyy)
library(dplyr)



ob &amp;lt;- trendy(c(&amp;quot;Rutte&amp;quot;, &amp;quot;Iran&amp;quot;,&amp;quot;Sex&amp;quot;,&amp;quot;Ajax&amp;quot;,&amp;quot;WC Papier&amp;quot;),
             from = Sys.Date() - 365,
             to = Sys.Date(),
             geo =&amp;quot;NL&amp;quot; ) %&amp;gt;% 
        get_interest()


ob %&amp;gt;% 
  #filter(date&amp;gt; &amp;#39;2018-11-25&amp;#39;) %&amp;gt;% 
  ggplot(aes(date,y = hits,group =keyword,color = keyword )) +
  geom_smooth(se = FALSE) +
  gghighlight()+
  theme_void()+
  facet_wrap(~keyword)+
  theme(legend.position = &amp;#39;top&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-30-trends/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interactive Graphs with Echarts4r and Tidygraph</title>
      <link>/post/2020-10-20-interactive-graphs-with-echarts4r-and-tidygraph/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-20-interactive-graphs-with-echarts4r-and-tidygraph/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts4r/echarts-en.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts4r/ecStat.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts4r/dataTool.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts4r-binding/echarts4r.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts-gl/echarts-gl.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/echarts-modularity/echarts-graph-modularity.min.js&#34;&gt;&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(echarts4r)
library(tidygraph)
library(tidyverse)
library(ggraph)
library(ggthemes)
theme_set(theme_graph())
theme_update(
  #plot.background = element_rect(fill = &amp;#39;#FDF6E3&amp;#39;),
  panel.background = element_rect(fill = &amp;#39;#FDF6E3&amp;#39;),
  legend.position = &amp;#39;top&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;#39;strategic_rivalries.csv&amp;#39;)
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 197 x 10
##    rivalryno rivalryname   sidea  sideb  styear endyear region type1 type2 type3
##        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1         1 Austria-Fran~ Austr~ France   1494    1918 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  2         2 Austria-Otto~ Austr~ Ottom~   1494    1908 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  3         4 France-Spain  France Spain    1494    1700 Europ~ posi~ spat~ &amp;lt;NA&amp;gt; 
##  4         3 Britain-Fran~ Great~ France   1494    1716 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  5         5 Ottoman Empi~ Ottom~ Spain    1494    1585 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  6         6 Ottoman Empi~ Ottom~ Venice   1494    1717 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  7         7 Portugal-Spa~ Portu~ Spain    1494    1580 Europ~ spat~ posi~ &amp;lt;NA&amp;gt; 
##  8         8 Portugal-Ven~ Portu~ Venice   1494    1580 Europ~ posi~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
##  9         9 Ottoman Empi~ Ottom~ Portu~   1501    1580 Europ~ posi~ &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
## 10        10 Britain-Spai~ Brita~ Spain    1568    1667 Europ~ posi~ spat~ &amp;lt;NA&amp;gt; 
## # ... with 187 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graph_edges_tbl &amp;lt;- data %&amp;gt;%
  filter(endyear&amp;gt;1900) %&amp;gt;% 
rename(from = sidea, to = sideb) 
  

ggraph_df &amp;lt;-  graph_edges_tbl  %&amp;gt;%  
    as_tbl_graph(directed  = FALSE) %&amp;gt;%
      activate(nodes) %&amp;gt;%
      mutate(
        betweenness_degree = centrality_betweenness(),
        centrality_degree = centrality_degree(),
        community = as.factor(group_louvain())
      ) 
      
graph_nodes_tbl &amp;lt;- ggraph_df %&amp;gt;%
    as_tibble()
node_names &amp;lt;- graph_nodes_tbl %&amp;gt;% distinct(name) %&amp;gt;%pull()
node_size &amp;lt;- graph_nodes_tbl%&amp;gt;% pull(centrality_degree)
node_group &amp;lt;- graph_nodes_tbl%&amp;gt;%pull(community) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph_df %&amp;gt;% 
  ggraph(layout = &amp;#39;nicely&amp;#39;) +

  geom_edge_link(width = 1,alpha = 0.7,color = &amp;#39;#e76f51&amp;#39;) +
    geom_node_text(aes(label = name,color =community ),size = 5,repel = TRUE) +
  geom_node_point(aes(size = betweenness_degree,color =community )) +
  #scale_color_tableau() +
  scale_edge_color_manual(values = c(&amp;#39;#118ab2&amp;#39;, &amp;#39;#ef476f&amp;#39;,&amp;#39;#2a9d8f&amp;#39;,&amp;#39;#e76f51&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-20-interactive-graphs-with-echarts4r-and-tidygraph/index.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graph_edges_tbl &amp;lt;- graph_edges_tbl %&amp;gt;% 
  mutate(edge_color = case_when(type1 == &amp;#39;ideological&amp;#39; ~ &amp;#39;#2a9d8f&amp;#39;,
                                type1 == &amp;#39;interventionary&amp;#39; ~ &amp;#39;#e76f51&amp;#39;,
                                type1 == &amp;#39;positional&amp;#39; ~ &amp;#39;#ef476f&amp;#39;,
                                type1 == &amp;#39;spatial&amp;#39; ~ &amp;#39;#118ab2&amp;#39;))
dim(graph_edges_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 162  11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges &amp;lt;- data.frame(
  source = graph_edges_tbl$from,
  target = graph_edges_tbl$to,
  categories = graph_edges_tbl$type1,
  stringsAsFactors = FALSE
)
nodes &amp;lt;- data.frame(
  name = node_names,
  value = node_size,
  size = node_size*3,
  grp = node_group,
  symbol =  &amp;quot;circle&amp;quot;,
  stringsAsFactors = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(&amp;#39;gray70&amp;#39;,length(graph_edges_tbl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot;
##  [9] &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot; &amp;quot;gray70&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#,timeline = TRUE
e_charts() %&amp;gt;% 
  e_graph(layout = &amp;#39;force&amp;#39;,
                    roam = T,
          draggable = T,
          cursor= &amp;#39;pointer&amp;#39;,
          # symbol = paste0(&amp;#39;path://&amp;#39;, figure),  # It works, but all the node share same figure.
          symbolKeepAspect = T,
          focusNodeAdjacency = T,
          # animation  = TRUE,
          #edgeSymbol = c(&amp;#39;circle&amp;#39;, &amp;#39;arrow&amp;#39;),
          #itemStyle = list(color = &amp;#39;black&amp;#39;, borderColor=&amp;#39;black&amp;#39;),
          lineStyle = list(color = &amp;#39;target&amp;#39;,width = 2,curveness= 0.1),
        
          #category = list(symbol = node_group),
         # textBorderColor = &amp;#39;transparent&amp;#39;,
          force.gravity = 2.9, edgeLabel = 1) %&amp;gt;% 
  e_graph_nodes(nodes, name, value, size, grp, symbol,legend = T) %&amp;gt;% 
  e_graph_edges(edges, source, target  ) %&amp;gt;% 
  e_title(&amp;quot;Strategic Rivalry Between Nations and States&amp;quot;,subtext = &amp;#39;Strategic Rivalry Between Nations and States&amp;#39;) %&amp;gt;% 
  e_modularity() %&amp;gt;% 
  e_labels(fontSize = 12) %&amp;gt;% 
  e_toolbox_feature(feature = &amp;quot;saveAsImage&amp;quot;, title = &amp;quot;Save As Image&amp;quot;) %&amp;gt;% 
  e_legend(show = T,type = &amp;quot;plain&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;echarts4r html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;theme&#34;:&#34;default&#34;,&#34;tl&#34;:false,&#34;draw&#34;:true,&#34;renderer&#34;:&#34;canvas&#34;,&#34;events&#34;:[],&#34;buttons&#34;:[],&#34;opts&#34;:{&#34;series&#34;:[{&#34;name&#34;:null,&#34;type&#34;:&#34;graph&#34;,&#34;layout&#34;:&#34;force&#34;,&#34;roam&#34;:true,&#34;draggable&#34;:true,&#34;cursor&#34;:&#34;pointer&#34;,&#34;symbolKeepAspect&#34;:true,&#34;focusNodeAdjacency&#34;:true,&#34;lineStyle&#34;:{&#34;color&#34;:&#34;target&#34;,&#34;width&#34;:2,&#34;curveness&#34;:0.1},&#34;force.gravity&#34;:2.9,&#34;edgeLabel&#34;:1,&#34;categories&#34;:[{&#34;name&#34;:&#34;1&#34;},{&#34;name&#34;:&#34;2&#34;},{&#34;name&#34;:&#34;4&#34;},{&#34;name&#34;:&#34;5&#34;},{&#34;name&#34;:&#34;6&#34;},{&#34;name&#34;:&#34;3&#34;},{&#34;name&#34;:&#34;12&#34;},{&#34;name&#34;:&#34;13&#34;},{&#34;name&#34;:&#34;11&#34;},{&#34;name&#34;:&#34;9&#34;},{&#34;name&#34;:&#34;8&#34;},{&#34;name&#34;:&#34;14&#34;},{&#34;name&#34;:&#34;10&#34;},{&#34;name&#34;:&#34;7&#34;},{&#34;name&#34;:&#34;15&#34;},{&#34;name&#34;:&#34;16&#34;},{&#34;name&#34;:&#34;17&#34;},{&#34;name&#34;:&#34;18&#34;}],&#34;data&#34;:[{&#34;name&#34;:&#34;Austria&#34;,&#34;value&#34;:&#34; 5&#34;,&#34;symbolSize&#34;:&#34;15&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Ottoman Empire&#34;,&#34;value&#34;:&#34; 7&#34;,&#34;symbolSize&#34;:&#34;21&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Britain&#34;,&#34;value&#34;:&#34; 7&#34;,&#34;symbolSize&#34;:&#34;21&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;France&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Afghanistan&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;China&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Iran&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Argentina&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Bolivia&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Colombia&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Greece&#34;,&#34;value&#34;:&#34; 5&#34;,&#34;symbolSize&#34;:&#34;15&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Ecuador&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Chile&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;El Salvador&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Honduras&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Guatemala&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Bulgaria&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Ethiopia&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Italy&#34;,&#34;value&#34;:&#34; 6&#34;,&#34;symbolSize&#34;:&#34;18&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Germany&#34;,&#34;value&#34;:&#34; 7&#34;,&#34;symbolSize&#34;:&#34;21&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Japan&#34;,&#34;value&#34;:&#34; 5&#34;,&#34;symbolSize&#34;:&#34;15&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Albania&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Hungary&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Poland&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Lithuania&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Czechoslovakia&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Costa Rica&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Iraq&#34;,&#34;value&#34;:&#34; 8&#34;,&#34;symbolSize&#34;:&#34;24&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Saudi Arabia&#34;,&#34;value&#34;:&#34; 8&#34;,&#34;symbolSize&#34;:&#34;24&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Egypt&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Russia&#34;,&#34;value&#34;:&#34;11&#34;,&#34;symbolSize&#34;:&#34;33&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Jordan&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Syria&#34;,&#34;value&#34;:&#34; 6&#34;,&#34;symbolSize&#34;:&#34;18&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;India&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Israel&#34;,&#34;value&#34;:&#34; 5&#34;,&#34;symbolSize&#34;:&#34;15&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;North Korea&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;12&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;West Germany&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;13&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Indonesia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;11&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;North Vietnam&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;9&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Thailand&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;9&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Cambodia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;9&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Morocco&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Cuba&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Burkina Faso&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;14&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Ghana&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;10&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Mauritania&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Algeria&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Burundi&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Kenya&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Sudan&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Chad&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Malawi&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Malaysia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;11&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Rhodesia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;South Africa&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Guyana&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;South Yemen&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Tanzania&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Equatorial Guinea&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;15&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Oman&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Libya&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Angola&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Cameroon&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;10&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Mozambique&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Belize&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Bahrain&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;16&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Guinea Bissau&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Armenia&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;17&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Croatia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Kazakhstan&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;18&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Bosnia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Eritrea&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Democratic Republic of Congo&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Djibouti&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Rwanda&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Prussia&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;United States&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Brazil&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Peru&#34;,&#34;value&#34;:&#34; 4&#34;,&#34;symbolSize&#34;:&#34;12&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Venezuela&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Nicaragua&#34;,&#34;value&#34;:&#34; 5&#34;,&#34;symbolSize&#34;:&#34;15&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Romania&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Serbia&#34;,&#34;value&#34;:&#34; 9&#34;,&#34;symbolSize&#34;:&#34;27&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Paraguay&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Panama&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;6&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Yemen&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Turkey&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;1&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Pakistan&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;South Korea&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;12&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Taiwan&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;2&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;East Germany&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;13&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Netherlands&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;11&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;South Vietnam&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;9&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Vietnam&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;9&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Spain&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Mali&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;14&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Somalia&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Ivory Coast&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;10&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Nigeria&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;10&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Togo&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;10&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Kuwait&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;4&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Uganda&#34;,&#34;value&#34;:&#34; 6&#34;,&#34;symbolSize&#34;:&#34;18&#34;,&#34;category&#34;:&#34;3&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Zambia&#34;,&#34;value&#34;:&#34; 3&#34;,&#34;symbolSize&#34;:&#34; 9&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Great Britain&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;5&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Singapore&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;11&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Gabon&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;15&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Zimbabwe&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;7&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Qatar&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;16&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Senegal&#34;,&#34;value&#34;:&#34; 2&#34;,&#34;symbolSize&#34;:&#34; 6&#34;,&#34;category&#34;:&#34;8&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Azerbaijan&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;17&#34;,&#34;symbol&#34;:&#34;circle&#34;},{&#34;name&#34;:&#34;Uzbekistan&#34;,&#34;value&#34;:&#34; 1&#34;,&#34;symbolSize&#34;:&#34; 3&#34;,&#34;category&#34;:&#34;18&#34;,&#34;symbol&#34;:&#34;circle&#34;}],&#34;links&#34;:[{&#34;source&#34;:&#34;Austria&#34;,&#34;target&#34;:&#34;France&#34;},{&#34;source&#34;:&#34;Austria&#34;,&#34;target&#34;:&#34;Ottoman Empire&#34;},{&#34;source&#34;:&#34;Ottoman Empire&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;France&#34;},{&#34;source&#34;:&#34;France&#34;,&#34;target&#34;:&#34;Prussia&#34;},{&#34;source&#34;:&#34;Austria&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Afghanistan&#34;,&#34;target&#34;:&#34;Iran&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Iran&#34;,&#34;target&#34;:&#34;Ottoman Empire&#34;},{&#34;source&#34;:&#34;Argentina&#34;,&#34;target&#34;:&#34;Brazil&#34;},{&#34;source&#34;:&#34;Bolivia&#34;,&#34;target&#34;:&#34;Peru&#34;},{&#34;source&#34;:&#34;Colombia&#34;,&#34;target&#34;:&#34;Peru&#34;},{&#34;source&#34;:&#34;Greece&#34;,&#34;target&#34;:&#34;Ottoman Empire&#34;},{&#34;source&#34;:&#34;Colombia&#34;,&#34;target&#34;:&#34;Ecuador&#34;},{&#34;source&#34;:&#34;Ecuador&#34;,&#34;target&#34;:&#34;Peru&#34;},{&#34;source&#34;:&#34;Colombia&#34;,&#34;target&#34;:&#34;Venezuela&#34;},{&#34;source&#34;:&#34;Chile&#34;,&#34;target&#34;:&#34;Peru&#34;},{&#34;source&#34;:&#34;Bolivia&#34;,&#34;target&#34;:&#34;Chile&#34;},{&#34;source&#34;:&#34;El Salvador&#34;,&#34;target&#34;:&#34;Honduras&#34;},{&#34;source&#34;:&#34;El Salvador&#34;,&#34;target&#34;:&#34;Guatemala&#34;},{&#34;source&#34;:&#34;Argentina&#34;,&#34;target&#34;:&#34;Chile&#34;},{&#34;source&#34;:&#34;Honduras&#34;,&#34;target&#34;:&#34;Nicaragua&#34;},{&#34;source&#34;:&#34;Austria&#34;,&#34;target&#34;:&#34;Italy&#34;},{&#34;source&#34;:&#34;Guatemala&#34;,&#34;target&#34;:&#34;Honduras&#34;},{&#34;source&#34;:&#34;Guatemala&#34;,&#34;target&#34;:&#34;Nicaragua&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Japan&#34;},{&#34;source&#34;:&#34;Bulgaria&#34;,&#34;target&#34;:&#34;Romania&#34;},{&#34;source&#34;:&#34;Bulgaria&#34;,&#34;target&#34;:&#34;Greece&#34;},{&#34;source&#34;:&#34;Bulgaria&#34;,&#34;target&#34;:&#34;Ottoman Empire&#34;},{&#34;source&#34;:&#34;Bulgaria&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Ottoman Empire&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Greece&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;France&#34;,&#34;target&#34;:&#34;Italy&#34;},{&#34;source&#34;:&#34;Ethiopia&#34;,&#34;target&#34;:&#34;Italy&#34;},{&#34;source&#34;:&#34;Italy&#34;,&#34;target&#34;:&#34;Ottoman Empire&#34;},{&#34;source&#34;:&#34;Bolivia&#34;,&#34;target&#34;:&#34;Paraguay&#34;},{&#34;source&#34;:&#34;Germany&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Germany&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Japan&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;Germany&#34;},{&#34;source&#34;:&#34;Japan&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Austria&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Albania&#34;,&#34;target&#34;:&#34;Greece&#34;},{&#34;source&#34;:&#34;Germany&#34;,&#34;target&#34;:&#34;Poland&#34;},{&#34;source&#34;:&#34;Hungary&#34;,&#34;target&#34;:&#34;Romania&#34;},{&#34;source&#34;:&#34;Hungary&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Italy&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Poland&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Lithuania&#34;,&#34;target&#34;:&#34;Poland&#34;},{&#34;source&#34;:&#34;Czechoslovakia&#34;,&#34;target&#34;:&#34;Hungary&#34;},{&#34;source&#34;:&#34;Czechoslovakia&#34;,&#34;target&#34;:&#34;Poland&#34;},{&#34;source&#34;:&#34;Costa Rica&#34;,&#34;target&#34;:&#34;Panama&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;Japan&#34;},{&#34;source&#34;:&#34;Iran&#34;,&#34;target&#34;:&#34;Iraq&#34;},{&#34;source&#34;:&#34;Iraq&#34;,&#34;target&#34;:&#34;Saudi Arabia&#34;},{&#34;source&#34;:&#34;Saudi Arabia&#34;,&#34;target&#34;:&#34;Yemen&#34;},{&#34;source&#34;:&#34;Czechoslovakia&#34;,&#34;target&#34;:&#34;Germany&#34;},{&#34;source&#34;:&#34;Germany&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;Germany&#34;},{&#34;source&#34;:&#34;Britain&#34;,&#34;target&#34;:&#34;Italy&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Iraq&#34;},{&#34;source&#34;:&#34;Russia&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Jordan&#34;},{&#34;source&#34;:&#34;Iraq&#34;,&#34;target&#34;:&#34;Syria&#34;},{&#34;source&#34;:&#34;Jordan&#34;,&#34;target&#34;:&#34;Saudi Arabia&#34;},{&#34;source&#34;:&#34;Jordan&#34;,&#34;target&#34;:&#34;Syria&#34;},{&#34;source&#34;:&#34;Syria&#34;,&#34;target&#34;:&#34;Turkey&#34;},{&#34;source&#34;:&#34;Afghanistan&#34;,&#34;target&#34;:&#34;Pakistan&#34;},{&#34;source&#34;:&#34;India&#34;,&#34;target&#34;:&#34;Pakistan&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;India&#34;},{&#34;source&#34;:&#34;Costa Rica&#34;,&#34;target&#34;:&#34;Nicaragua&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Israel&#34;},{&#34;source&#34;:&#34;Iraq&#34;,&#34;target&#34;:&#34;Israel&#34;},{&#34;source&#34;:&#34;Israel&#34;,&#34;target&#34;:&#34;Jordan&#34;},{&#34;source&#34;:&#34;Israel&#34;,&#34;target&#34;:&#34;Syria&#34;},{&#34;source&#34;:&#34;North Korea&#34;,&#34;target&#34;:&#34;South Korea&#34;},{&#34;source&#34;:&#34;Russia&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Taiwan&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;West Germany&#34;,&#34;target&#34;:&#34;East Germany&#34;},{&#34;source&#34;:&#34;Indonesia&#34;,&#34;target&#34;:&#34;Netherlands&#34;},{&#34;source&#34;:&#34;North Vietnam&#34;,&#34;target&#34;:&#34;South Vietnam&#34;},{&#34;source&#34;:&#34;Thailand&#34;,&#34;target&#34;:&#34;Vietnam&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Iran&#34;},{&#34;source&#34;:&#34;Greece&#34;,&#34;target&#34;:&#34;Turkey&#34;},{&#34;source&#34;:&#34;Cambodia&#34;,&#34;target&#34;:&#34;South Vietnam&#34;},{&#34;source&#34;:&#34;Morocco&#34;,&#34;target&#34;:&#34;Spain&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Saudi Arabia&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Russia&#34;},{&#34;source&#34;:&#34;Iran&#34;,&#34;target&#34;:&#34;Iraq&#34;},{&#34;source&#34;:&#34;Cuba&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Burkina Faso&#34;,&#34;target&#34;:&#34;Mali&#34;},{&#34;source&#34;:&#34;Ethiopia&#34;,&#34;target&#34;:&#34;Somalia&#34;},{&#34;source&#34;:&#34;Ghana&#34;,&#34;target&#34;:&#34;Ivory Coast&#34;},{&#34;source&#34;:&#34;Ghana&#34;,&#34;target&#34;:&#34;Nigeria&#34;},{&#34;source&#34;:&#34;Ghana&#34;,&#34;target&#34;:&#34;Togo&#34;},{&#34;source&#34;:&#34;Mauritania&#34;,&#34;target&#34;:&#34;Morocco&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Syria&#34;},{&#34;source&#34;:&#34;Iraq&#34;,&#34;target&#34;:&#34;Kuwait&#34;},{&#34;source&#34;:&#34;Saudi Arabia&#34;,&#34;target&#34;:&#34;Syria&#34;},{&#34;source&#34;:&#34;Algeria&#34;,&#34;target&#34;:&#34;Morocco&#34;},{&#34;source&#34;:&#34;Burundi&#34;,&#34;target&#34;:&#34;Rwanda&#34;},{&#34;source&#34;:&#34;Indonesia&#34;,&#34;target&#34;:&#34;Malaysia&#34;},{&#34;source&#34;:&#34;Kenya&#34;,&#34;target&#34;:&#34;Somalia&#34;},{&#34;source&#34;:&#34;Sudan&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Chad&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Malawi&#34;,&#34;target&#34;:&#34;Tanzania&#34;},{&#34;source&#34;:&#34;Malawi&#34;,&#34;target&#34;:&#34;Zambia&#34;},{&#34;source&#34;:&#34;Argentina&#34;,&#34;target&#34;:&#34;Great Britain&#34;},{&#34;source&#34;:&#34;Ethiopia&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Malaysia&#34;,&#34;target&#34;:&#34;Singapore&#34;},{&#34;source&#34;:&#34;Rhodesia&#34;,&#34;target&#34;:&#34;Zambia&#34;},{&#34;source&#34;:&#34;South Africa&#34;,&#34;target&#34;:&#34;Zambia&#34;},{&#34;source&#34;:&#34;Chad&#34;,&#34;target&#34;:&#34;Libya&#34;},{&#34;source&#34;:&#34;Guyana&#34;,&#34;target&#34;:&#34;Venezuela&#34;},{&#34;source&#34;:&#34;South Yemen&#34;,&#34;target&#34;:&#34;Yemen&#34;},{&#34;source&#34;:&#34;Iraq&#34;,&#34;target&#34;:&#34;Saudi Arabia&#34;},{&#34;source&#34;:&#34;Tanzania&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Equatorial Guinea&#34;,&#34;target&#34;:&#34;Gabon&#34;},{&#34;source&#34;:&#34;Oman&#34;,&#34;target&#34;:&#34;South Yemen&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Vietnam&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Libya&#34;},{&#34;source&#34;:&#34;Libya&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Angola&#34;,&#34;target&#34;:&#34;Democratic Republic of Congo&#34;},{&#34;source&#34;:&#34;Angola&#34;,&#34;target&#34;:&#34;South Africa&#34;},{&#34;source&#34;:&#34;Cameroon&#34;,&#34;target&#34;:&#34;Nigeria&#34;},{&#34;source&#34;:&#34;Mozambique&#34;,&#34;target&#34;:&#34;Rhodesia&#34;},{&#34;source&#34;:&#34;Cambodia&#34;,&#34;target&#34;:&#34;Vietnam&#34;},{&#34;source&#34;:&#34;Mozambique&#34;,&#34;target&#34;:&#34;South Africa&#34;},{&#34;source&#34;:&#34;Colombia&#34;,&#34;target&#34;:&#34;Nicaragua&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Iran&#34;},{&#34;source&#34;:&#34;Iran&#34;,&#34;target&#34;:&#34;Saudi Arabia&#34;},{&#34;source&#34;:&#34;Iran&#34;,&#34;target&#34;:&#34;Israel&#34;},{&#34;source&#34;:&#34;Honduras&#34;,&#34;target&#34;:&#34;Nicaragua&#34;},{&#34;source&#34;:&#34;South Africa&#34;,&#34;target&#34;:&#34;Zimbabwe&#34;},{&#34;source&#34;:&#34;Belize&#34;,&#34;target&#34;:&#34;Guatemala&#34;},{&#34;source&#34;:&#34;Bahrain&#34;,&#34;target&#34;:&#34;Qatar&#34;},{&#34;source&#34;:&#34;Kenya&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Guinea Bissau&#34;,&#34;target&#34;:&#34;Senegal&#34;},{&#34;source&#34;:&#34;Kenya&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Mauritania&#34;,&#34;target&#34;:&#34;Senegal&#34;},{&#34;source&#34;:&#34;Saudi Arabia&#34;,&#34;target&#34;:&#34;Yemen&#34;},{&#34;source&#34;:&#34;Armenia&#34;,&#34;target&#34;:&#34;Azerbaijan&#34;},{&#34;source&#34;:&#34;Croatia&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Egypt&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Kazakhstan&#34;,&#34;target&#34;:&#34;Uzbekistan&#34;},{&#34;source&#34;:&#34;Bosnia&#34;,&#34;target&#34;:&#34;Croatia&#34;},{&#34;source&#34;:&#34;Bosnia&#34;,&#34;target&#34;:&#34;Serbia&#34;},{&#34;source&#34;:&#34;Eritrea&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Sudan&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Afghanistan&#34;,&#34;target&#34;:&#34;Iran&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;Japan&#34;},{&#34;source&#34;:&#34;China&#34;,&#34;target&#34;:&#34;United States&#34;},{&#34;source&#34;:&#34;Democratic Republic of Congo&#34;,&#34;target&#34;:&#34;Rwanda&#34;},{&#34;source&#34;:&#34;Democratic Republic of Congo&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Djibouti&#34;,&#34;target&#34;:&#34;Eritrea&#34;},{&#34;source&#34;:&#34;Eritrea&#34;,&#34;target&#34;:&#34;Ethiopia&#34;},{&#34;source&#34;:&#34;Rwanda&#34;,&#34;target&#34;:&#34;Uganda&#34;},{&#34;source&#34;:&#34;Chad&#34;,&#34;target&#34;:&#34;Sudan&#34;},{&#34;source&#34;:&#34;Russia&#34;,&#34;target&#34;:&#34;United States&#34;}],&#34;modularity&#34;:{&#34;modularity&#34;:true},&#34;label&#34;:{&#34;show&#34;:true,&#34;position&#34;:&#34;top&#34;,&#34;fontSize&#34;:12}}],&#34;legend&#34;:{&#34;data&#34;:[1,2,4,5,6,3,12,13,11,9,8,14,10,7,15,16,17,18],&#34;show&#34;:true,&#34;type&#34;:&#34;plain&#34;},&#34;title&#34;:[{&#34;text&#34;:&#34;Strategic Rivalry Between Nations and States&#34;,&#34;subtext&#34;:&#34;Strategic Rivalry Between Nations and States&#34;}],&#34;toolbox&#34;:{&#34;feature&#34;:{&#34;saveAsImage&#34;:{&#34;title&#34;:&#34;Save As Image&#34;}}}},&#34;dispose&#34;:true},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  #e_theme(&amp;quot;essos&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How Easy Is It to Understand What Donald Trump Says?</title>
      <link>/post/2020-10-19-readability-of-trump-and-biden-speeches/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-19-readability-of-trump-and-biden-speeches/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Aside from their political differences, Donald Trump and Barack Obama have very contrasting personalities, traits and characters. Obama is known to be a great communicator and an articulate speaker whose speeches are used in English classes to show how one should speak proper English. On the other hand, Trump is not the most eloquent English speaker or US president in history. Every now and then, you can find a clip on the web where Donald Trump is being mocked for the way he speaks or mispronounces words. This is so obvious that even non-native English speakers can notice how Trumpâ€™s speeches are very simple and inarticulate. Of course, this was not a bad thing for Trump at all. Actually, almost every political analyst that you see on the news talks about the fact that a vast majority of Trumpâ€™s fervent supporters are not college-educated Americans. We can attribute this to the fact that he knows how to speak to his audience and his base supporters using their language (Although it is more likely that he cannot speak better English better than this level).&lt;/p&gt;
&lt;p&gt;This post will investigate how difficult it is to understand what each US politicians talked about in the 2020 US Election cycle. I will use several readability metrics that can help us compute text comprehensibility. A wide range of these measures are implemented in the &lt;a href=&#34;https://github.com/shivam5992/textstat&#34;&gt;{&lt;code&gt;textstat&lt;/code&gt;}&lt;/a&gt; python package, and it is super easy to calculate them using this package.&lt;/p&gt;
&lt;p&gt;I compiled a list of US Election-related speeches from rev.com and turned them into an R package called &lt;a href=&#34;https://github.com/mcnakhaee/us2020election&#34;&gt;&lt;code&gt;{us2020election}&lt;/code&gt;&lt;/a&gt;. I use this package as my data source for my analysis. Like some of my other posts, I use Python to perform the analysis and R to visualize my results. Now letâ€™s get started by importing the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(reticulate)
library(ggthemes)
library(us2020election)
library(ggridges)
theme_set(theme_tufte())
theme_update(legend.position = &amp;#39;none&amp;#39;,
          text = element_text(family = &amp;#39;Lobser&amp;#39;),
          plot.title = element_text(margin = margin(t= 10,b= 5),family = &amp;#39;Lobser&amp;#39;),
          plot.subtitle = element_text(margin = margin(b= 10),family = &amp;#39;Lobser&amp;#39;),
          panel.background = element_rect(fill = &amp;#39;#FDF6E3&amp;#39;),
          plot.background = element_rect(fill = &amp;#39;#FDF6E3&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd 
import textstat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several readability measures for English text included in {&lt;code&gt;textstat&lt;/code&gt;}. Calculating these measures is very straightforward and easy. I will explain what each metric represents in more details.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;us_election_speeches = r.us_election_speeches
us_election_speeches[&amp;#39;Flesch_Reading_Ease_formula&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.flesch_reading_ease(x))
us_election_speeches[&amp;#39;gunning_fog&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.gunning_fog(x))
us_election_speeches[&amp;#39;smog_index&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.smog_index(x))
us_election_speeches[&amp;#39;automated_readability_index&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.automated_readability_index(x))
us_election_speeches[&amp;#39;coleman_liau_index&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.coleman_liau_index(x))
us_election_speeches[&amp;#39;linsear_write_formula&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.linsear_write_formula(x))
us_election_speeches[&amp;#39;dale_chall_readability_score&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.dale_chall_readability_score(x))
us_election_speeches[&amp;#39;text_standard&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.text_standard(x))
us_election_speeches[&amp;#39;text_standard_float&amp;#39;] = us_election_speeches[&amp;#39;text&amp;#39;].apply(lambda x: textstat.text_standard(x,float_output  = True))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Letâ€™s look at the resulting dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_election_speeches &amp;lt;- py$us_election_speeches 
us_election_speeches %&amp;gt;% 
glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 286
## Columns: 15
## $ speaker                      &amp;lt;chr&amp;gt; &amp;quot;Barack Obama&amp;quot;, &amp;quot;Mike Pence&amp;quot;, &amp;quot;Kamala ...
## $ title                        &amp;lt;chr&amp;gt; &amp;quot;Barack Obama Campaign Roundtable Even...
## $ text                         &amp;lt;chr&amp;gt; &amp;quot;Barack Obama: (00:01)\nâ€¦ or the â€™40s ...
## $ date                         &amp;lt;chr&amp;gt; &amp;quot;Oct 21, 2020&amp;quot;, &amp;quot;Oct 21, 2020&amp;quot;, &amp;quot;Oct 2...
## $ location                     &amp;lt;chr&amp;gt; &amp;quot;Philadelphia, Pennsylvania&amp;quot;, &amp;quot;Portsmo...
## $ type                         &amp;lt;chr&amp;gt; &amp;quot;Roundtable&amp;quot;, &amp;quot;Campaign Speech&amp;quot;, &amp;quot;Camp...
## $ Flesch_Reading_Ease_formula  &amp;lt;dbl&amp;gt; 78.38, 67.99, 65.35, 85.99, 71.04, 81....
## $ gunning_fog                  &amp;lt;dbl&amp;gt; 8.80, 9.32, 10.80, 5.37, 8.30, 7.31, 5...
## $ smog_index                   &amp;lt;dbl&amp;gt; 9.8, 11.5, 11.6, 8.1, 10.4, 8.7, 8.2, ...
## $ automated_readability_index  &amp;lt;dbl&amp;gt; 9.0, 10.8, 11.5, 5.3, 8.8, 6.9, 5.5, 5...
## $ coleman_liau_index           &amp;lt;dbl&amp;gt; 7.95, 9.11, 8.71, 6.48, 8.12, 6.90, 6....
## $ linsear_write_formula        &amp;lt;dbl&amp;gt; 5.375000, 5.333333, 11.666667, 15.0000...
## $ dale_chall_readability_score &amp;lt;dbl&amp;gt; 5.77, 5.75, 6.27, 5.18, 5.65, 5.66, 5....
## $ text_standard                &amp;lt;chr&amp;gt; &amp;quot;8th and 9th grade&amp;quot;, &amp;quot;8th and 9th grad...
## $ text_standard_float          &amp;lt;dbl&amp;gt; 9, 9, 12, 6, 8, 6, 6, 5, 11, 6, 5, 7, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I am going to visualize the changes in the distribution of speech complexity for each politician. To make things more, I will select a list of politicians that Iâ€™d like to analyze in this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;speakers &amp;lt;- c(&amp;#39;Barack Obama&amp;#39;,&amp;#39;Pete Buttigieg&amp;#39;,&amp;#39;Mike Pence&amp;#39;,&amp;#39;Elizabeth Warren&amp;#39;,&amp;#39;Bernie Sanders&amp;#39;,&amp;#39;Donald Trump&amp;#39;,&amp;#39;Kamala Harris&amp;#39;,&amp;#39;Joe Biden&amp;#39;,&amp;#39;Mike Bloomberg&amp;#39;)
custom_palette &amp;lt;-c(
    &amp;#39;Mike Bloomberg&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Amy Klobuchar&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Joe Biden&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Pete Buttigieg&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Elizabeth Warren&amp;#39; =  &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Barack Obama&amp;#39;  = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Bernie Sanders&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Kamala Harris&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Donald Trump&amp;#39;  = &amp;#39;#E15759&amp;#39; ,
     &amp;#39;Mike Pence&amp;#39; = &amp;#39;#E15759&amp;#39; 
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, I created a function to make ridge plots for each metric easier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot &amp;lt;- function(metric = Flesch_Reading_Ease_formula,subtitle = subtitle) {
  metrics &amp;lt;- rlang::enquo(metric)
  us_election_speeches %&amp;gt;%
    separate_rows(speaker, sep = &amp;#39;,&amp;#39;) %&amp;gt;%
    filter(speaker %in% speakers, type != &amp;#39;Debate&amp;#39;) %&amp;gt;%
    add_count(speaker) %&amp;gt;%
    ggplot() +
    geom_density_ridges(aes(
      x = !!metrics ,
      y = speaker,
      fill = speaker
    )) +
    labs(x = &amp;#39;&amp;#39;, y = &amp;#39;&amp;#39;,title = &amp;quot;How Easy Is It to Comprehend Different US Politicians?&amp;quot;,subtitle = str_wrap(subtitle,width = 100)) +
    scale_fill_manual(values = custom_palette) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, letâ€™s look at several readability measure in more depth.&lt;/p&gt;
&lt;div id=&#34;flesch-reading-ease-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Flesch Reading Ease scores&lt;/h3&gt;
&lt;p&gt;The first readability score that I will look at is based on the Flesch Reading Ease formula. It computes the number of syllables to determine how easy a piece of text is. The maximum value of Flesch Reading Ease is 122, and there is no minimum value for it. Higher Flesch Reading Ease scores indicate that the text (speech) is easier to understand by the audience. In our case, it would show how sophisticated each politician is in terms of language use. You can find more about this metric on &lt;a href=&#34;https://en.wikipedia.org/wiki/Fleschâ€“Kincaid_readability_tests&#34;&gt;Wikipedia&lt;/a&gt;!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(Flesch_Reading_Ease_formula ,
            subtitle = &amp;#39;The Flesch Reading Ease scores measure the complexity of a text document. Higher scores indicate a text is easier to comprehend.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;we can interpret the scores using the following table:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;66%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;School level&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;100.00â€“90.00&lt;/td&gt;
&lt;td&gt;5th grade&lt;/td&gt;
&lt;td&gt;Very easy to read. Easily understood by an average 11-year-old student.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;90.0â€“80.0&lt;/td&gt;
&lt;td&gt;6th grade&lt;/td&gt;
&lt;td&gt;Easy to read. Conversational English for consumers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;80.0â€“70.0&lt;/td&gt;
&lt;td&gt;7th grade&lt;/td&gt;
&lt;td&gt;Fairly easy to read.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;70.0â€“60.0&lt;/td&gt;
&lt;td&gt;8th &amp;amp; 9th grade&lt;/td&gt;
&lt;td&gt;Plain English. Easily understood by 13- to 15-year-old students.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;60.0â€“50.0&lt;/td&gt;
&lt;td&gt;10th to 12th grade&lt;/td&gt;
&lt;td&gt;Fairly difficult to read.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;50.0â€“30.0&lt;/td&gt;
&lt;td&gt;College&lt;/td&gt;
&lt;td&gt;Difficult to read.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30.0â€“10.0&lt;/td&gt;
&lt;td&gt;College graduate&lt;/td&gt;
&lt;td&gt;Very difficult to read. Best understood by university graduates.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;10.0â€“0.0&lt;/td&gt;
&lt;td&gt;Professional&lt;/td&gt;
&lt;td&gt;Extremely difficult to read. Best understood by university graduates.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;gunning-fog-index&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gunning fog index&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gunning_fog_index&#34;&gt;The Gunning fog index&lt;/a&gt; is another metric to measure the complexity of a text document. It shows how many years of education one might need to understand a piece of text. Larger values of the Gunning fog index correspond to more difficult writings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(gunning_fog,subtitle = &amp;#39;The Gunning fog index measure the complexity of a text document. Larger values of the Gunning fog index correspond to more difficult writings.&amp;#39; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-smog-index&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The SMOG index&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/SMOG&#34;&gt;The SMOG index&lt;/a&gt; computes the ratio of polysyllables (words with three or more syllables) in sentences to determine text complexity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(smog_index,subtitle = &amp;#39;The SMOG index measure the complexity of a text document. Larger values of the SMOG index indicate more difficult writings.&amp;#39; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linsear-write-formula&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linsear Write Formula&lt;/h3&gt;
&lt;p&gt;Like previous the metric, &lt;a href=&#34;https://en.wikipedia.org/wiki/Linsear_Write&#34;&gt;the Linsear Write Formula&lt;/a&gt; uses words with three or more syllables to compute text readability. It also relies on the sentence length to measure how difficult reading a text could be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(linsear_write_formula, subtitle = &amp;#39;The Linsear Write Formula measure the complexity of a text document. Larger values indicate more difficult writings.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dale-chall-readability-score&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Daleâ€“Chall_readability_formula&#34;&gt;Dale-Chall Readability Score&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This metric is different from the other metrics that we have talked about. It uses a dictionary of 3000 words that are easy to read and understand for a fourth-grade student. So, Words that are not in this dictionary are considered to be complex. The higher the Dale-Chall Score is, the more difficult it is to read a text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(dale_chall_readability_score,subtitle = &amp;#39;The Dale-Chall Readability Score measure the complexity of a text document. The higher the Dale-Chall Score is, the more difficult it is to read a text.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;960&#34; /&gt;
### A unified readability&lt;/p&gt;
&lt;p&gt;We introduced several readability metrics, but each one of them might give us a slightly different result. There is a way in &lt;code&gt;textstats&lt;/code&gt; to combine all these metrics and have a single readability metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_election_speeches %&amp;gt;%
  filter(speaker %in% speakers) %&amp;gt;%
  mutate(text_standard = str_replace(text_standard,&amp;#39; and &amp;#39;,&amp;#39;-&amp;#39;),
        text_standard = factor(
    text_standard,
    levels = c(
      &amp;#39;4th-5th grade&amp;#39;,
      &amp;#39;5th-6th grade&amp;#39;,
      &amp;#39;6th-7th grade&amp;#39;,
      &amp;#39;7th-8th grade&amp;#39;,
      &amp;#39;8th-9th grade&amp;#39;,
      &amp;#39;9th-10th grade&amp;#39;,
      &amp;#39;10th-11th grade&amp;#39;,
      &amp;#39;11th-12th grade&amp;#39;,
      &amp;#39;12th-13th grade&amp;#39;,
      &amp;#39;14th-15th grade&amp;#39;
    )
  )) %&amp;gt;%
  count(speaker, text_standard) %&amp;gt;%
  mutate(n = n + 1) %&amp;gt;%
  ggplot()  +
  geom_col(aes(x = text_standard , y =  n, fill = speaker)) +
  labs(x = &amp;#39;&amp;#39;, y = &amp;#39;Number of Speeches&amp;#39;, title = &amp;quot;How Easy Is It to Understand &amp;#39;Trump&amp;#39;s Speeches?&amp;quot;,
       subtitle = &amp;#39;Based on several readability tests, the education level that one needs to comprehend the 2020 Election speeches by different US politicians is illustrated in this plot.&amp;#39; ) +
  scale_fill_manual(values = custom_palette) +
  scale_y_log10() +
  facet_wrap(~ speaker, ncol = 1) +
  theme(axis.text  = element_text(size = 13),
        axis.title.y = element_text(size = 15,margin = margin(r = 10,l = 10)),
        plot.title = element_text(size = 20,margin = margin(b = 10,t = 10)),
        plot.subtitle = element_text(size = 14,margin = margin(b = 10)),
        strip.text = element_text(size = 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, we can observe that Trump never gave a speech to an audience with difficulty more than the 7th or 8th grade. We can also convert this readability metric to numbers to visualize and compare it to other metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_plot(text_standard_float,subtitle = &amp;#39;The complexity of a text document were measured based on several readability metrics where larger values indicate more difficult writings.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-19-readability-of-trump-and-biden-speeches/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We can consistently see that Trump speeches are less sophisticated and less complex than the speeches given by the rest of politicians. We can attribute this to his lack of sophistication in terms of language, the fact that he knows how can speak to his audience or both. Also, we can notice that Mike Pence and Barack Obama seem to use a more an advanced language in their speeches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R Packages</title>
      <link>/project/2020-10-19-r-packages/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/project/2020-10-19-r-packages/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;delgosha&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://github.com/mcnakhaee/delgosha&#34;&gt;Delgosha&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tidyverse.org/lifecycle/#experimental&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/lifecycle-experimental-orange.svg&#34; alt=&#34;Lifecycle: experimental&#34; /&gt;&lt;/a&gt;
Delgosha is an opinionated package which aims to provide a collection of
ggplot2 themes for RTL languages (mostly Persian).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mcnakhaee/delgosha/raw/master/README_files/figure-gfm/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;img&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;1https://github.com/mcnakhaee/delgosha/blob/master/README_files/figure-gfm/unnamed-chunk-3-1.pnghttps://github.com/mcnakhaee/delgosha/blob/master/README_files/figure-gfm/unnamed-chunk-3-1.png)[]&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dadegan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;### &lt;a href=&#34;https://github.com/mcnakhaee/dadegan&#34;&gt;Dadegan&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Dadegan is a simple package which contains a handful of useful Persian datasets.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Tuesday Week 37: NLP and Friends</title>
      <link>/post/2020-10-15-tidy-tuesday-week-37-nlp-and-friend/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-15-tidy-tuesday-week-37-nlp-and-friend/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I experience a few difficult and sad moments in my life in the past few months and it was not particularly an easy job for me to cope with these hard moments. However, I found a medicine to fight the sadness that was caused by these painful experience: watching Friends! I have watched Friends many many times but still it can bring smile to my face. Every time I feel being sad, I just start to watch several episodes on Friends, one after another and the laughter does the rest!&lt;/p&gt;
&lt;p&gt;So, when I realized that the dataset we are going to work with in the 37th week of 2020 Tidy Tuesdays project, I was absolutely thrilled and excited to get my hands on this dataset and analyze it. As a big fan of Friends, many ideas popped into my head on how I can make an interesting analysis on this dataset. In this post, I will go through three of them:
1. In Which locations (scenes) Friends was most filmed?
2. How characters in the show interacted with each other?
3. What kinds of actions female and male characters in the show were doing?&lt;/p&gt;
&lt;p&gt;I will addressed these three questions in the rest of this post!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggthemes)
library(tidytuesdayR)
library(rsyntax)
library(spacyr)
library(tidytext)
library(tidygraph)
library(igraph)
library(ggraph)
library(hrbrthemes)
library(DT)
spacy_initialize(model = &amp;quot;en_core_web_lg&amp;quot;)

theme_set(  theme_fivethirtyeight()) 
theme_update(
    text = element_text(family = &amp;#39;Poppins Light&amp;#39;, color = &amp;#39;#774936&amp;#39;),
    plot.title = element_text(margin = margin(t = 10, b = 10)),
    plot.subtitle = element_text(margin = margin(b = 20)),
    axis.text.y = element_blank(),
    plot.background = element_rect(fill = &amp;#39;#FFFAF0&amp;#39;),
    panel.background = element_rect(fill = &amp;#39;#FFFAF0&amp;#39;),
    legend.position = &amp;#39;none&amp;#39;,
    panel.grid.major  = element_line(
      color = &amp;#39;#eddcd2&amp;#39;,
      size = 0.5,
      linetype = &amp;#39;dashed&amp;#39;
    ),
    panel.grid.minor = element_line(
      color = &amp;#39;#eddcd2&amp;#39;,
      size = 0.01,
      linetype = &amp;#39;dashed&amp;#39;
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us load the Friends dataset using the handy &lt;code&gt;tt_load&lt;/code&gt; function from the &lt;code&gt;tidytuesdayR&lt;/code&gt; package!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt &amp;lt;- tt_load(&amp;quot;2020-09-08&amp;quot;)
friends_df &amp;lt;- tt$friends &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns three tables but since the data that I need , I will only be working with the friends transcript dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friends_df %&amp;gt;% 
  head(20) %&amp;gt;% 
  datatable()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friends_df &amp;lt;- tt$friends &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;top-locations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Top Locations&lt;/h3&gt;
&lt;p&gt;Now letâ€™s find out which locations this show was filmed the most. This information is available in the &lt;code&gt;text&lt;/code&gt; column.
In this column,Usually the location where the scene took place comes after the &lt;code&gt;Scene:&lt;/code&gt; string . So, we only need to find and filter rows that have this string. Unfortunately, this information is not available for the first and the last two seasons of Friends!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friends_df %&amp;gt;%
  filter(str_detect(text, fixed(&amp;#39;Scene:&amp;#39;))) %&amp;gt;% 
  head() %&amp;gt;% 
  datatable()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scences &amp;lt;- friends_df %&amp;gt;%
  filter(str_detect(text, fixed(&amp;#39;Scene:&amp;#39;))) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(
    location = if_else(season &amp;lt;9,str_split(text, &amp;#39;,&amp;#39;)[[1]][1],str_split(text, fixed(&amp;#39;.&amp;#39;))[[1]][1]),
    location = str_remove_all(location, &amp;#39;erm&amp;#39;),
    location = str_remove_all(location, fixed(&amp;#39;erm&amp;#39;)),
    location = str_remove(location, fixed(&amp;#39;[Scene:&amp;#39;)) ,
    location = str_trim(location)
  ) %&amp;gt;% 
  filter(!str_detect(location, fixed(&amp;#39;]&amp;#39;)),
         !location %in% c(&amp;#39;Monica&amp;#39;, &amp;#39;Chandler&amp;#39;))

scenes_location_count &amp;lt;- scences %&amp;gt;%
  count(location, sort = TRUE) %&amp;gt;%
  filter(n &amp;gt; 10 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now letâ€™s sort and look at the top locations in Friends:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scenes_location_count  %&amp;gt;% 
  DT::datatable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, Central Perk was the location where most scenes took place!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; scenes_location_count %&amp;gt;% 
  ggplot(aes(x = fct_reorder(location,n),n)) +
  geom_col(fill = &amp;#39;#ef476f&amp;#39;) +
  coord_flip() +
  labs(title = &amp;#39;Most Filmed Locations in Friends&amp;#39; ) +
  theme_fivethirtyeight() +
  theme(legend.position = &amp;#39;none&amp;#39;, 
        axis.text.x = element_text(angle = 90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But letâ€™s look at how many times each different locations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scences %&amp;gt;% 
  count(season,location,sort = TRUE) %&amp;gt;% 
  filter(n&amp;gt;5) %&amp;gt;% 
  ggplot(aes(x= season,y=n)) +
  geom_line(color = &amp;#39;red&amp;#39;,size = 1)+
  geom_point(color = &amp;#39;red&amp;#39;,size = 3)+
  labs(title = &amp;#39;Where Was Freinds Filmed?&amp;#39;,subtitle = &amp;#39;&amp;#39;) +
  facet_wrap(~location) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it inspired me to perform the same analysis on the Friendsâ€™ transcript. However, my approach is slightly different from what Julia Sigles employed. She used bigrams to find a combination of pronouns and verbs while My approach uses named entity recognition (NER) form the natural language processing literature. In my previous posts, I used Spacy and Python to extract named entities from text data, but in this post I will exclusively use
the &lt;code&gt;spacyr&lt;/code&gt; package in R (which behind the scene uses Spacy and python).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;female_characters &amp;lt;- c(&amp;#39;monica&amp;#39;,&amp;#39;rachel&amp;#39;,&amp;#39;phoebe&amp;#39;,&amp;#39;emily&amp;#39;,&amp;#39;carol&amp;#39;,&amp;#39;janice&amp;#39;,&amp;#39;susan&amp;#39;,&amp;#39;kathy&amp;#39;,&amp;#39;elizabeth&amp;#39;,&amp;#39;she&amp;#39;)
male_characters &amp;lt;- c(&amp;#39;ross&amp;#39;,&amp;#39;joey&amp;#39;,&amp;#39;chandler&amp;#39;,&amp;#39;mike&amp;#39;,&amp;#39;richard&amp;#39;,&amp;#39;gunther&amp;#39;,&amp;#39;paolo&amp;#39;,&amp;#39;bob&amp;#39;,&amp;#39;paul&amp;#39;,&amp;#39;frank&amp;#39;,&amp;#39;he&amp;#39;)


male_speakers &amp;lt;- c(&amp;#39;Ross Geller&amp;#39;,&amp;#39;Chandler Bing&amp;#39;,&amp;#39;Joey Tribbiani&amp;#39;,&amp;#39;Richard Burke&amp;#39;,&amp;#39;Mike Hannigan&amp;#39;,&amp;#39;Frank Buffay Jr.&amp;#39;,&amp;#39;Jack Geller&amp;#39;,&amp;#39;David&amp;#39;,&amp;#39;Peter Becker&amp;#39;,&amp;#39;Eddie Menuek&amp;#39;,&amp;#39;Gunther&amp;#39;,&amp;#39;Paul Stevens&amp;#39;)

female_speakers &amp;lt;- c(&amp;#39;Rachel Green&amp;#39;,&amp;#39;Phoebe Buffay&amp;#39;,&amp;#39;Janice Litman Goralnik&amp;#39;,&amp;#39;Monica Geller&amp;#39;,&amp;#39;Emily Waltham&amp;#39;,&amp;#39;Charlie Wheeler&amp;#39;,&amp;#39;Judy Geller&amp;#39;,&amp;#39;Amy Green&amp;#39;,&amp;#39;Carol Willick&amp;#39;,&amp;#39;Mona&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I start by separating the text column into sentences and give each sentence a unique id which I use later for joining dataframes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friends_sentences &amp;lt;- friends_df %&amp;gt;%
  unnest_tokens(sentence, text, token = &amp;#39;sentences&amp;#39;) %&amp;gt;%
  rownames_to_column() %&amp;gt;%
  rename(doc_id = rowname,
         text = sentence) 
friends_sentences %&amp;gt;%
  tail() %&amp;gt;% 
  datatable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main function that I am going to use from &lt;code&gt;spacyr&lt;/code&gt; is the â€˜spacy_parseâ€™ function which can return named entities, dependencies, part-of-speech tags, etc. By default, it does not give us dependencies so we need to set &lt;code&gt;dependency = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tq &amp;lt;- tquery(pos = &amp;quot;VERB&amp;quot;, label = &amp;quot;verb&amp;quot;, children(relation = &amp;quot;nsubj&amp;quot;, label = &amp;quot;subject&amp;quot;))
tokens_df_dep &amp;lt;- friends_sentences %&amp;gt;% 
  spacy_parse(dependency = TRUE) %&amp;gt;%
  annotate_tqueries(&amp;quot;annotation&amp;quot;, query_name = tq) %&amp;gt;%
  filter(!is.na(annotation))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I join tokens that I created using spacy and the firends sentences dataframe.&lt;/p&gt;
&lt;p&gt;I will pull the documnets id that:&lt;/p&gt;
&lt;p&gt;token&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;friends-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Friendsâ€™ Network&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Which Presidential Debate Was More Chaotic?</title>
      <link>/post/2020-10-15-which-presidential-debate-was-more-chaotic/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-15-which-presidential-debate-was-more-chaotic/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Many people watched the first Presidential Debate between Biden and Trump and thought that this debate was chaotic, full of vulgar language, interruptions, and in a word, really ugly! Some people even consider this debate as &lt;a href=&#34;https://www.politico.com/news/2020/09/30/the-worst-presidential-debate-in-history-423765&#34;&gt;the worst debate in the modern history of US Presidential Elections!&lt;/a&gt; Four years ago, Trump was also a presidential candidate and ran against Hillary Clinton. The Presidential Debates in 2016 were not exceptionally friendly or civilized. So, the question is what made the 2020 first debate unique and chaotic in many peopleâ€™s minds. In this blog post, I will investigate this question and compare the 2020 and 2016 Presidential debates and the Vice Presidential debates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # metapackage of all tidyverse packages
library(lubridate)
library(readxl)
library(ggthemes)
library(showtext)
library(plotly)
font_add_google(&amp;quot;Lobster&amp;quot;, &amp;quot;Lobster&amp;quot;)
font_add_google(&amp;quot;Overpass&amp;quot;, &amp;quot;Overpass&amp;quot;)
options(repr.plot.width=20, repr.plot.height=15)

biden_col &amp;lt;- &amp;#39;#118ab2&amp;#39;
trump_col &amp;lt;- &amp;#39;#ef476f&amp;#39;
wallace_col &amp;lt;- &amp;#39;#ffd166&amp;#39;
cross_talk_col &amp;lt;- &amp;#39;#06d6a0&amp;#39;
moderator &amp;lt;- &amp;#39;#ffd166&amp;#39;
audience &amp;lt;- &amp;#39;#e36414&amp;#39;
clinton_col &amp;lt;- &amp;#39;#118ab2&amp;#39;
text_col &amp;lt;-  &amp;#39;gray80&amp;#39;

theme_set(theme_void())
theme_update(  
    legend.position = &amp;#39;top&amp;#39;,
    legend.text = element_text(
      size = 20,
      family = &amp;#39;Lobster&amp;#39;,
      color = text_col
    ),
    text = element_text(family = &amp;#39;Lobster&amp;#39;, color = text_col),
    plot.title = element_text(
      size = 40,
      margin = margin(b = 40, t = 50, l = 50),
      hjust = 0.5,
      family = &amp;#39;Lobster&amp;#39;,
      color = text_col
    ),
    plot.caption = element_text(
      size = 20,
      ,
      margin = margin(b = 50, t = 50),
      family = &amp;#39;Lobster&amp;#39;,
      color = text_col
    ),
    plot.background = element_rect(fill = &amp;#39;gray14&amp;#39;),
    panel.background = element_rect(fill = &amp;#39;gray14&amp;#39;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;biden-and-trumps-first-debate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Biden and Trumpâ€™s first debate&lt;/h3&gt;
&lt;p&gt;Let us look at the first debate between Trump and Biden and how much each candidate used to speak uninterrupted by the other candidate or the moderator. I used &lt;a href=&#34;https://www.kaggle.com/rmphilly18/vice-presidential-debate-2020-transcript&#34;&gt;this dataset which is available on Kaggle&lt;/a&gt; to computed how many seconds Trump and Biden talked without being cut off by the other candidate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;presidential_debate_2020 &amp;lt;- read_delim(&amp;#39;presidential_debate_2020.csv&amp;#39;,
                        delim = &amp;#39;\t&amp;#39;,
                        col_types = list(col_character(),
                                         col_character(),
                                         col_character(),
                                         col_character(),
                                         col_integer(),
                                         col_integer(),
                                         col_double(),
                                         col_integer()))
presidential_debate_2020 &amp;lt;- presidential_debate_2020 %&amp;gt;% 
  mutate(minutes = if_else(nchar(time)&amp;lt;6,paste(&amp;#39;00:&amp;#39;,time,sep = &amp;#39;&amp;#39;),time),
         minutes = paste(&amp;#39;2020-09-29&amp;#39;,minutes,sep = &amp;#39;&amp;#39;),
         minutes = lubridate::ymd_hms(minutes),
         speaker = if_else(str_detect(speaker , &amp;#39;Chris Wallace&amp;#39;),&amp;#39;Chris Wallace (Moderator)&amp;#39;,speaker),
          ) %&amp;gt;% 
    mutate(minute_start = lag(minutes,n= 1),
         duration =minutes - minute_start,
         duration = lead(duration,n =1),
         seconds_in_end = lead(seconds_in),
         text = str_wrap(text,width =30))


presidential_debate_2020[1,5] &amp;lt;- 0

glimpse(presidential_debate_2020)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 788
## Columns: 12
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;Chris Wallace (Moderator)&amp;quot;, &amp;quot;Chris Wallace (Moderat...
## $ text           &amp;lt;chr&amp;gt; &amp;quot;Good evening from the Health\nEducation Campus of C...
## $ url            &amp;lt;chr&amp;gt; &amp;quot;https://www.rev.com/transcript-editor/shared/C-8bDp...
## $ time           &amp;lt;chr&amp;gt; &amp;quot;1:20&amp;quot;, &amp;quot;2:10&amp;quot;, &amp;quot;2:49&amp;quot;, &amp;quot;2:51&amp;quot;, &amp;quot;2:51&amp;quot;, &amp;quot;3:11&amp;quot;, &amp;quot;4:0...
## $ seconds_in     &amp;lt;int&amp;gt; 0, 130, 169, 171, 171, 191, 241, 293, 322, 329, 334,...
## $ seconds_spoken &amp;lt;int&amp;gt; 50, 39, 2, 0, 20, 50, 52, 29, 7, 5, 2, 36, 56, 26, 2...
## $ words_per_min  &amp;lt;dbl&amp;gt; 148.8000, 156.9231, 120.0000, NA, NA, 159.6000, 180....
## $ num_words      &amp;lt;int&amp;gt; 124, 102, 4, 4, 2, 133, 156, 98, 15, 16, 3, 117, 157...
## $ minutes        &amp;lt;dttm&amp;gt; 2020-09-29 00:01:20, 2020-09-29 00:02:10, 2020-09-2...
## $ minute_start   &amp;lt;dttm&amp;gt; NA, 2020-09-29 00:01:20, 2020-09-29 00:02:10, 2020-...
## $ duration       &amp;lt;drtn&amp;gt; 50 secs, 39 secs, 2 secs, 0 secs, 20 secs, 50 secs,...
## $ seconds_in_end &amp;lt;int&amp;gt; 130, 169, 171, 171, 191, 241, 293, 322, 329, 334, 33...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;presidential_debate_2020 %&amp;gt;%
  ggplot(aes(
    x = seconds_in,
    y = 1,
    xend = seconds_in_end,
    yend = 1,
    color = speaker
  )) +
  geom_segment(size = 40, alpha = 0.7) +

  guides(color = guide_legend(override.aes = list(size = 25))) +
  scale_color_manual(values = c(wallace_col, trump_col, biden_col)) +
  scale_y_continuous(limits = c(0.85, 1.13)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;&amp;#39;,
    title = &amp;#39;How Did The First US Presidential Debate Go?&amp;#39;,
    #subtitle = &amp;#39;This plot illustrates how much time each presidential candidate spoke &amp;#39;,
    fill = &amp;#39;&amp;#39;,
    color = &amp;#39;&amp;#39;  ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-15-which-presidential-debate-was-more-chaotic/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clinton-vs.-trump&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Clinton vs.Â Trump&lt;/h3&gt;
&lt;p&gt;This plot clearly shows that there were many interruptions during the first debate! It also shows that it was Trump who interrupted most in the debate. To give it more context, let us compare it to the 2016 Debates between Trump and Clinton. The transcripts of these debates are available in &lt;a href=&#34;https://www.kaggle.com/mrisdal/2016-us-presidential-debates&#34;&gt;this dataset that I found on Kaggle&lt;/a&gt;. However, this dataset does not include information about how many seconds or minutes each candidate spent speaking in the debates. So, I decided to use the number of words each candidate spoke to measure continuity in their speeches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_2016 &amp;lt;- read_csv(&amp;#39;presidential_debate_2016.csv&amp;#39;)
debate_2016 &amp;lt;-debate_2016 %&amp;gt;% 
    mutate(Text = as.character(Text),
    num_chars = str_length(Text),
    num_word = str_count(Text),
    Speaker = case_when(Speaker == &amp;#39;CANDIDATES&amp;#39;~&amp;#39;Crosstalk&amp;#39;,
                        Speaker == &amp;#39;QUESTION&amp;#39;~&amp;#39;Question&amp;#39;,
                        Speaker %in% c(&amp;#39;Cooper&amp;#39;, &amp;#39;Holt&amp;#39;,&amp;#39;Wallace&amp;#39;,&amp;#39;Raddatz&amp;#39;,&amp;#39;Quijano&amp;#39;) ~&amp;#39;Moderator&amp;#39;,
                        TRUE ~ Speaker))

presidential_debate_2016 &amp;lt;- debate_2016 %&amp;gt;% 
  filter(!Speaker %in% c(&amp;#39;Kaine&amp;#39;,&amp;#39;Pence&amp;#39;),
         Date != &amp;#39;10/4/16&amp;#39;)


first_debate &amp;lt;- presidential_debate_2016%&amp;gt;% 
  filter(Date == &amp;#39;9/26/16&amp;#39;)
second_debate &amp;lt;- presidential_debate_2016 %&amp;gt;% 
  filter(Date == &amp;#39;10/9/16&amp;#39;)
third_debate &amp;lt;- presidential_debate_2016 %&amp;gt;% 
  filter(Date == &amp;#39;10/19/2016&amp;#39;)

glimpse(presidential_debate_2016)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,028
## Columns: 6
## $ Line      &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...
## $ Speaker   &amp;lt;chr&amp;gt; &amp;quot;Moderator&amp;quot;, &amp;quot;Audience&amp;quot;, &amp;quot;Clinton&amp;quot;, &amp;quot;Audience&amp;quot;, &amp;quot;Moderato...
## $ Text      &amp;lt;chr&amp;gt; &amp;quot;Good evening from Hofstra University in Hempstead, New Y...
## $ Date      &amp;lt;chr&amp;gt; &amp;quot;9/26/16&amp;quot;, &amp;quot;9/26/16&amp;quot;, &amp;quot;9/26/16&amp;quot;, &amp;quot;9/26/16&amp;quot;, &amp;quot;9/26/16&amp;quot;, &amp;quot;9...
## $ num_chars &amp;lt;int&amp;gt; 1257, 10, 20, 10, 17, 10, 1115, 820, 1018, 171, 1570, 515...
## $ num_word  &amp;lt;int&amp;gt; 1257, 10, 20, 10, 17, 10, 1115, 820, 1018, 171, 1570, 515...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_debate&amp;lt;- first_debate %&amp;gt;% 
  mutate(cumsum_nwords = cumsum(num_word),
         cum_sum_lag = lag(cumsum_nwords),
         debate = &amp;#39;First Debate&amp;#39;)

first_debate[1,7] &amp;lt;- 0

second_debate&amp;lt;- second_debate %&amp;gt;% 
  mutate(cumsum_nwords = cumsum(num_word),
         cum_sum_lag = lag(cumsum_nwords),
         debate = &amp;#39;Second Debate&amp;#39;)

second_debate[1,7] &amp;lt;- 0

third_debate&amp;lt;- third_debate %&amp;gt;% 
  mutate(cumsum_nwords = cumsum(num_word),
         cum_sum_lag = lag(cumsum_nwords),
         debate = &amp;#39;Third Debate&amp;#39;)

third_debate[1,7] &amp;lt;- 0


presidential_debates_2016 &amp;lt;- bind_rows(first_debate,second_debate ,third_debate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;presidential_debates_2016 %&amp;gt;%
ggplot(aes(
    x = cum_sum_lag,
    y = 1,
    xend = cumsum_nwords,
    yend = 1,
    color = Speaker
  )) +
  geom_segment(size = 40, alpha = 0.7) +

  guides(color = guide_legend(override.aes = list(size = 18))) +
  scale_color_manual(values = c(&amp;#39;Moderator&amp;#39; = moderator ,&amp;#39;Trump&amp;#39; = trump_col  ,
                               &amp;#39;Clinton&amp;#39; = clinton_col ,
                              &amp;#39;Crosstalk&amp;#39;  =&amp;#39;#9d4edd&amp;#39;,
                              &amp;#39;Audience&amp;#39; =  audience ,
                              &amp;#39;Question&amp;#39;  =  &amp;#39;#e85d04&amp;#39;)) +
  scale_y_continuous(limits = c(0.85, 1.13)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;&amp;#39;,
    title = &amp;#39;How Did 2016 US Presidential Debates Go?&amp;#39;,
    fill = &amp;#39;&amp;#39;,
    color = &amp;#39;&amp;#39;,
    caption =  
  ) +
  facet_wrap(~debate,nrow = 3) +
  theme(strip.text = element_text(size = 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-15-which-presidential-debate-was-more-chaotic/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, the debates between Trump and Clinton were also wild, but they were less anarchic than the debate between Biden and Trump. For, there were fewer disruptions in those debates compared to what we saw in the 2020 plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vice-presidential-debates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vice Presidential debates&lt;/h3&gt;
&lt;p&gt;Now letâ€™s look at the debates between the Vice Presidential candidates. Usually, these debates are more civilized and less heated as they attract less attention.&lt;/p&gt;
&lt;div id=&#34;harris-vs.-pence&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Harris vs.Â Pence&lt;/h4&gt;
&lt;p&gt;I found &lt;a href=&#34;https://www.kaggle.com/headsortails/us-election-2020-presidential-debates&#34;&gt;a dataset of the 2020 VP debate on Kaggle&lt;/a&gt;. Again, here I used the same approach that I used for the 2016 debate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vice_presidential_debate_2020 &amp;lt;- read_csv(&amp;#39;vice_presidential_debate_2020.csv&amp;#39;)
vice_presidential_debate_2020 &amp;lt;- vice_presidential_debate_2020 %&amp;gt;%
mutate(num_chars = str_length(text ),
       cumsum_nwords = cumsum(num_chars),
      cum_sum_lag = lag(cumsum_nwords))

vice_presidential_debate_2020[1,6] &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vice_presidential_debate_2020 %&amp;gt;%
ggplot(aes(
    x = cum_sum_lag,
    y = 1,
    xend = cumsum_nwords,
    yend = 1,
    color = speaker 
  )) +
  geom_segment(size = 40, alpha = 0.7) +

  guides(color = guide_legend(override.aes = list(size = 18))) +
  scale_color_manual(values = c(&amp;quot;Susan Page&amp;quot; = moderator ,&amp;#39;Mike Pence&amp;#39; = trump_col  ,
                               &amp;#39;Kamala Harris&amp;#39; = clinton_col )) +
  scale_y_continuous(limits = c(0.85, 1.13)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;&amp;#39;,
    title = &amp;#39;How Did 2020 US Vice-Presidential Debates Go?&amp;#39;,
    fill = &amp;#39;&amp;#39;,
    color = &amp;#39;&amp;#39;,
    caption =  
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-15-which-presidential-debate-was-more-chaotic/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vice-presidential-debate&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2016 Vice Presidential debate&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vp_debates &amp;lt;- debate_2016 %&amp;gt;% 
  filter(Date == &amp;#39;10/4/16&amp;#39;) %&amp;gt;% 
  mutate(cumsum_nwords = cumsum(num_word),
         cum_sum_lag = lag(cumsum_nwords)) 
  
vp_debates[1,5] &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vp_debates %&amp;gt;%
ggplot(aes(
    x = cum_sum_lag,
    y = 1,
    xend = cumsum_nwords,
    yend = 1,
    color = Speaker 
  )) +
  geom_segment(size = 40, alpha = 0.7) +

  guides(color = guide_legend(override.aes = list(size = 18))) +
  scale_color_manual(values = c(&amp;#39;Moderator&amp;#39; = moderator ,&amp;#39;Pence&amp;#39; = trump_col  ,
                               &amp;#39;Kaine&amp;#39; = clinton_col ,
                              &amp;#39;Crosstalk&amp;#39;  =&amp;#39;#9d4edd&amp;#39;,
                              &amp;#39;Audience&amp;#39; =  audience )) +
  scale_y_continuous(limits = c(0.85, 1.13)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;&amp;#39;,
    title = &amp;#39;How Did 2016 US Vice-Presidential Debates Go?&amp;#39;,
    fill = &amp;#39;&amp;#39;,
    color = &amp;#39;&amp;#39;,
    caption =  
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-15-which-presidential-debate-was-more-chaotic/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, we can see that Pence and Kaine cut each other off many times during the VP debate. However, we can also observe that they could speak uninterrupted on some occasions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Happiest, Saddest, Most Energetic and Most Popular Persian Singers on Spotify</title>
      <link>/post/happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;I am a music lover, and like my other hobbies, I am really interested in applying data science methods to it. A few months ago, I participated in the third week of the TidyTuesday project, where I made a map of Spotify songs based on audio features and a dimensionality reduction algorithm called UMAP. Since then, I have been using Spotifyâ€™s Web API to collect data, and recently, I decided to look at some of my favorite Iranian artists and their songs on Spotify. We have different genres and types of music, and while pop and rap are very popular among the younger generation, I like the traditional style more. Nevertheless, I was always curious to understand how different traditional music and pop music are. For this reason, that I like the most These are a few questions that I would like to answer:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How different audio features can be among top Persian singers?&lt;/li&gt;
&lt;li&gt;What are the most danceable and least danceable Persian songs?&lt;/li&gt;
&lt;li&gt;Who is the most popular Persian singer, and what is the most popular song?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kableExtra)
library(tidyverse)
library(googlesheets4)
library(tidymodels)
library(gghighlight)
library(hrbrthemes)
library(ggthemes)
library(ggrepel)
library(ggalt)
library(extrafont)
library(ggtext)
library(ggforce)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;I compiled a list of Persian Singers manually and collected information about their available songs on Spotify using the &lt;code&gt;spotifyr&lt;/code&gt; package in R which lets us use R to access the Spotifyâ€™s API. This process was cumbersome as sometimes I was not getting what I was looking for. For instance, sometimes, songs that belonged to another random artist were retrieved. For each singer, we can only retrieve the top 10 popular songs. It means that the rest of the songs have no popularity scores. In the end, I collected various kinds of information about more than 10000 songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop &amp;lt;- read_csv(&amp;#39;https://raw.githubusercontent.com/mcnakhaee/datasets/master/Persian_Songs_Spotify.csv&amp;#39;)

head(songs_audio_plus_pop) %&amp;gt;% 
  kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
track_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
poet
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
lyrics
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
lyrics source
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
disc_number
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
duration_ms
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
explicit
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
track_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
track_name_farsi
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
artist_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
artist_name_farsi
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
popularity
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
track_number
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
album_href
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
album_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
album_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
album_release_date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
album_total_tracks
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
album_release_year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
track_href
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
danceability
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
energy
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
key
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
loudness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mode
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
speechiness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
acousticness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
instrumentalness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
liveness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
valence
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tempo
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
time_signature
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
key_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
mode_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
key_mode
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
artist_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
lyrics_1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
poet_1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
lyric source
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
genre
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31iPeC6I0AiRW8InOxNKzm
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
446880
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ghazale Taze
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6GcmAWrnnMb2BuVriPhBLa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Va Eshgh Amad
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2020-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2020
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/31iPeC6I0AiRW8InOxNKzm&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/31iPeC6I0AiRW8InOxNKzm&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.437
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.390
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-7.170
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0299
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.839
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.51e-05
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1360
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.330
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
131.913
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4Fi46ha8teWYTwk0b8fNPi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
851920
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ayeeneye Hosn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6GcmAWrnnMb2BuVriPhBLa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Va Eshgh Amad
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2020-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2020
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/4Fi46ha8teWYTwk0b8fNPi&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/4Fi46ha8teWYTwk0b8fNPi&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.379
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.146
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-10.008
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0414
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.60e-04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0812
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.346
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
105.634
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
major
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
F major
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0lQAe6EslKA7CUsS7SCW6Q
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
293160
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tarke Eshgh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6GcmAWrnnMb2BuVriPhBLa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Va Eshgh Amad
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2020-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2020
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/0lQAe6EslKA7CUsS7SCW6Q&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/0lQAe6EslKA7CUsS7SCW6Q&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.437
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.453
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.392
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0349
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.664
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.07e-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.501
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
94.651
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
F minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6dAFmJdVsKk5ksCpGqnKgO
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
648720
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moghbacheye Bade Foroosh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6GcmAWrnnMb2BuVriPhBLa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Va Eshgh Amad
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2020-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2020
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/6dAFmJdVsKk5ksCpGqnKgO&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/6dAFmJdVsKk5ksCpGqnKgO&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.488
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.138
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-12.287
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.915
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.58e-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
110.967
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4VSDJGyEdSMB8UL4fDSCvv
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
273480
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bigharar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6GcmAWrnnMb2BuVriPhBLa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Va Eshgh Amad
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2020-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2020
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/4VSDJGyEdSMB8UL4fDSCvv&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/4VSDJGyEdSMB8UL4fDSCvv&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.301
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.443
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.702
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0334
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.657
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.50e-06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.410
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1tqsOZ3fGtMXL0r2ySBpvA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
260754
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Negar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salar Aghili
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;lt;U+0633&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0631&amp;gt; &amp;lt;U+0639&amp;gt;&amp;lt;U+0642&amp;gt;&amp;lt;U+06CC&amp;gt;&amp;lt;U+0644&amp;gt;&amp;lt;U+06CC&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
09Hepb4NioQ6sO87tsDyiz
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Negar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-10-30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2019
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://api.spotify.com/v1/tracks/1tqsOZ3fGtMXL0r2ySBpvA&#34; class=&#34;uri&#34;&gt;https://api.spotify.com/v1/tracks/1tqsOZ3fGtMXL0r2ySBpvA&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.577
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.366
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6.668
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0368
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.834
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.90e-06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.367
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77.453
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C minor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;overall-song-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overall Song Features&lt;/h3&gt;
&lt;p&gt;Apart from variables such as the album that a song belongs to and its date of release, Spotifyâ€™s API can give us several features that capture a songâ€™s different audio characteristics.&lt;/p&gt;
&lt;p&gt;You can see a full list of these features in this link. However, I am only interested in some of these features, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;valence&lt;/strong&gt; measures the happiness of a song.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;energy&lt;/strong&gt; is relatively self-explanatory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tempo&lt;/strong&gt; measures the speed of a song.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;loudness&lt;/strong&gt; is also self-explanatory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;acousticness&lt;/strong&gt; identifies whether the track is acoustic&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;instrumentalness&lt;/strong&gt; shows whether a track contains no vocals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;danceability&lt;/strong&gt; determines how good a song is for dancing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jakelawlor/TidyTuesday_JL/blob/master/CodeFiles/Jan21.20.Spotify.Rmd&#34;&gt;This excellent visualization&lt;/a&gt; inspired me to create a similar plot for some of the most well-known Persian singers and see how their audio features differ from each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;artists &amp;lt;-
  c( &amp;#39;Sirvan Khosravi&amp;#39;,&amp;#39;Hesameddin Seraj&amp;#39;,&amp;#39;Rastak&amp;#39;,&amp;#39;Shahram Nazeri&amp;#39;,&amp;#39;Hossein Alizadeh&amp;#39;,&amp;#39;Reza Sadeghi&amp;#39;,&amp;#39;Alireza Eftekhari&amp;#39;,&amp;#39;Mohammadreza Shajarian&amp;#39;,
     &amp;#39;Salar Aghili&amp;#39;,&amp;#39;Morteza Pashaei&amp;#39;, &amp;#39;Alireza Ghorbani&amp;#39;,&amp;#39;Homayoun Shajarian&amp;#39;, &amp;#39;Mohsen Yeganeh&amp;#39; ,&amp;#39;Morteza Pashaei&amp;#39;,&amp;#39;Moein&amp;#39;,&amp;#39;Farzad Farzin&amp;#39;,
     &amp;#39;Babak Jahanbakhsh&amp;#39;, &amp;#39;Ehsan Khajeh Amiri&amp;#39;,&amp;#39;Siavash Ghomayshi&amp;#39;,&amp;#39;Xaniar Khosravi&amp;#39;,&amp;#39;Tohi&amp;#39; ,&amp;#39;Mohsen Chavoshi&amp;#39;,&amp;#39;Amir Tataloo&amp;#39;,
     &amp;#39;Hamed Homayoun&amp;#39;,&amp;#39;Kayhan Kalhor&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will plot the average, the minimum, and the maximum value of each feature for each singer. That gives us a good picture of how different their audio characteristics are from each other. However, we must make the right adjustments to the dataset before visualizing it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We need to transform the original dataset into a long-dataframe, which can be done by &lt;code&gt;pivot_longer&lt;/code&gt; from the&lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We should rescale each audio feature, otherwise, the plot would not make any sense.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;order &amp;lt;- c(
  &amp;quot;valence&amp;quot;,
  &amp;quot;energy&amp;quot;,
  &amp;quot;tempo&amp;quot;,
  &amp;quot;loudness&amp;quot;,
  &amp;quot;acousticness&amp;quot;,
  &amp;quot;instrumentalness&amp;quot;,
  &amp;quot;danceability&amp;quot;
)

scaled_features_long &amp;lt;- songs_audio_plus_pop %&amp;gt;%
  mutate_at(order, scales::rescale, to = c(0, 7)) %&amp;gt;%
  filter(!is.na(popularity)) %&amp;gt;%
  filter(artist_name %in% artists) %&amp;gt;%
  mutate(artist_name = factor(artist_name))  %&amp;gt;%
  pivot_longer(
    names_to = &amp;#39;metric&amp;#39;,
    cols = c(
      &amp;quot;valence&amp;quot;,
      &amp;quot;energy&amp;quot;,
      &amp;quot;tempo&amp;quot;,
      &amp;quot;loudness&amp;quot;,
      &amp;quot;acousticness&amp;quot;,
      &amp;quot;danceability&amp;quot;),
    values_to = &amp;#39;value&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can visualize the results for each artist. As mentioned before, I will compare artists by the minimum (red), the average (orange), and maximum (yellow) values of each audio feature in their songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  ### This plots the average of each audio feature
  geom_polygon(
    data = scaled_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), mean) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name,),
    alpha = .54,
    size = 1.5,
    show.legend = T,
    fill = &amp;#39;#FF1654&amp;#39;
  ) +
  ### This plots the maximum of each audio feature
  geom_polygon(
    data = scaled_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), max) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name,),
    alpha = .44,
    size = 1.5,
    show.legend = T,
    fill = &amp;#39;#FFE066&amp;#39;
  ) +
  ### This plots the mimumn of each audio feature
  geom_polygon(
    data = scaled_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), min) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name,),
    alpha = .84,
    size = 1.5,
    show.legend = T,
    fill =  &amp;quot;#EF476F&amp;quot;
  ) +
  scale_x_discrete(
    limits = order,
    labels = c(
      &amp;quot;Happy&amp;quot;,
      &amp;quot;Energy&amp;quot;,
      &amp;quot;Fast&amp;quot;,
      &amp;quot;Loud&amp;quot;,
      &amp;quot;Acoustic&amp;quot;,
      &amp;quot;Instrumental&amp;quot;,
      &amp;quot;Danceable&amp;quot;
    )
  ) +
  coord_polar(clip = &amp;#39;off&amp;#39;) +
  theme_minimal() +
  labs(title = &amp;quot;Persian Singers and Their Audio Characteristics&amp;quot;,
       caption = &amp;#39;Source: Spotify \n Visualization: mcnakhaee&amp;#39;) +
  ylim(0, 8) +
  facet_wrap( ~ artist_name, ncol = 4) +
  theme(
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_blank(),
    axis.text.x = element_text(
      family =  &amp;#39;Montserrat&amp;#39;,
      size = 13.5,
      margin = ggplot2::margin(30, 0, 20, 0)
    ),
    plot.caption = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 11,
      color = &amp;#39;grey80&amp;#39;
    ) ,
    text = element_text(family =  &amp;#39;Montserrat&amp;#39;),
    strip.text = element_text(family =  &amp;#39;Montserrat&amp;#39;, size = 18),
    strip.text.x = element_text(margin = ggplot2::margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;)),
    panel.spacing = unit(3.5, &amp;quot;lines&amp;quot;),
    panel.grid = element_blank(),
    plot.title = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 32,
      color = &amp;#39;gray10&amp;#39;
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;2112&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-more-closely-at-each-audio-feature&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Looking more closely at each audio feature&lt;/h3&gt;
&lt;p&gt;My first plot is informative, but it only gives us an overall picture of audio features. However, I would like to have a more detailed picture of singers and the audio features for each of their songs. For this reason, I will also make a separate plot for each audio feature where every song and its corresponding feature values are shown. I will also mark a few popular songs from each artist with a different color on this plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set a custom theme for our plots
theme_set(theme_void() +
  theme(
    text = element_text(family =  &amp;#39;Montserrat&amp;#39;),
    axis.text.x = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      color = &amp;#39;gray80&amp;#39;,
      size = 18
    ),
    axis.text.y = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 20),
      color = &amp;#39;gray80&amp;#39;,
      size = 20
    ),
    axis.title.x = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 22,
      color = &amp;#39;gray80&amp;#39;
    ),
    plot.title = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(40, 0, 40, 0),
      size = 35,
      color = &amp;#39;gray80&amp;#39;
    ),
    plot.caption = element_text(family =&amp;#39;Montserrat&amp;#39;,
                                  margin = ggplot2::margin(30, 0, 20, 20),
                                      size = 20,
                                  color = &amp;#39;gray70&amp;#39;) ,
    legend.position = &amp;#39;none&amp;#39;,
    plot.background = element_rect(fill = &amp;quot;#516869&amp;quot;)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again here, I will change the dataset to make it ready for visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter &amp;lt;- songs_audio_plus_pop %&amp;gt;% 
  filter(artist_name %in% artists) %&amp;gt;% 
  mutate(is_popular = !is.na(popularity)) %&amp;gt;%
  distinct(artist_name,track_name,.keep_all = T) %&amp;gt;% 
  mutate(is_popular_size = if_else(!is.na(popularity),popularity,25),
         is_popular_alpha = if_else(!is.na(popularity),0.8,0.5)) %&amp;gt;% 
  mutate(track_name= str_wrap(track_name, width = 15)) %&amp;gt;% 
  mutate(popular_track_name = if_else(!is.na(track_name_farsi)&amp;amp; !is.na(popularity) &amp;amp; nchar(track_name) &amp;lt; 20 &amp;amp; !explicit,track_name,&amp;#39;&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;happiness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Happiness&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name, y = valence)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
  ) +
  geom_text_repel(
    aes(label = popular_track_name , x = artist_name , y = valence),
    family = &amp;#39;Montserrat&amp;#39;,
    color = &amp;#39;gray99&amp;#39;,
    size = 5,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name)) +
  scale_color_manual(values = c(&amp;#39;#FFD166&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;energy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Energy&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name, y = energy)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
  ) +
  geom_text_repel(
    aes(label = popular_track_name , x = artist_name , y = energy),
    family = &amp;#39;Montserrat&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name)
  ) +
  scale_color_manual(values = c(&amp;#39;#EF476F&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acousticness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acousticness&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name, y = acousticness)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
  ) +
  geom_text_repel(
    aes(label = popular_track_name , x = artist_name , y = acousticness),
    family = &amp;#39;Montserrat&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name)
  ) +
  scale_color_manual(values = c(&amp;#39;#118AB2&amp;#39;, &amp;#39;#06D6A0&amp;#39;)) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;danceability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Danceability&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name, y = danceability)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
  ) +
  geom_text_repel(
    aes(label = popular_track_name , x = artist_name , y = danceability),
    family = &amp;#39;Montserrat&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name)
  ) +
  scale_color_manual(values = c(&amp;#39;#A5668B&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loudness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loudness&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name, y = loudness)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
    
  ) +
  geom_text_repel(
    aes(label = popular_track_name , x = artist_name , y = loudness),
    family = &amp;#39;Montserrat&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name)
  ) +
  scale_color_manual(values = c(&amp;#39;#06D6A0&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;most-popular-songs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Popular Songs&lt;/h2&gt;
&lt;p&gt;As I mentioned previously, we can only retrieve his/her top 10 popular songs for each artist. The popularity of a track is a value between 0 (the least popular) and 100 (the most popular). Spotify uses an algorithm to calculate popularity scores, which is heavily influenced by the total number of times a song has been played recently. You can read more about it in this &lt;a href=&#34;https://developer.spotify.com/documentation/web-api/reference/tracks/get-track/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Knowing this fact about how popularity is measured, we can visualize songs and artists that have been popular and played recently.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop &amp;lt;- songs_audio_plus_pop %&amp;gt;%
  filter(
    !artist_name %in% c(
      &amp;#39;Hatam Asgari&amp;#39;,
      &amp;#39;Kaveh Deylami&amp;#39;,
      &amp;#39;Nasser Abdollahi&amp;#39;,
      &amp;#39;Peyman Yazdanian&amp;#39;,
      &amp;#39;Abbas Ghaderi&amp;#39;,
      &amp;#39;Mohammad Golriz&amp;#39;,
      &amp;#39;Hamid Hami&amp;#39;,
      &amp;#39;Koveyti Poor&amp;#39;,
      &amp;#39;Mohsen Sharifian&amp;#39;,
      &amp;#39;Soheil Nafissi&amp;#39;))
songs_audio_plus_pop %&amp;gt;%
  filter(!is.na(popularity)) %&amp;gt;%
  mutate(track_name = if_else(!is.na(track_name), track_name, track_name)) %&amp;gt;%
  group_by(artist_name) %&amp;gt;%
  summarize(
    avg_pop = mean(popularity),
    min_pop = min(popularity),
    max_pop = max(popularity),
    most_popular = track_name[which.max(popularity)],
    least_popular = track_name[which.min(popularity)]
  ) %&amp;gt;%
  mutate(
    artist_name = fct_reorder(artist_name, avg_pop),
  ) %&amp;gt;%
  
  ggplot(aes(x = min_pop , xend = max_pop, y = artist_name)) +
  geom_dumbbell(
    colour_x = &amp;#39;#ef476f&amp;#39;,
    colour_xend = &amp;#39;#118ab2&amp;#39;,
    size_x = 7,
    size_xend = 7
  ) +
  geom_text(
    aes(x = min_pop - 1, y = artist_name, label = least_popular),
    size = 7,
    family = &amp;#39;Montserrat&amp;#39;,
    hjust = 1
  ) +
  geom_text(
    aes(x = max_pop + 1, y = artist_name, label = most_popular),
    size = 7,
    family = &amp;#39;Montserrat&amp;#39;,
    hjust = 0
  ) +
  scale_x_continuous(sec.axis = dup_axis()) +
  theme_tufte() +
  theme(
    plot.title = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(0, 0, 40, 0),
      size = 45
    ),
    plot.subtitle = element_markdown(
      family = &amp;#39;Montserrat&amp;#39;,
      size = 15,
      margin = ggplot2::margin(20, 0, 40, 0),
      hjust = 1
      
    ),
    axis.text.x = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 20
    ),
    
    axis.text.y = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 20
    ),
    axis.title.x = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 30
    ),
    plot.caption = element_text(family =&amp;#39;Montserrat&amp;#39;,
                                margin = ggplot2::margin(30, 0, 20, 20),
                                size = 20,
                                color = &amp;#39;gray20&amp;#39;) ,
    axis.title.y = element_blank(),
    plot.background = element_rect(fill = &amp;#39;#FCF0E1&amp;#39;),
    plot.margin = unit(c(1, 1, 1.5, 1.2), &amp;quot;cm&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-04-happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows the most popular song and the least popular track of each artist among his top 10 songs. The artists are also sorted based on their average popularity.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the 2020 Democratic Presidential Debates - Part 2</title>
      <link>/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Many of us could not watch every 2020 Democratic Primary debate. It was important for some of us to know what happened during the debates. In my case, I was reading about what happened in debates in some online newspapers, or I watched a highlight of a debate on Youtube the next day. However, they only give a summary of a debate or just broadcast a portion of debates that includes a heated exchange of opinions between candidates. As a result, many important issues raised by candidates will be &lt;strong&gt;ignored and forgotten in the&lt;/strong&gt; aftermath of a debate. So, it is crucial to summarize the debateâ€™s content so that everyone could understand what went on in the debate and what issues each candidate addressed during his/her speech. In this blog post, I will show you how I used some NLP techniques for exploring the content of debates and give you a comprehensive overview of topics that each candidate discussed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mcnakhaee.com/post/2020-02-23-the-most-eloeuent-democratic-candidate/&#34;&gt;In my last blog post&lt;/a&gt;, I explained that I had the three following goals in mind when I started exploring the 2020 Democratic Debates :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To know how eloquent presidential candidates are.&lt;/li&gt;
&lt;li&gt;To find out who used more positive or more negative words in his/her speech by performing sentiment analysis.&lt;/li&gt;
&lt;li&gt;A map of topics, individuals, and entities that each candidate mentioned in his/her speech by using named entity recognition and network analysis..&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I only discussed how I approached the first two aspects of my experiment in my last blogpost. Now it is time to investigate the third and last one.&lt;/p&gt;
&lt;p&gt;Initially, my aim was to use network analysis to determine potential allies and enemies on the debate stage. For example, &lt;a href=&#34;https://www.theguardian.com/us-news/2020/mar/04/mike-bloomberg-out-60-second-attack-elizabeth-warren-destroyed-campaign&#34;&gt;Elizabeth Warren mentioned Mike Bloomberg several times and attacked him harshly in the 9th debate&lt;/a&gt;. During the same debate, &lt;a href=&#34;https://www.independent.co.uk/news/world/americas/us-election/amy-klobuchar-pete-buttigieg-handshake-democratic-debate-video-a9348621.html&#34;&gt;Amy Klobuchar and Pete Buttigieg clashed bitterly with each other&lt;/a&gt;. These are just two instances of many other heated exchanges between the candidates that happened throughout the ten debates.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To make things more precise, I transformed my objective into two questions that I would like to answer:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How many times did a candidate address (mention) other candidates during a debate?&lt;/li&gt;
&lt;li&gt;How did he/she refer to a candidate(in a friendly or unfriendly manner)?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;A simple approach to answering these questions is to store the names of all candidates in a variable (for example, a vector in R or a list in Python), iterate over the transcript, compute the sentiment, count and store the number of times that another candidate brought up a candidateâ€™s name.&lt;/p&gt;
&lt;p&gt;However, this approach is a little bit challenging and requires a lot of manual data pre-processing efforts. For each democratic candidate, one must compile a comprehensive combination of ways that may be used to call a candidate, and to prepare such a list seems to be a very time-consuming task. For example, other candidates mentioned Bernie Sanders in many different ways, including Bernie, Bernie Sanders, or Senator Sanders.&lt;/p&gt;
&lt;p&gt;I realized that I could use Named Entity Recognition (NER), a technique from the Natural Language Processing (NLP) literature, to extract candidatesâ€™ names from the transcript and solve this problem more efficiently. Using this approach, I can find candidatesâ€™ names from the transcript, but I can also find the names of other politicians, individuals, and even organizations and further extend my analysis to include many more topics and issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Workflow&lt;/h2&gt;
&lt;p&gt;I made use of both Python and R in my analysis. My workflow includes the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I access the transcript of debates using this package.&lt;/li&gt;
&lt;li&gt;I use tidytext to split the transcript into multiple sentences and also for sentiment analysis.&lt;/li&gt;
&lt;li&gt;I extract several types of Named Entities from each sentence, using Spacy,&lt;/li&gt;
&lt;li&gt;I compute the sentiment of each sentence using TextBlob library in Python.&lt;/li&gt;
&lt;li&gt;I transferred the results to R for visualization. There, I visualize the network of mentions and entities using ggraph and ggplot library.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that I could have implemented all the steps in R. For instance, Spacy has an R wrapper called Spacyr, which gives the same functionality that I need for this analysis. However, Iâ€™d like to increase the number of tools that I can use. Notably, using Python and R side by side is an exciting challenge for me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(demdebates2020)
library(tidytext)
library(tidygraph)
library(tidyverse)
library(ggraph)
library(gghighlight)
library(ggthemes)
library(kableExtra)
library(reticulate)
library(magrittr)
library(pluralize)
theme_set(
  theme_graph(base_family = &amp;#39;Montserrat&amp;#39;)  +
    theme(
      panel.border = element_blank(),
      plot.title = element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#540b0e&amp;#39;,
        size = 42,
        margin = ggplot2::margin(40, 40, 20, 10),
        hjust = 0
      ),
      plot.subtitle =  element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#7d4f50&amp;#39;,
        size = 30,
        margin = ggplot2::margin(20, 40, 80, 10),
        hjust = 0
      ),
      plot.caption =  element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#540b0e&amp;#39;,
        size = 16,
        margin = ggplot2::margin(0, 0, 20, 20),
      ),
      legend.position = &amp;#39;none&amp;#39;,
      plot.background = element_rect(fill = &amp;#39;#FCF0E1&amp;#39;),
      
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;loading-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.2 Loading the dataset&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speech
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
All right. So with that business out of the way, we want to get to it. And weâ€™ll start this evening with Senator Elizabeth Warren.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Senator, good evening to you.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thank you. Good to be here.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
You have many plans - free college, free child care, government health care, cancellation of student debt, new taxes, new regulations, the breakup of major corporations. But this comes at a time when 71 percent of Americans say the economy is doing well, including 60 percent of Democrats. What do you say to those who worry this kind of significant change could be risky to the economy?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
So I think of it this way. Who is this economy really working for? Itâ€™s doing great for a thinner and thinner slice at the top. Itâ€™s doing great for giant drug companies. Itâ€™s just not doing great for people who are trying to get a prescription filled.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Itâ€™s doing great for people who want to invest in private prisons, just not for the African Americans and Latinx whose families are torn apart, whose lives are destroyed, and whose communities are ruined.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;tokenization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.1. Tokenization&lt;/h3&gt;
&lt;p&gt;As I mentioned before, I use tidytext to tokenize the transcript dataset based on sentences. For sentence tokenization, you need to set &lt;code&gt;token = &#39;sentences&#39;&lt;/code&gt; in &lt;code&gt;unnest_tokens()&lt;/code&gt; function. I think sentence tokenization is a reasonable choice because candidates might change the subject or the tone of their speech in each sentence.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debates &amp;lt;- debates %&amp;gt;%
 unnest_tokens(sentence, speech, token = &amp;#39;sentences&amp;#39;,to_lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) %&amp;gt;%
 kable() %&amp;gt;%
 kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentence
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
222
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
285
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
328
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
576
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(COMMERCIAL BREAK)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
284
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;named-entity-recognition-using-spacy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Named Entity Recognition using Spacy&lt;/h3&gt;
&lt;p&gt;Now we change to python for NER, but we need to install and import a few python libraries before starting the analysis.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import spacy
from textblob import TextBlob&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In RStudio and Rmarkdown notebooks, with the help of the &lt;code&gt;reticulate&lt;/code&gt; library, we can easily load the debate dataset in our R environment to our Python environment.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;debates = r.debates&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in in the transcript dataset there are rows for both the candidates and the moderators who asked questions from candidates. However, we are particularly interested in what the candidates said, so we only filter rows corresponding to candidates.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates = debates[(debates[&amp;#39;type&amp;#39;] == &amp;#39;Candidate&amp;#39;) &amp;amp; (pd.notnull(debates[&amp;#39;sentence&amp;#39;])) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are almost ready to extract the named entities. However, to use Spacyâ€™s NLP features such as NER, we first need to download and load a pre-trained English language model. There are &lt;a href=&#34;https://spacy.io/usage/models&#34;&gt;several English language models&lt;/a&gt; with different sizes available in Spacy. I used the largest language model available as it might be better and more accurate.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nlp = spacy.load(&amp;#39;en_core_web_lg&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spacyâ€™s NER model is trained on the &lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2013T19&#34;&gt;OntoNotes 5&lt;/a&gt; corpus, and it can detect several types of named entities, including:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;TYPE&lt;/th&gt;
&lt;th&gt;DESCRIPTION&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;PERSON&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;People, including fictional.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;NORP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Nationalities or religious or political groups.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;FAC&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Buildings, airports, highways, bridges, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;ORG&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Companies, agencies, institutions, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;GPE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Countries, cities, states.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LOC&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Non-GPE locations, mountain ranges, bodies of water.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;PRODUCT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Objects, vehicles, foods, etc. (Not services.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;EVENT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Named hurricanes, battles, wars, sports events, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;WORK_OF_ART&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Titles of books, songs, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LAW&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Named documents made into laws.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;LANGUAGE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Any named language.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;DATE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Absolute or relative dates or periods.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;TIME&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Times smaller than a day.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;PERCENT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Percentage, including â€%â€œ.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;MONEY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Monetary values, including unit.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;QUANTITY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Measurements, as of weight or distance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;ORDINAL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;â€œfirstâ€, â€œsecondâ€, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;CARDINAL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Numerals that do not fall under another type.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, there are many types of named entities, but I narrow down my analysis to just a handful of them, including &lt;code&gt;PERSON&lt;/code&gt;, &lt;code&gt;ORG&lt;/code&gt;, &lt;code&gt;GPE&lt;/code&gt;, &lt;code&gt;NORP&lt;/code&gt;, &lt;code&gt;LAW&lt;/code&gt;, and &lt;code&gt;LOC&lt;/code&gt;.
The named entity labels are stored in &lt;code&gt;label_&lt;/code&gt; attribute. To do so, we need to create &lt;code&gt;Doc&lt;/code&gt; object using &lt;code&gt;nlp()&lt;/code&gt; method. When we call &lt;code&gt;nlp()&lt;/code&gt; on the input text, spacy uses the language model to tokenize the document first. Then, spacy applies a tagger, parser, and named entity recognizer steps as its processing pipelineâ€™s next components. The named entities can be accessed by &lt;code&gt;ents&lt;/code&gt; attribute of the document object.&lt;/p&gt;
&lt;p&gt;If you are interested to learn more about Spacy and how it works, I have provided some links at the end of this post.&lt;/p&gt;
&lt;p&gt;I define a python function that iterates over all named entities and see to which class of named entities (by default &lt;code&gt;PERSON&lt;/code&gt;) they belong. I apply this function to the transcript column in the original dataset and store each extracted type of entity as a separate column.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def extract_entities_delim(text,type_ent = &amp;#39;PERSON&amp;#39;):
  ent_text = &amp;#39;&amp;#39;
  doc = nlp(text)
  for e in doc.ents:
    if e.label_ == type_ent:
      ent_text = e.text+ &amp;#39;;&amp;#39; + ent_text 
  return ent_text&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates[&amp;#39;PERSON&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x))
candidates[&amp;#39;ORG&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;ORG&amp;#39;))
candidates[&amp;#39;GPE&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;GPE&amp;#39;))
candidates[&amp;#39;NORP&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;NORP&amp;#39;))
candidates[&amp;#39;LAW&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;LAW&amp;#39;))
candidates[&amp;#39;LOC&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;LOC&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;Next, I use TextBlob to compute each sentenceâ€™s sentiment and store its polarity score in a separate column called &lt;code&gt;polarity_sentiment&lt;/code&gt; (TextBlob also returns a &lt;code&gt;subjectivity&lt;/code&gt; score, but for simplicity, I will not use this score in my analysis). The polarity sentiment score is a value between -1 and 1. If the value is larger than 0, it means that the sentence has a positive sentiment. On the other hand, if the returned value is smaller than 0, it indicates that the sentenceâ€™s sentiment is negative.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def polarity_sentiment(text):
  blob = TextBlob(text)
  return blob.sentiment.polarity
  
candidates[&amp;#39;polarity_sentiment&amp;#39;] = candidates.sentence.apply(lambda x:polarity_sentiment(x))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5.Network Visualization&lt;/h2&gt;
&lt;p&gt;A network (graph) can nicely represent how candidates mentioned individuals and entities in their speeches. We have two types of nodes in this network:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The first set of nodes represent candidates on the debate stage (&lt;strong&gt;from nodes)&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The second set of nodes represent named entities (including the name of candidates themselves) that the candidates referred to in their speeches (&lt;strong&gt;to&lt;/strong&gt; &lt;strong&gt;nodes&lt;/strong&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If a candidate mentions a named entity in his/her speech, we connect the candidate node and the named entity node via our networkâ€™s edge. It is also fair to assume that the candidate-entity network should be weighted because candidates tend to place a varying level of importance on different issues, topics, and people (named entities).&lt;/p&gt;
&lt;p&gt;We have two options for specifying weights for edges in the network:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can use the number of times that a candidate mentioned a named entity in his/her speech. This shows how much a named entity was important to that candidate.&lt;/li&gt;
&lt;li&gt;We can group by candidates and named entities and compute their average sentiment score. By doing so, we can measure how each candidate described these named entities. However, this approach might not be as accurate as we want.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having said that, it is time to go back to R and visualize the network of candidates and named entities using the &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;&lt;code&gt;ggraph&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;tidygraph&lt;/code&gt; libraries. For each class of named entities, I use &lt;code&gt;as_tbl_graph()&lt;/code&gt; function, create a unique graph table dataset, and visualize the network.&lt;/p&gt;
&lt;p&gt;So, let us load the sentiment-entity dataset that I created in the Python environment to the R environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates &amp;lt;- py$candidates&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(candidates) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
X1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentence
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PERSON
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
ORG
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
GPE
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
NORP
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
LAW
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
LOC
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
polarity_sentiment
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
370
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Well, first, the economy.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
371
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
We know that not everyone is sharing in this prosperity.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
372
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
And Donald Trump just sits in the White House and gloats about whatâ€™s going on, when you have so many people that are having trouble affording college and having trouble affording their premiums.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Donald Trump;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
373
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
So I do get concerned about paying for college for rich kids.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.375
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
374
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I do.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
375
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
But I think my plan is a good one.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.700
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;the-candidateperson-network&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5.1 The Candidate/Person Network&lt;/h4&gt;
&lt;p&gt;First, I will visualize the candidate/person network. However, I should remind you that in the beginning of the democratic primary, many democratic candidates were competing against each other in the race and on the debate stage. If I were to visualize every individual that each candidate had ever in the network, the results would become unreadable. So, just like my last blog post, I selected a few democratic candidates to show my analysis.&lt;/p&gt;
&lt;p&gt;Furthermore, I will only highlight nodes corresponding to the top 6 democratic candidates and other interesting individuals, including Donald Trump and Barack Obama.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interesting_individuals &amp;lt;-
 c(
  &amp;quot;Bernie Sanders&amp;quot; ,
  &amp;quot;Elizabeth Warren&amp;quot; ,
  &amp;quot;Mike Bloomberg&amp;quot;  ,
  &amp;quot;Pete Buttigieg&amp;quot; ,
  &amp;quot;Amy Klobuchar&amp;quot; ,
  &amp;quot;Joe Biden&amp;quot;,
  &amp;#39;Donald Trump&amp;#39;,
  &amp;#39;Barack Obama&amp;#39;
 )



custom_palette &amp;lt;-
  c(
    &amp;#39;Mike Bloomberg&amp;#39; = &amp;#39;#EDC948&amp;#39;,
    &amp;#39;Amy Klobuchar&amp;#39; = &amp;#39;#59A14F&amp;#39; ,
    &amp;#39;Joe Biden&amp;#39; = &amp;#39;#E15759&amp;#39;,
    &amp;#39;Pete Buttigieg&amp;#39; = &amp;#39;#B07AA1&amp;#39;,
    &amp;#39;Elizabeth Warren&amp;#39; =  &amp;#39;#F28E2B&amp;#39;,
    &amp;#39;Bernie Sanders&amp;#39; =  &amp;#39;#4E79A7&amp;#39; ,
    &amp;#39;Donald Trump&amp;#39; = &amp;#39;#BC3908&amp;#39;,
    &amp;#39;Barack Obama&amp;#39; = &amp;#39;#00afb9&amp;#39;,
    &amp;#39;Others&amp;#39; = &amp;#39;#540b0e&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persons_graph_table &amp;lt;- candidates %&amp;gt;%
 separate_rows(PERSON, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
 filter(speaker %in% interesting_individuals,PERSON != &amp;#39;&amp;#39;, debate %in% c(8, 9, 10)) %&amp;gt;%
 mutate(from = speaker, to = PERSON) %&amp;gt;%
 group_by(from, to) %&amp;gt;%
 summarize(n_mentions = n(),
      mean_sent = mean(polarity_sentiment),
      sent =case_when(mean_sent &amp;lt; -0.01 ~ &amp;#39;Negative&amp;#39;,
               mean_sent &amp;gt; 0.01 ~ &amp;#39;Positive&amp;#39;,
              TRUE ~ &amp;#39;Neutral&amp;#39; )
               ) %&amp;gt;%
 ungroup() %&amp;gt;%
 as_tbl_graph() %&amp;gt;%
 mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edge_cols &amp;lt;- c(&amp;#39;#e63946&amp;#39;,&amp;#39;#f1faee&amp;#39;,&amp;#39;#457B9D&amp;#39;)
ggraph(persons_graph_table, layout = &amp;#39;kk&amp;#39;) + 
  geom_edge_link(aes(edge_width = n_mentions,colour = sent )) +
  geom_node_point(aes(color = interesting_individuals ),size = 5) + 
  geom_node_label(aes(label = name,color = interesting_individuals),repel = TRUE,size= 8) + 
 scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Who Mentioned Whom in the 2020 Democratic Debates?&amp;#39;) +
  scale_edge_colour_manual(values = edge_cols) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look at the graph carefully, you will notice three issues with this network. First of all, the sentiment scores do not necessarily indicate how a candidate thinks about that person. For instance, Bernie Sanders and Khashoggiâ€™s edge is red (i.e.Â negative sentiment), but Bernie Sanders did not talk negatively about Khashoggi at all but rather how he was murdered. Secondly, there are several nodes in the network that belong to the same individual. For example, Bernie Sanders tends to address other candidates by their first names, but other (younger) candidates usually use the last name to address each other.&lt;/p&gt;
&lt;p&gt;The third issue is that some nodes do not represent a person. The transcript dataset is full of errors, and many names are misspelled. Although Spacy is a very powerful library for NER, sometimes it gives us wrong results, and its detected named entities are not always correct. For this reason, we also need to perform a post-processing step in which we remove some incorrectly spelled words or replaced them with their correct forms. I found two ways to deal with these issues: 1. or we can use a name matching algorithm to match the partial names with its full name. This approach can be challenging because we need to have a list of all possible full names, which is only available for the candidates.2. We can manually find undesirable names and replace them with what we want.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-candidate-names&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5.1 Matching candidate names&lt;/h4&gt;
&lt;p&gt;A python library called &lt;code&gt;fuzzywuzzy&lt;/code&gt;can help us match two strings based on different similarity criteria. However, before using this library, I transform the original dataset into a long dataframe where each row belongs to a pair of candidate-person (from-to), and I move it back to our python environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- candidates %&amp;gt;%
 filter(PERSON != &amp;#39;&amp;#39;) %&amp;gt;%
 separate_rows(PERSON, sep = &amp;#39;;&amp;#39;) 
head(candidates_long)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-tibble-6-x-16&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A tibble: 6 x 16&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt; X1 speaker background type  gender debate   day order sentence PERSON ORG  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
1 372 Amy Kl~ NA Cand~ female 1 1 11 And Don~ â€œDona~ &lt;NA&gt;
2 372 Amy Kl~ NA Cand~ female 1 1 11 And Don~â€&#34; &lt;NA&gt;
3 384 Amy Kl~ NA Cand~ female 1 1 99 Itâ€™s so~ â€œBara~ &lt;NA&gt;
4 384 Amy Kl~ NA Cand~ female 1 1 99 Itâ€™s so~â€&#34; &lt;NA&gt;
5 422 Amy Kl~ NA Cand~ female 1 1 329 But the~ â€œDona~ &lt;NA&gt;
6 422 Amy Kl~ NA Cand~ female 1 1 329 But the~â€&#34; &lt;NA&gt;
# â€¦ with 5 more variables: GPE &lt;chr&gt;, NORP &lt;chr&gt;, LAW &lt;chr&gt;, LOC &lt;chr&gt;,
# polarity_sentiment &lt;dbl&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates_long = r.candidates_long&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Letâ€™s look at the full names of democratic candidates.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
candidate_lists = pd.unique(candidates_long.speaker)
print(candidate_lists)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I define a python function called &lt;code&gt;match_names&lt;/code&gt;that uses &lt;code&gt;process.extractOne&lt;/code&gt; function to select the first matched named entity with at least 80 percent similarity to a candidateâ€™s full name. The matched names are stored in a separate column called &lt;code&gt;dem_candidate_full_name&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from fuzzywuzzy import fuzz 
from fuzzywuzzy import process &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def match_names(name):
  try:
    return process.extractOne(name, candidate_lists,score_cutoff = 80)[0] 
  except:
    return None

candidates_long[&amp;#39;dem_candidate_full_name&amp;#39;] = candidates_long.PERSON.apply(lambda x: match_names(x) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can return to R and continue our analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- py$candidates_long
glimpse(candidates_long)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a-word-of-caution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A word of caution&lt;/h4&gt;
&lt;p&gt;We need to be very careful with the results of the name-matching algorithm. There are too many politicians with the name â€˜Johnâ€™ and a John might refer to â€œJohn McCainâ€ or â€œJohn Boltonâ€ not the candidate â€œJohn Hickenlooperâ€. So, as a post-processing step, I manually explore the dataset to correct the few mistakes that the matching algorithm had made.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- candidates_long %&amp;gt;%
  mutate(PERSON = if_else(
    !is.na(dem_candidate_full_name),
    dem_candidate_full_name,
    PERSON),
  PERSON = case_when(PERSON == &amp;#39;John Hickenlooper&amp;#39; &amp;amp; str_detect(sentence,&amp;#39;McCain&amp;#39;) ~ &amp;#39;John McCain&amp;#39;,
                      PERSON == &amp;#39;John Hickenlooper&amp;#39; &amp;amp; str_detect(sentence,&amp;#39;Bolton&amp;#39;) ~ &amp;#39;John Bolton&amp;#39;,
                      PERSON == &amp;#39;John Delaney&amp;#39; &amp;amp; speaker == &amp;#39;Joe Biden&amp;#39; ~&amp;#39;John McCain&amp;#39;,
                      PERSON == &amp;#39;John Delaney&amp;#39; &amp;amp; speaker == &amp;#39;   Amy Klobuchar&amp;#39; ~&amp;#39;John McCain&amp;#39;,
                      TRUE ~  PERSON)
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-name-correction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.2 Manual name correction&lt;/h3&gt;
&lt;p&gt;We have a better dataset now, but there are still a lot of inaccurate named entities or inconsistencies in the dataset. Letâ€™s start by removing named entities that do not correspond with a real person.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_person &amp;lt;-
  c(
    &amp;#39;y adema&amp;#39; ,
    &amp;#39;Appalachia&amp;#39; ,
    &amp;#39;AUMF&amp;#39; ,
    &amp;#39;bias&amp;#39;,
    &amp;#39;nondisclosur&amp;#39; ,
    &amp;#39;Mathew 25&amp;#39;,
    &amp;#39;Idlib&amp;#39;,
    &amp;#39;ye&amp;#39;,
    &amp;#39;Everytown&amp;#39;,
    &amp;#39;Kurd&amp;#39;,
    &amp;#39;Roe V.&amp;#39;,
    &amp;#39;Wade&amp;#39;,
    &amp;#39;Trumpism&amp;#39;,
    &amp;#39;Casey&amp;#39;,
    &amp;#39;brown&amp;#39;,
    &amp;#39;Grandpa&amp;#39;,
    &amp;#39;Dad&amp;#39;,
    &amp;quot;Josh&amp;quot;,
    &amp;#39;Uighurs&amp;#39;,
    &amp;#39;Roe&amp;#39;,
    &amp;#39;PolitiFact&amp;#39;,
    &amp;#39;Latinx&amp;#39;,
    &amp;#39;Brady&amp;#39;,
    &amp;#39;pre-K.&amp;#39;,
    &amp;#39;Brady Bill&amp;#39;,
    &amp;#39;pro-Israel&amp;#39;,
    &amp;#39;ho&amp;#39;,
    &amp;#39;Dreamer&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have another problem left. Some individuals were mentioned in different ways, and we have several nodes for them in the graph. To solve this issue, I use &lt;code&gt;str_detect&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt; package to manually modify them names. I must say this was the most tedious and time-consuming part of my analysis!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persons_graph_table &amp;lt;-  candidates_long %&amp;gt;%
  filter(speaker %in% interesting_individuals,
         !PERSON %in% non_person,
         nchar(PERSON)&amp;gt;1) %&amp;gt;%
  dplyr::rowwise() %&amp;gt;%
  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %&amp;gt;%
  mutate(from = speaker, to = PERSON) %&amp;gt;%
  mutate(
    to = case_when(
      to %in% c(
        &amp;#39;Donald&amp;#39;,
        &amp;#39;Donald Trump&amp;#39;,
        &amp;#39;Donald trump&amp;#39;,
        &amp;#39;Trump&amp;#39;,
        &amp;#39;President Trump&amp;#39;,
        &amp;quot;Donald Trump&amp;#39;s&amp;quot;
      ) ~ &amp;#39;Donald Trump&amp;#39;,
      to %in% c(&amp;#39;Hillar&amp;#39;,
                &amp;#39;Clinton&amp;#39;,
                &amp;#39;Hillary&amp;#39;) ~ &amp;#39;Hillary Clinton&amp;#39;,
      to %in% c(&amp;#39;Obama&amp;#39;,
                &amp;#39;Barack&amp;#39;) ~ &amp;#39;Barack Obama&amp;#39;,
      str_detect(to, &amp;#39;Trump&amp;#39;) ~ &amp;#39;Donald Trump&amp;#39;,
      str_detect(to, &amp;#39;Vind&amp;#39;) ~ &amp;#39;Vindman&amp;#39;,
      str_detect(to, &amp;#39;Assad&amp;#39;) ~ &amp;#39;Assad&amp;#39;,
      str_detect(to, &amp;#39;McCarthy&amp;#39;) ~ &amp;#39;McCarthy&amp;#39;,
      str_detect(to, &amp;#39;Trudeau&amp;#39;) ~ &amp;#39;Justin Trudeau&amp;#39;,
      str_detect(to, &amp;#39;Bannon&amp;#39;) ~ &amp;#39;Steve Bannon&amp;#39;,
      str_detect(to, &amp;#39;Netanyahu&amp;#39;) ~ &amp;#39;Netanyahu&amp;#39;,
      str_detect(to, &amp;#39;Martin Luther&amp;#39;) ~ &amp;#39;Martin Luther King&amp;#39;,
      str_detect(to, &amp;#39;Mandela&amp;#39;) ~ &amp;#39;Mandela&amp;#39;,
      str_detect(to, &amp;#39;Xi&amp;#39;) ~ &amp;#39;Xi Jinping&amp;#39;,
      str_detect(to, &amp;#39;Putin&amp;#39;) ~ &amp;#39;Putin&amp;#39;,
      str_detect(to, &amp;#39;Mitch&amp;#39;) ~ &amp;#39;Mitch Mcconnell&amp;#39;,
      str_detect(to, &amp;#39;Lindsey&amp;#39;) ~ &amp;#39;Lindsey Graham&amp;#39;,
      str_detect(to, &amp;#39;Romney&amp;#39;) ~ &amp;#39;Mitt Romney&amp;#39;,
      str_detect(to, &amp;#39;George&amp;#39;) ~ &amp;#39;George Bush&amp;#39;,
      str_detect(to, &amp;#39;Bush&amp;#39;) ~ &amp;#39;George Bush&amp;#39;,
      str_detect(to, &amp;#39;Turner&amp;#39;) ~ &amp;#39;Nina Turner&amp;#39;,
      str_detect(to, &amp;#39;Clyburn&amp;#39;) ~ &amp;#39;Jim Clyburn&amp;#39;,
      str_detect(to, &amp;#39;Cheney&amp;#39;) ~ &amp;#39;Dick Cheney&amp;#39;,
      str_detect(to, &amp;#39;Shaheen&amp;#39;) ~ &amp;#39;Jeanne Shaheen&amp;#39;,
      str_detect(to, &amp;#39;Hart&amp;#39;) ~ &amp;#39;Quentin Hart&amp;#39;,
      str_detect(to, &amp;#39;Cokie&amp;#39;) ~ &amp;#39;Cokie Roberts&amp;#39;,
      str_detect(to, &amp;#39;Kelly&amp;#39;) ~ &amp;#39;Laura Kelly&amp;#39;,
      str_detect(to, &amp;#39;Berry&amp;#39;) ~ &amp;#39;Seth Berry&amp;#39;,
      str_detect(to, &amp;#39;Grassley&amp;#39;) ~ &amp;#39;Chuck Grassley&amp;#39;,
      str_detect(to, &amp;#39;Tommy&amp;#39;) ~ &amp;#39;Tom Steyer&amp;#39;,
      str_detect(to, &amp;#39;Pelosi&amp;#39;) ~ &amp;#39;Nancy Pelosi&amp;#39;,
      str_detect(to, &amp;#39;Kim&amp;#39;) ~ &amp;#39;Kim Jong-un&amp;#39;,
      str_detect(to, &amp;#39;Pence&amp;#39;) ~ &amp;#39;Mike Pence&amp;#39;,
      str_detect(to, &amp;#39;Schatz&amp;#39;) ~ &amp;#39;Brian Schatz&amp;#39;,
      str_detect(to, &amp;#39;Gates&amp;#39;) ~  &amp;#39;Robert Gates&amp;#39;,
      str_detect(to, &amp;#39;Jill&amp;#39;) ~ &amp;#39;Jill Biden&amp;#39;,
      str_detect(to, &amp;#39;Casey Jo&amp;#39;) ~ &amp;#39;Casey Jo&amp;#39;,
      str_detect(to, &amp;#39;Franklin&amp;#39;) |
      str_detect(to, &amp;#39;FDR&amp;#39;) ~ &amp;#39;Franklin D. Roosevelt&amp;#39;,
      str_detect(to, &amp;#39;Welch&amp;#39;) ~ &amp;#39;Joseph Welch&amp;#39;,
      str_detect(to, &amp;#39;Beau&amp;#39;) ~ &amp;#39;Beau Biden&amp;#39;,
      str_detect(to, &amp;#39;Rudy Giuliani&amp;#39;) ~ &amp;#39;Rudy Giuliani&amp;#39;,
      str_detect(to, &amp;#39;Bolton&amp;#39;) ~ &amp;#39;John Bolton&amp;#39;,
      str_detect(to, &amp;#39;McCain&amp;#39;) ~ &amp;#39;John McCain&amp;#39;,
      str_detect(to, &amp;#39;Truman&amp;#39;) ~ &amp;#39;Harry Truman&amp;#39;,
      str_detect(to, &amp;#39;Dunford&amp;#39;) ~ &amp;#39;Joe Dunford&amp;#39;,
      str_detect(to, &amp;#39;Breyer&amp;#39;) ~ &amp;#39;Justice Breyer&amp;#39;,
      str_detect(to, &amp;#39;Cindy&amp;#39;) ~ &amp;#39;Cindy McCain&amp;#39;,
      to == &amp;#39;Dick&amp;#39; ~ &amp;#39;Uncle Dick&amp;#39;,
      to == &amp;#39;Charles&amp;#39; ~ &amp;#39;Charles Fried&amp;#39;,
      to == &amp;#39;JFK&amp;#39; |
        (to == &amp;#39;Kennedy&amp;#39; &amp;amp; speaker == &amp;#39;Joe Biden&amp;#39;) ~ &amp;#39;John F. Kennedy&amp;#39;,
      to == &amp;#39;Kennedy&amp;#39; &amp;amp; speaker == &amp;#39;Amy Klobuchar&amp;#39; ~ &amp;#39;Ted Kennedy&amp;#39;,
      to %in% c(&amp;#39;Joey&amp;#39;) ~ &amp;#39;Himself&amp;#39;,
      to %in% c(
        &amp;#39;Ady&amp;#39;,
        &amp;#39;Carl&amp;#39;,
        &amp;#39;Ady Barkan&amp;#39;,
        &amp;#39;Derek&amp;#39;,
        &amp;#39;Mark&amp;#39;,
        &amp;#39;Salvador&amp;#39;,
        &amp;#39;Rachael&amp;#39;,
        &amp;#39;Nicole&amp;#39;
      ) ~ &amp;#39;American Constituents&amp;#39;,
      to %in% c(
        &amp;#39;David&amp;#39;,
        &amp;#39;Chuck&amp;#39;,
        &amp;#39;Wolf&amp;#39;,
        &amp;#39;   Wolf&amp;#39;,
        &amp;#39;Margaret&amp;#39;,
        &amp;#39;Brianne&amp;#39;,
        &amp;#39;Adam&amp;#39;,
        &amp;#39;Jake&amp;#39;,
        &amp;#39;Norah&amp;#39;,
        &amp;#39;Judy&amp;#39;,
        &amp;#39;Gayle&amp;#39;,
        &amp;#39;Dana&amp;#39;,
        &amp;#39;Jorge - it&amp;#39;,
        &amp;#39;Lester&amp;#39;,
        &amp;#39;Rachel&amp;#39;
      ) ~ &amp;#39;Moderator&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can visualize the network with modified node names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edge_cols &amp;lt;- c(&amp;#39;#e63946&amp;#39;, &amp;#39;#f1faee&amp;#39;, &amp;#39;#457B9D&amp;#39;)
ggraph(persons_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), color = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals),size = 6) +
  geom_node_label(
    aes(label = name, color = interesting_individuals),
    repel = TRUE,
    size = 8,
    label.r = 0.4,
    check_overlap = TRUE
  ) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Individuals Mentioned by Top Democratic Candidates During the Democratic Primary Debates&amp;#39;,
       subtitle = &amp;#39;This graph shows which individuals or politicians were mentioned by top 6 democratic candidates over the course of first ten priamary debates.&amp;#39;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;candidates-interaction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.3 Candidates interaction&lt;/h3&gt;
&lt;p&gt;In the last sections, I explained how the top 6 remaining candidates mentioned other individuals during their speeches on the debate stage. However, with a little bit of modification to our previous chunk of code, we can extend the analysis and investigate how all democratic candidates interacted with each other over the course of 10 debates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The name of all candidaes
all_candidates &amp;lt;- candidates_long %&amp;gt;%
  distinct(speaker) %&amp;gt;%
  pull()

candidates_graph_table &amp;lt;- candidates_long %&amp;gt;%
  filter(!is.na(dem_candidate_full_name),
         dem_candidate_full_name == PERSON) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(debate = as.factor(debate)) %&amp;gt;%
  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %&amp;gt;%
  mutate(from = speaker, to = dem_candidate_full_name) %&amp;gt;%
  group_by(from, to, debate) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph(directed = TRUE) %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;)) %&amp;gt;%
  activate(nodes) %&amp;gt;%
  mutate(bet_cent = centrality_betweenness(),
         deg_cent = centrality_degree())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preparing the circular layout for the network
# Credit to https://www.timlrx.com/2018/10/14/visualising-networks-in-asoiaf-part-ii/ for helping me with the circular layout
full_layout &amp;lt;-
  create_layout(graph = candidates_graph_table,
                layout = &amp;quot;linear&amp;quot;,
                circular = T)

xmin &amp;lt;- min(full_layout$x)
xmax &amp;lt;- max(full_layout$x)
ymin &amp;lt;- min(full_layout$y)
ymax &amp;lt;- max(full_layout$y)

ggraph(
  full_layout,
  layout = &amp;#39;manual&amp;#39;,
  x = x,
  y = y,
  circular = TRUE
) +
  geom_edge_arc(aes(edge_width = n_mentions,
                    alpha = n_mentions,),
                colour = &amp;#39;#540b0e&amp;#39;,) +
  geom_node_point(aes(color = interesting_individuals, size = deg_cent + 40)) +
  geom_node_text(
    aes(
      label = name,
      color = interesting_individuals,
      x = x * 1.15,
      y = y * 1.15,
      angle = ifelse(
        atan(-(x / y)) * (180 / pi) &amp;lt; 0,
        90 + atan(-(x / y)) * (180 / pi),
        270 + atan(-x / y) * (180 / pi)
      )
    ),
    size = 8
  ) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;The Network of Interactions Among Democratic Candidates During Democratic Primary Debates&amp;#39;,
       #subtitle = &amp;#39;This graph shows how democtratic candidates mentioned other candidates on the debate stage.&amp;#39;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;) +
  expand_limits(x = c(xmin - 0.2, xmax + 0.2),
                y = c(ymin - 0.2, ymax + 0.2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are self-explanatory and satisfying. One also can make an animation and show the network over time.&lt;/p&gt;
&lt;p&gt;Now, letâ€™s repeat the same steps and visualize the network for other types of named entities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;organization-and-companies-named-entities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.4 Organization and companies named entities&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_org &amp;lt;- c(&amp;#39;Trump&amp;#39;,&amp;#39;Vindmen&amp;#39;,&amp;#39;a New Yorker&amp;#39;,&amp;#39;Title&amp;#39;,&amp;#39;Obama&amp;#39;,&amp;quot;Donald Trump&amp;#39;s&amp;quot;,&amp;#39;Bernie&amp;#39;,&amp;#39;state&amp;#39;,&amp;#39;Court&amp;#39;,&amp;#39;Ours&amp;#39;,&amp;#39;Education&amp;#39;)
non_org_laws &amp;lt;- c(&amp;#39;Green New Deal&amp;#39;,&amp;#39;Federal Controlled Substance Act&amp;#39;)

org_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(ORG, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(ORG)) %&amp;gt;%
  mutate(ORG = str_remove_all(ORG, &amp;#39;the &amp;#39;),
         ORG = str_remove_all(ORG, &amp;#39;this &amp;#39;),) %&amp;gt;%
  filter(
    
    speaker %in% interesting_individuals,!ORG %in% non_org,!ORG %in% non_org_laws,
    debate %in% c(6, 7, 8, 9, 10),
    nchar(ORG)&amp;gt;1
  ) %&amp;gt;%
  mutate(from = speaker, to = ORG) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      to %in% c(&amp;#39;United Nations&amp;#39;,
                &amp;#39;U.N.&amp;#39;,
                &amp;#39;UN&amp;#39;) ~ &amp;#39;United Nations&amp;#39;,
      str_detect(to, &amp;#39;Department&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;State&amp;#39;) ~ &amp;#39;Department of State&amp;#39;,
      str_detect(to, &amp;#39;Department&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;Defence&amp;#39;) ~ &amp;#39;Department of State&amp;#39;,
      str_detect(to, &amp;#39;Supreme&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;Court&amp;#39;) ~ &amp;#39;Supreme Court&amp;#39;,
      str_detect(to, &amp;#39;Treasury&amp;#39;) ~ &amp;#39;Department of the Treasury&amp;#39;,
      str_detect(to, &amp;#39;Unitetd&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;State&amp;#39;) ~ &amp;#39;United State&amp;#39;,
      str_detect(to, &amp;#39;Yale&amp;#39;) ~ &amp;#39;Yale&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(org_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions,alpha=n_mentions),
    colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 7) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Organizations and Institutions Mentioned by Top Democratic Candidates During the Debates&amp;#39;,
       subtitle = &amp;#39;This plot shows which organizations and institutions were mentioned by top democratic candidates during the last 5 debates.&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-countries-and-cities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.5 The network of named entities for countries and cities&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gpe_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(GPE, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(GPE)) %&amp;gt;%
  mutate(GPE = str_remove_all(GPE, &amp;#39;the &amp;#39;),
         GPE = str_remove_all(GPE, &amp;#39;this &amp;#39;)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,
         debate %in% c(6, 7, 8, 9, 10)) %&amp;gt;%
  mutate(from = speaker, to = GPE) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(gpe_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions),
                 colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 9) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Countries, Cities and States Mentioned by Top Democratic Candidates During the Last Five Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-laws&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.6 The network of named entities for laws&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_law &amp;lt;-
  c(&amp;#39;the ZIP Code&amp;#39;, &amp;quot;&amp;quot;)

law_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(LAW, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(LAW)) %&amp;gt;%
  mutate(LAW = str_remove_all(LAW, &amp;#39;the &amp;#39;)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,!LAW %in% non_law) %&amp;gt;%
  mutate(from = speaker, to = LAW) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      str_detect(to , &amp;#39;Constitution&amp;#39;) ~ &amp;#39;Constitution&amp;#39;,
      str_detect(to , &amp;#39;Roe&amp;#39;) ~ &amp;#39;Roe V. Wade&amp;#39;,
      str_detect(to , &amp;#39;War Powers Act&amp;#39;) ~ &amp;#39;War Powers Act&amp;#39;,
      str_detect(to , &amp;#39;New START&amp;#39;) ~ &amp;#39;New START Treaty&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(law_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 7) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Laws Mentioned by Top Democratic Candidates During the First Ten Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-nationalities-religious-or-political-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.7 The network of named entities for nationalities, religious or political groups&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_norp &amp;lt;- c(&amp;#39;Coronavirus&amp;#39;, &amp;#39;&amp;#39;)
norp_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(NORP, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(NORP)) %&amp;gt;%
  mutate(NORP = singularize(NORP)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,!NORP %in% non_norp,
         debate %in% c(6, 7, 8, 9, 10)) %&amp;gt;%
  mutate(from = speaker, to = NORP) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      str_detect(to, &amp;#39;African&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;American&amp;#39;) ~ &amp;#39;African-American&amp;#39;,
      str_detect(to, &amp;#39;republican&amp;#39;) ~ &amp;#39;Republican&amp;#39;,
      str_detect(to, &amp;#39;Democrat&amp;#39;) ~ &amp;#39;Democrat&amp;#39;,
      str_detect(to, &amp;#39;Jew&amp;#39;) ~ &amp;#39;Jew&amp;#39;,
      str_detect(to, &amp;#39;Palestinian&amp;#39;) ~ &amp;#39;Palestinian&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(norp_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 10) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Nationalities, religious or Political Groups Mentioned by Top Democratic Candidates During the Last Five Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;p&gt;##Resources:
A very useful place to learn more how spacy works the spacyâ€™s online course by one of its founders and developers.
[1] &lt;a href=&#34;https://course.spacy.io/en/&#34; class=&#34;uri&#34;&gt;https://course.spacy.io/en/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;amp;t=676s&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;amp;t=676s&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is
&lt;a href=&#34;http://www.favstats.eu/post/demdebates/&#34; class=&#34;uri&#34;&gt;http://www.favstats.eu/post/demdebates/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the 2020 Democratic Party Presidential Debates - Part 1</title>
      <link>/post/2020-02-23-the-most-eloquent-democratic-candidate/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-23-the-most-eloquent-democratic-candidate/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I am not a US citizen, nor have I been to the United States, but that does not mean that I should not care about the result of the US presidential election. The outcome of the election plays an important role in my life and almost everyoneâ€™s else around the world. So, I have been following the US politics for a few years.&lt;/p&gt;
&lt;p&gt;I consider everything and every issue around me as a data science problem and an opportunity to use data science. The US presidential election is not an exception, and a few weeks ago, I was wondering how I can use data science techniques to analyze the presidential election in the U.S and the Democratic Primary elections. Luckily, a few days later, I found &lt;a href=&#34;https://github.com/favstats/demdebates2020&#34;&gt;an amazing R package&lt;/a&gt; on Twitter which contains the transcripts of speeches given by all candidates in the Democratic Partyâ€™s debates.&lt;/p&gt;
&lt;p&gt;Now that I had access to a dataset, it was time to think about how I should use it and how I can extract useful knowledge from it. There are many possibilities for studying the debatesâ€™ transcript, but I was interested in investigating 3 aspects of the debates and candidates:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Determining the most eloquent presidential candidate.&lt;/li&gt;
&lt;li&gt;Sentiment analysis of the transcripts to find out who used positive or most negative words on the stage.&lt;/li&gt;
&lt;li&gt;Network analysis of the transcripts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post, I will explain how I used R and the tidytext package to investigate the first two points. Discussing how I approached needs a much longer and separate post, so I will write about here.&lt;/p&gt;
&lt;div id=&#34;who-is-the-most-eloquent-candidate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Who is the most eloquent candidate?&lt;/h3&gt;
&lt;p&gt;Everyone agrees that Donald Trump only uses a basic English vocabulary in his speeches and tweets, and he is not the most eloquent president in the US history. But what about his future challenger from the democratic party and how skillful his opponents are with words? For this reason, I analyzed the transcripts from this point of view to determine how good they are with words.&lt;/p&gt;
&lt;p&gt;An eloquent person has gained a rich vocabulary and uses a wide range of complex words in his/her speeches. An inarticulate person has a limited vocabulary and uses simple and everyday words in his/her writings or conversations. Ideally, I think we can measure eloquence by counting the number of unique words and the number of sophisticated words that a person uses.&lt;/p&gt;
&lt;p&gt;However, I could not find a dataset of English words along with their perceived complexity. So, to measure the eloquence of the presidential candidates, I defined two other metrics that I hope can serve as an approximation to the truth:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Vocabulary size&lt;/strong&gt;: The ratio of unique words that a candidate used in his/her debate speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vocabulary complexity&lt;/strong&gt;: The ratio of stop-words (words that are very common and rarely add much value to the content). Intuitively, a lower ratio of stopword usage by a candidate shows that the candidate is more articulate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I use the &lt;code&gt;Tidytext&lt;/code&gt; library and its stopword list to compute my defined metrics. Note that there are several stopword lexicons out there, and the choice of lexicon can slightly change the outcome.&lt;/p&gt;
&lt;p&gt;Itâ€™s time to start the analysis itself, but before that I need to import a few libraries and set and customize the theme that I am going to use for visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readability)
library(syllable)

library(demdebates2020)
library(tidytext)
library(tidyverse)
library(gghighlight)
library(ggthemes)
library(kableExtra)
#set a default theme for visualization
theme_set(theme_fivethirtyeight())
#customize the default theme 
theme_update(legend.position = &amp;#39;none&amp;#39;,
             text = element_text(family = &amp;#39;Montserrat&amp;#39;),
      plot.title = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;,size = 25, margin = margin(0, 0, 20, 0)),
      axis.text.x = element_blank(),
      axis.text.y = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;,size = 15, margin = margin(0, 0, 20, 0)),
      panel.spacing = unit(2, &amp;quot;points&amp;quot;),
      axis.title.x = element_blank(),
      axis.title.y = element_blank())

custom_palette &amp;lt;-c(
    &amp;#39;Mike Bloomberg&amp;#39; = &amp;#39;#EDC948&amp;#39;,
    &amp;#39;Amy Klobuchar&amp;#39; = &amp;#39;#59A14F&amp;#39; ,
    &amp;#39;Joe Biden&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Pete Buttigieg&amp;#39; = &amp;#39;#B07AA1&amp;#39;,
    &amp;#39;Elizabeth Warren&amp;#39; =  &amp;#39;#F28E2B&amp;#39;,
    &amp;#39;Bernie Sanders&amp;#39; = &amp;#39;#E15759&amp;#39; 
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The field of the democratic primary election is full of candidates. So, for the sake of simplicity and clarity, I am going to analyze candidates that are still in the race (as of February 23rd) and were present in the last two democratic debate. It means that I will compare six democratic candidates including &lt;em&gt;Bernie Sanders&lt;/em&gt;, &lt;em&gt;Elizabeth Warren&lt;/em&gt;, &lt;em&gt;Mike Bloomberg&lt;/em&gt;, &lt;em&gt;Pete Buttigieg&lt;/em&gt;, &lt;em&gt;Amy Klobuchar&lt;/em&gt; and &lt;em&gt;Joe Biden&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;speakers &amp;lt;- debates %&amp;gt;%
  filter(!is.na(speech), type == &amp;#39;Candidate&amp;#39; ,debate == 9) %&amp;gt;%
  distinct(speaker) %&amp;gt;%
  pull(speaker)
speakers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Bernie Sanders&amp;quot;   &amp;quot;Elizabeth Warren&amp;quot; &amp;quot;Mike Bloomberg&amp;quot;   &amp;quot;Pete Buttigieg&amp;quot;  
## [5] &amp;quot;Amy Klobuchar&amp;quot;    &amp;quot;Joe Biden&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;readability_scores &amp;lt;- debates %&amp;gt;%
  filter(!is.na(speech), type == &amp;#39;Candidate&amp;#39;) %&amp;gt;%
  with(readability(speech,list(speaker,debate))) 

readability_scores %&amp;gt;% 
  filter(speaker %in% speakers) %&amp;gt;% 
  pivot_longer(Flesch_Kincaid:Average_Grade_Level,names_to = &amp;#39;readability_measure&amp;#39;,values_to = &amp;#39;value&amp;#39;) %&amp;gt;% 
  ggplot(aes(x = debate, y = value,color = speaker)) +
  geom_point(size = 6,alpha = 0.7)+
  geom_line(size = 2,alpha = 0.9) +
 scale_color_manual(values = custom_palette) +
  labs(color = &amp;#39;&amp;#39;) +
  facet_wrap(~readability_measure,scales = &amp;#39;free_y&amp;#39;) +
  theme(legend.position = &amp;#39;top&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before computing my desirable metrics, I should tokenize and transform the transcript into a &lt;em&gt;&lt;a href=&#34;https://www.tidytextmining.com/tidytext.html&#34;&gt;tidy format&lt;/a&gt;&lt;/em&gt; (one word per row). After that, I will create a logical variable called &lt;code&gt;is_stop_word&lt;/code&gt; to determine whether a word is stopword or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_vocab_df &amp;lt;- debates %&amp;gt;%
  filter(!is.na(speech), type == &amp;#39;Candidate&amp;#39;,speaker %in% speakers) %&amp;gt;%
  unnest_tokens(word, speech) %&amp;gt;%
  mutate(is_stop_word = word %in% stop_words$word) %&amp;gt;%
  group_by(speaker) %&amp;gt;%
  summarize(stop_word_ratio = sum(is_stop_word) / n(),
            vocab_size = n_distinct(word)/ n())  %&amp;gt;% 
  arrange(stop_word_ratio) 

#show the output  
head(debate_vocab_df)  %&amp;gt;% 
kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
stop_word_ratio
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
vocab_size
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bernie Sanders
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6588343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0882325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6942596
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0911883
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pete Buttigieg
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6978231
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1147498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7189251
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1027148
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Biden
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7195672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0842458
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mike Bloomberg
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7415574
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2083830
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After computing my metrics it is time to visualize them with &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-word-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stop word ratio&lt;/h3&gt;
&lt;p&gt;I will start by visualizing the stop word ratio for each candidate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_vocab_df %&amp;gt;%
  mutate(speaker = fct_reorder(speaker,stop_word_ratio,.desc =  TRUE)) %&amp;gt;% 
  ggplot(aes(x = speaker , y = stop_word_ratio,fill = speaker)) +
  geom_col(show.legend = FALSE) +
  geom_label(aes(label = round(stop_word_ratio,digits = 3)) ,size = 5) +
  coord_flip() +
  scale_fill_manual(values = custom_palette) +
  labs(title = &amp;quot;The ratio of stopwords used by Democratic canidates in the debates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that Bernie Sanders had used the lowest percentage of stopwords in his speeches. On the other hand, Mike Bloomberg had used the largest ratio of stopwords in his first and only debates so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vocabulary-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vocabulary size&lt;/h3&gt;
&lt;p&gt;The vocabulary size metric shows a different trend as Mike Bloomberg has the highest score among the rest of the candidates. Of course and as I mentioned before, Bloomberg has appeared only once on the debate stage, and it might be too soon to draw a conclusion about his eloquence.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; debate_vocab_df %&amp;gt;%
  mutate(speaker = fct_reorder(speaker,vocab_size,.desc =  FALSE)) %&amp;gt;% 
  ggplot(aes(x = speaker , y = vocab_size,fill = speaker)) +
  geom_col(show.legend = FALSE) +
  geom_label(aes(label = round(vocab_size,digits = 3) ,size = 8)) +
  coord_flip() +
  scale_fill_manual(values = custom_palette) +
  labs(title = &amp;quot;The ratio of unique Words used by Democratic candidates in the Debates&amp;quot;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, there is no outright winner in terms of language skills among Democratic candidates. Bernie Sanders had the best score in terms of vocabulary complexity, but he has the least ratio of unique words among his competitors. Also, one can argue that being eloquent might not be advantage to a candidate and win them an election ( look at the person who is the current president). Finally, I must emphasize that my metrics are rather arbitrary and should be taken with a grain of salt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment analysis&lt;/h3&gt;
&lt;p&gt;In this part of my blog post, I examine how the language used by each top candidate had changed over the course of debates. I will use the tidy text approach to measure sentiment in the text. There are four main sentiment lexicons in the tidytext library, but in this experiment I am just using the &lt;a href=&#34;https://rdrr.io/cran/textdata/man/lexicon_loughran.html&#34;&gt;Loughran lexicon&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment &amp;lt;- debates %&amp;gt;%
  filter(type == &amp;#39;Candidate&amp;#39;, is.na(background)) %&amp;gt;%
  unnest_tokens(word, speech) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;loughran&amp;quot;))

head(debate_senteneces_sentiment,3) %&amp;gt;% 
kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentiment
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
destroyed
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
corruption
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prosperity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment %&amp;gt;%
group_by(speaker,debate,sentiment)%&amp;gt;%
summarise(sentiment_score = n())%&amp;gt;%
ungroup() %&amp;gt;%
filter(speaker %in% speakers) %&amp;gt;%
ggplot(aes(x = debate,y= sentiment_score,color = speaker)) +
geom_line(size = 3,alpha = 0.8) +
  geom_point(size  = 4) +
scale_color_manual(values = custom_palette) +
  scale_x_continuous(breaks = seq(1,10),labels = seq(1,10)) +
  labs(title = &amp;#39;What kinds of Language Have the Top Deomocratic Candidates Used in the Debates?&amp;#39;,
       color = &amp;#39;&amp;#39;,
       x = &amp;#39;&amp;#39;) +
facet_wrap(sentiment ~ . ,ncol = 1,scales = &amp;#39;free&amp;#39;) +
theme(strip.text = element_text(size = 20),
      strip.background = element_rect(fill = &amp;#39;gray80&amp;#39;) ,
     legend.text = element_text(size = 15),
     title = element_text(size = 25),
     legend.position = &amp;#39;top&amp;#39;,
     axis.text.y = element_blank(),
     axis.text.x = element_text(size = 15),
     axis.ticks.y = element_blank(),
     axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An the overall sentiment score for each candidate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment %&amp;gt;%
group_by(speaker,sentiment)%&amp;gt;%
summarise(sentiment_score = n())%&amp;gt;%
ungroup() %&amp;gt;%
filter(speaker %in% speakers) %&amp;gt;%
  ggplot(aes(speaker,sentiment_score,fill = speaker)) +
  geom_col(alpha =0.8) +
scale_fill_manual(values = custom_palette) +
  coord_flip() +
  facet_wrap(~sentiment, ncol = 5) +
  theme(strip.text = element_text(size = 20),
      strip.background = element_rect(fill = &amp;#39;gray80&amp;#39;) ,
     title = element_text(size = 25),
     legend.position = &amp;#39;none&amp;#39;,
     axis.title.y = element_blank(),
     axis.title.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that negativity has been the most predominant emotion during Democratic debates (which absolutely makes sense since they want to unseat a president). Bernie Sanders used the highest number of words with negative sentiment in his remarks.&lt;/p&gt;
&lt;p&gt;Performing sentiment analysis in tidytext is straightforward and easy, but sometimes the results are not what we hope and what the candidate actually meant. So, also take these results with a grain of salt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;If you are interested to learn more about tidy text mining in r, the following links can be helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tidytextmining.com/tidytext.html&#34; class=&#34;uri&#34;&gt;https://www.tidytextmining.com/tidytext.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://education.rstudio.com/blog/2020/02/conf20-tidytext/&#34; class=&#34;uri&#34;&gt;https://education.rstudio.com/blog/2020/02/conf20-tidytext/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel for Scikit-Learn Users and Vise Versa</title>
      <link>/post/2020-02-14-tidymodel-for-scikit-learn-users-and-vise-versa/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-14-tidymodel-for-scikit-learn-users-and-vise-versa/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Advantages
There are many ways to do one thing
The output is a table which you can use as an input to everything that works with a table&lt;/p&gt;
&lt;p&gt;Disadvantages&lt;/p&gt;
&lt;p&gt;##Classification Models&lt;/p&gt;
&lt;div id=&#34;regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Models&lt;/h2&gt;
&lt;div id=&#34;making-prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Prediction&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model selection&lt;/h3&gt;
&lt;p&gt;reasonable defaults for tidymodel&lt;/p&gt;
&lt;p&gt;tidymodel by default tuning paramters are set for us. We can also specify them ourselves.&lt;/p&gt;
&lt;p&gt;you can even tune the preprocessing steps in Tidymodel.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pipelines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pipelines&lt;/h2&gt;
&lt;p&gt;pipelines are handy:
they make your code much shorter
data leakage&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unsupervised Learning&lt;/h2&gt;
&lt;div id=&#34;pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-Processing&lt;/h2&gt;
&lt;p&gt;inverse transform&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;p&gt;My preferable way&lt;/p&gt;
&lt;p&gt;Automatic machine learning&lt;/p&gt;
&lt;p&gt;parellal processing&lt;/p&gt;
&lt;p&gt;Things that are unique to Scikit-learn&lt;/p&gt;
&lt;p&gt;Things that are unique to Tidymodels&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://juliasilge.com/blog/best-hip-hop/&#34; class=&#34;uri&#34;&gt;https://juliasilge.com/blog/best-hip-hop/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Football Data</title>
      <link>/post/2020-02-12-analyzing-football-data/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-12-analyzing-football-data/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;names(html_table)&amp;lt;- col_names
names(table_wiki)
View(as_tibble(table_wiki,.name_repair = â€œminimalâ€)[1,1])&lt;/p&gt;
&lt;p&gt;t &amp;lt;- as_tibble(table_wiki,.name_repair = â€œminimalâ€)[1,1]
as.vector(t)
table_wiki[1:10]&lt;/p&gt;
&lt;p&gt;col_names &amp;lt;- url %&amp;gt;%
read_html() %&amp;gt;%
html_nodes(xpath = paste(â€™//*&lt;span class=&#34;citation&#34;&gt;[@id=&#34;stats_standard_ks_3260&#34;]&lt;/span&gt;â€™)) %&amp;gt;%
html_table(header = NA,fill = TRUE) %&amp;gt;%
as.data.frame() %&amp;gt;%
slice(1)&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Makes a Song Popular? An Explainable Machine Learning Approach</title>
      <link>/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_chunk$set(warning = FALSE, message = FALSE,eval = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tune)
library(rsample)
library(yardstick)
library(dials)
library(workflows)
library(parsnip)
library(infer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#devtools::install_github(&amp;quot;tidymodels/tidymodels&amp;quot;)
#remotes::install_github(&amp;quot;wilkelab/ggtext&amp;quot;,build = &amp;#39;binary&amp;#39;)
library(tidyverse)
library(tidymodels)
library(lubridate)
library(corrr)
library(pins)
library(genius)
library(reticulate)

spotify_songs &amp;lt;- pin(read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&amp;#39;))
head(spotify_songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- spotify_songs %&amp;gt;%  
  dplyr::rowwise() %&amp;gt;% 
  mutate(shorter_names = unlist(str_split(track_name,&amp;#39;-&amp;#39;))[1]) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- py$spotify_songs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have you ever wondered why some songs from an artist become so popular and others are just total failure?
Look at the next plot. Even The Beetles had a few not so popular songs.&lt;/p&gt;
&lt;p&gt;So what can be the recipe for popularity? Or why does a song become(un)popular? Is it solely releated to the artists that play a song? Can it be the songâ€™s audio features?&lt;/p&gt;
&lt;p&gt;##Exploratory Data Analysis&lt;/p&gt;
&lt;div id=&#34;machine-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;Another more complex way to look at this problem is to use machine learning algorithms. We can train a machine learning model to accurately predict the popularity of a song. Now if we look inside the patterns that this model learning model has learned, we might be able to find out why a song has become popular or unpopular.
In the second part of this post, I will demonstrate how I designed a machine learning workflow to predict the popularity of songs based on several audio features. Here my goal from using an ML model is not just to predict popularity but rather to figure out which factors contribute to it.&lt;/p&gt;
&lt;p&gt;However, peeking inside an ML algorithm and discovering how it makes prediction is not alwayse straighforward. Only the inner-workings of a few ML algorithms such as decision trees and linear modelsare transparent. These algorithms are very simple and might be powerful enough to model the complexities and the common knowledge is that they are not accurate. Of course you can make a decision tree fairly accurate by increasing its depth but the resulting tree would become exteremly messy and hard to understand.In addition, deeper tree are more likely to overfit.&lt;/p&gt;
&lt;p&gt;There are also more powerful and more accurate algorithms such as random forests, xgboost or deep neural networks but understanding how they make predictions is very challenging (sometimes they are called black-box models). That is translated to a widespread belief amont ML community that there is a trade-off between the accuracy and the interpretability of an ML algorithm. However, a few other researchers reject this claim and believe it is just a popular myth and you can indeed find an interpretable and accurate ML algorithm.&lt;/p&gt;
&lt;p&gt;Anyways, over the past five years a lot of methods have been proposed to some â€œapproximateâ€ how an ML algorithm predicts an outcome. Two popular methods that are widely used to interpret machine learning algorithms are LIME and SHAP.&lt;/p&gt;
&lt;p&gt;We can look inside a machine learning algorithms from two aspects:&lt;/p&gt;
&lt;p&gt;Based on what feature values, an ML algorithm has made prediction about the popularity of â€œa particularâ€ song. Local explanations
Global explanations such as feature importance scores to understand to the popularity of songs.&lt;/p&gt;
&lt;p&gt;In this part of my post, first I will train a random forest and an XGBoost model to predict song popularity and then I will discover how they make prediction using SHAP, LIME and feature importance scores. Hopefully, these patterns can help us better understand which factors might contribute to a songâ€™s popularity.&lt;/p&gt;
&lt;p&gt;To use a explainable machine learning methods for this purpose, it is important to obtain a reasonable performance on the prediction task. Otherwise, the result would be unreliable and useless.&lt;/p&gt;
&lt;p&gt;I wonâ€™t use common pre-processing steps such as normalization because random forest and XGBoost are not sensitive to non-normalized data. Also, some pre-processing steps might make the interpretation of the results less intuitive.&lt;/p&gt;
&lt;p&gt;I have mainly used scikit-learn for training ML models but recently I have become passionately interested in the Tidymodels ecosystem. So, here I have decided to use Tidymodels and its features for model development.
Workflows are similar to &lt;code&gt;pipelines&lt;/code&gt; in scikit-learn.
My designed workflow consists of the following step:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;I create a preprocessing recipe using the &lt;code&gt;recipe&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I split the input dataset into a training and testing set&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I build a random forest and an XGBoost model using the &lt;code&gt;parsnip&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I use &lt;code&gt;tune&lt;/code&gt; package and tuning the hyper-paramters of the random forest and the XGBoost model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I use&lt;code&gt;rsample&lt;/code&gt; package to perform cross-validation and train both models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I evaluate the performance of the trained models based on metrics from the &lt;code&gt;yardstick&lt;/code&gt; package and select the best model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://tidymodels.github.io/yardstick/reference/index.html&#34; class=&#34;uri&#34;&gt;https://tidymodels.github.io/yardstick/reference/index.html&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Finally, I explain the predictions of the machine learning model in hope of finding interesting patterns that might tell us something about why a song becomes popular. Not that this step is not implemented as a part of the Tidymodel workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;machine-learning-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pre-processing&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;bulding-the-ml-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bulding the ML models&lt;/h3&gt;
&lt;p&gt;first we need to specify the type of the model that we want to train and if necessary its hyper-paramters. Then we have to determine the mode of the ML task that we would like to solve. Our problem is a regression problem, so we set the mode as &lt;code&gt;regression&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;set the engine or the implementation of the model (Ranger)&lt;/p&gt;
&lt;p&gt;4.set the mode of the ML task (Regression)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs %&amp;gt;% 
 select(track_popularity, c(12:23)) %&amp;gt;% 
 correlate() %&amp;gt;% 
  network_plot(min_cor = 0.1,color = c(&amp;#39;#1a535c&amp;#39;,&amp;#39;#4ecdc4&amp;#39;,&amp;#39;#f7fff7&amp;#39;,&amp;#39;#ff6b6b&amp;#39;,&amp;#39;#ffe66d&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs %&amp;gt;% 
 ggplot(aes(track_popularity)) +
 geom_histogram(fill = &amp;#39;indianred&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shap&lt;/h2&gt;
&lt;div id=&#34;machine-learning-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#rf &amp;lt;- rand_forest(trees = 100, mode = &amp;#39;regression&amp;#39;) %&amp;gt;% 
 #set_engine(&amp;quot;randomForest&amp;quot;) %&amp;gt;% 
 #fit(Species ~. ,data = iris_training)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references-and-further-readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References and further readings&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaylinpavlik.com/classifying-songs-genres/&#34; class=&#34;uri&#34;&gt;https://www.kaylinpavlik.com/classifying-songs-genres/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://konradsemsch.netlify.com/2019/10/testing-the-tune-package-from-tidymodels-analysing-the-relationship-between-the-upsampling-ratio-and-model-performance/&#34; class=&#34;uri&#34;&gt;https://konradsemsch.netlify.com/2019/10/testing-the-tune-package-from-tidymodels-analysing-the-relationship-between-the-upsampling-ratio-and-model-performance/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Going Back to the Roots! How Much Influence Did Arabic Have on Persian Literature?</title>
      <link>/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Since the &lt;a href=&#34;https://en.wikipedia.org/wiki/Muslim_conquest_of_Persia&#34;&gt;conquest of Persia (now Iran) by the Muslim forces in the 7th century&lt;/a&gt;, Arabic culture and language have had an enormous influence on Iran and Iranians. Although Iran had never fully adapted Arabic as its primary language, the new Persian (Farsi) language is a mix of Arabic and the old Persian (Pahlavi) and almost uses the same alphabet for writing. Also, in some parts of Iran, Arabic is the daily-life language.
Over the past 100 years, a very few (narrowly-minded and mostly racist) scholars have tried to erase Arabic words from the Persian literature. Since I was a kid, I have always wanted to put my data science skills and tools to&lt;/p&gt;
&lt;p&gt;I decided to start a small project and determine how much influence Arabic has had on Persian Literature and poetry over time. Simply, my goal is to look at every word used in poems and determine whether it comes from Arabic, or it is originally a Persian word. Then I count the occurrence of each of them and compute their ratio.&lt;/p&gt;
&lt;p&gt;However, this is not an easy task for several reasons. Although determining the origin of a word is not difficult for a well-educated person, determining the root language of each word manually is not feasible. So, I tried smarter ways (but less accurate) to achieve the same goal. Like many other languages, Persian poems are different from daily written or spoken Persian, and therefore standard NLP methods are not as effective as before.&lt;/p&gt;
&lt;p&gt;Ideally, we need a complete dataset of words with Arabic roots used in Persian to solve this task. However, as far as I know, this dataset does not exist, and I must use other approaches:
1. Some rules and exceptions can be used to distinguish Persian words from Arabic words. For example, unlike Persian, Arabic does not have four letters representing â€œpâ€, â€œjâ€ such as Japan, â€œgâ€ such as game and â€œchâ€ in its alphabet. It means that any word that consists of one of these letters it is definitely a non-Arabic word. On the other hand, we do not have any letters in the Persian alphabet for representing the â€˜thâ€™ letter (and a few other letters) in Arabic. Therefore, words that consist of these letters are likely to be Arabic words.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# fa --&amp;gt; Farsi (Persian)
# ar ---&amp;gt; Arabic
# un ----&amp;gt; Unkown
def arabic_word(word):
    if &amp;#39;Ø«&amp;#39; in word:
        return &amp;#39;ar&amp;#39;
    elif &amp;#39;Ø­&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;Øµ&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    if &amp;#39;Ø¶&amp;#39; in word:
        return &amp;#39;ar&amp;#39;
    elif &amp;#39;Ø¸&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;Ø¹&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;Ø·&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;Ù‚&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;Ú˜&amp;#39; in word:
        return &amp;#39;fa&amp;#39;
    elif &amp;#39;Ú¯&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    elif &amp;#39;Ú†&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    elif &amp;#39;Ù¾&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    else:
        return &amp;#39;un&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Unfortunately, the rules mentioned above are not comprehensive, and they cannot determine the origin of many words. So, I turned to the python port of the &lt;a href=&#34;https://github.com/Mimino666/langdetect&#34;&gt;&lt;code&gt;Langdetect&lt;/code&gt;&lt;/a&gt; library for help. If the above rules can not determine the origin of a word, I will ask this library to identify the language. I should mention that langdetect can sometimes be wrong, so the final results might not be 100% accurate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I must also mention that I performed a few preprocessing steps, such as removing stopwords on the poetry corpus. A few other operations such as stemming could have been performed, but my initial assessment might not significantly change the final results.
After preprocessing, I stored all the information about the ratio of Arabic and Persian words for each poet in a separate dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lang_ratio_df &amp;lt;- read_csv(&amp;#39;lang_ratio_df.csv&amp;#39;)
head(lang_ratio_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   poet               century    ar    fa ratio period         
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          
## 1 Abusaeid Abolkheir       5  3014  8277 0.364 Khorasani Style
## 2 Ahmad Shamlou           14  8232 28862 0.285 Contemporary   
## 3 Akhavan-Sales           14  3338 14937 0.223 Contemporary   
## 4 Amir Khusrow             8 10582 41997 0.252 Iraqi Style    
## 5 Anvari                   6 29430 67188 0.438 Iraqi Style    
## 6 Artimani                10  2616  7706 0.339 Indian Style&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I visualized the ratio of words for each poet using the &lt;code&gt;ggplot&lt;/code&gt; library in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; lang_ratio_df %&amp;gt;%
    mutate(
    poet = fct_reorder(poet, ratio),

    period = factor(
      period,
      levels = c(&amp;#39;Khorasani Style&amp;#39;,&amp;#39;Iraqi Style&amp;#39;,&amp;#39;Indian Style&amp;#39;,&amp;#39;Contemporary&amp;#39; )
    )) %&amp;gt;% 
  ggplot(aes(x = poet, y = ratio , color = period)) +
  geom_point(size = 4) +
  geom_segment(aes(
    y = 0, yend = ratio, x = poet, xend = poet), size = 1) +
  geom_text(
    aes(x = poet,  y = ratio,label = scales::percent(ratio)), size = 5, nudge_y = .2,family = &amp;#39;Montserrat&amp;#39;) +
  labs( x = &amp;#39;&amp;#39;, y = &amp;#39;&amp;#39;, title = &amp;#39;The Estimated Ratio of Arabic Words Used by Famous Persion Poets&amp;#39;) +
  scale_color_tableau() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  coord_flip() +
  facet_wrap( ~ period, scales = &amp;quot;free_y&amp;quot;, ncol = 2) +
  theme_tufte() +
  theme(
    text = element_text(family = &amp;#39;Montserrat&amp;#39;),
    legend.title =  element_text(size = 20),
    axis.ticks.x = element_blank(),
    legend.text = element_text(
      size = 15,
    margin = ggplot2::margin(0, 20, 0, 0)),
    plot.title = element_text(
      face = &amp;quot;bold&amp;quot;,
      color = &amp;#39;gray&amp;#39;,
      size = 22,
      margin = ggplot2::margin(0, 20, 20, 0),
      hjust = 0.5,
      vjust = 0.5),
        strip.text = element_text(
      color = &amp;#39;gray80&amp;#39;,
      size = 18 ,
      margin = ggplot2::margin(1, 0, 1, 0)),
    legend.position = &amp;#39;none&amp;#39;,
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(size = 12, color = &amp;#39;gray&amp;#39;),
    plot.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
    panel.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
    panel.border = element_rect(fill = NA, color = NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see above, every poet used at least a sizable number of Arabic words in his/her work. Most notably, Ferdowsi wrote &lt;a href=&#34;https://en.wikipedia.org/wiki/Shahnameh&#34;&gt;Shahname (the Book of Kings)&lt;/a&gt;, which recounts the myths and legends of Persian Kings and Heroes and is the oldest piece of poetry analyzed in my experiment, also includes a considerable number of Arabic words. Other top Persian poets such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Hafez&#34;&gt;Hafez&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Saadi_Shirazi&#34;&gt;Saadi&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Rumis&#34;&gt;Rumi&lt;/a&gt; used Arabic words in almost 40%-50% of their works.&lt;/p&gt;
&lt;p&gt;It can be best shown using the following plot, which is made using the &lt;a href=&#34;https://emilhvitfeldt.github.io/ggpage/&#34;&gt;&lt;code&gt;ggpage&lt;/code&gt;&lt;/a&gt; package in R. The plot shows the distribution of words and their origins for several top Persian poets. Note that I only used a random subset of words from each poetâ€™s works and not their whole works of poetry in this plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_poets_df &amp;lt;- read_csv(&amp;#39;sample_poets.csv&amp;#39;)
head(sample_poets_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   word  lang  poet   century
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 &amp;lt;U+0628&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0645&amp;gt;   fa    Rudaki       3
## 2 &amp;lt;U+0627&amp;gt;&amp;lt;U+0634&amp;gt;&amp;lt;U+06A9&amp;gt;   fa    Rudaki       3
## 3 &amp;lt;U+063A&amp;gt;&amp;lt;U+0645&amp;gt;    ar    Rudaki       3
## 4 &amp;lt;U+0647&amp;gt;&amp;lt;U+0645&amp;gt;&amp;lt;U+06CC&amp;gt;   fa    Rudaki       3
## 5 &amp;lt;U+0628&amp;gt;&amp;lt;U+0631&amp;gt;&amp;lt;U+0645&amp;gt;   fa    Rudaki       3
## 6 &amp;lt;U+0646&amp;gt;&amp;lt;U+0647&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0646&amp;gt;&amp;lt;U+06CC&amp;gt; fa    Rudaki       3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpage_df %&amp;gt;%
  mutate(poet = fct_reorder(poet, century)) %&amp;gt;%
  ggpage_plot(aes(fill = lang)) +
  
  labs(title = &amp;#39;Distribution of Persian and Arabic Words Used by Top Persian Poets&amp;#39;, fill = &amp;#39;&amp;#39;) +
  scale_fill_manual(values = plotcolors,
                    guide = &amp;#39;legend&amp;#39; ,
                    labels = c(&amp;#39;Arabic&amp;#39;,&amp;#39;Persian&amp;#39;)) +
  
  facet_wrap(~ poet, nrow = 3) +
  theme(
    strip.text = element_text(
      size = 15,
      face = &amp;quot;bold&amp;quot;,
      margin = ggplot2::margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;),
      color = &amp;#39;white&amp;#39;
    ),
        text = element_text(family = &amp;#39;Montserrat&amp;#39;),

    legend.position = &amp;#39;top&amp;#39;,
    legend.text = element_text(
      size = 15,
      margin = ggplot2::margin(10, 10, 10, 10)
    ),
    panel.spacing = unit(1, &amp;quot;points&amp;quot;),
    plot.title = element_text(
      face = &amp;quot;bold&amp;quot;,
      size = 22,
      margin = ggplot2::margin(30, 0, 30, 0),
      hjust = 0.5
    ),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = &amp;#39;#000F2B&amp;#39;),
    panel.border = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;How Much Influence Did Arabic Have on Persian Literature has been one of my questions since I started to read and study literature. Nobody had been able to answer this question, and I could not have answered it without the help of data science.&lt;/p&gt;
&lt;p&gt;My analysis shows that the Arabic language has contributed significantly to our literature and culture. The golden era of Persian poetry can be seen as a result of its integration with Arabic. Persian also made its contribution to the Arabic language and Arabic poetry. So, talking about erasing one language from the other is not helpful or wise, and I hope everyone realizes that.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Map of Spotify Songs</title>
      <link>/post/2020-02-01-the-map-of-spotify-songs/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-01-the-map-of-spotify-songs/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md&#34;&gt;4th week of the Tidy Tuesday project&lt;/a&gt;, a very interesting and fun dataset was proposed to the data science community. The dataset contains information about thousands of songs on Spotifyâ€™s platform and along with their metadata and audio features. You can download the dataset can using the following piece of code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md&#34;&gt;4th week of the Tidy Tuesday project&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&amp;#39;)
head(spotify_songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 23
##   track_id track_name track_artist track_popularity track_album_id
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         
## 1 6f807x0~ I Don&amp;#39;t C~ Ed Sheeran                 66 2oCs0DGTsRO98~
## 2 0r7CVbZ~ Memories ~ Maroon 5                   67 63rPSO264uRjW~
## 3 1z1Hg7V~ All the T~ Zara Larsson               70 1HoSmj2eLcsrR~
## 4 75Fpbth~ Call You ~ The Chainsm~               60 1nqYsOef1yKKu~
## 5 1e8PAfc~ Someone Y~ Lewis Capal~               69 7m7vv9wlQ4i0L~
## 6 7fvUMiy~ Beautiful~ Ed Sheeran                 67 2yiy9cd2QktrN~
## # ... with 18 more variables: track_album_name &amp;lt;chr&amp;gt;,
## #   track_album_release_date &amp;lt;chr&amp;gt;, playlist_name &amp;lt;chr&amp;gt;, playlist_id &amp;lt;chr&amp;gt;,
## #   playlist_genre &amp;lt;chr&amp;gt;, playlist_subgenre &amp;lt;chr&amp;gt;, danceability &amp;lt;dbl&amp;gt;,
## #   energy &amp;lt;dbl&amp;gt;, key &amp;lt;dbl&amp;gt;, loudness &amp;lt;dbl&amp;gt;, mode &amp;lt;dbl&amp;gt;, speechiness &amp;lt;dbl&amp;gt;,
## #   acousticness &amp;lt;dbl&amp;gt;, instrumentalness &amp;lt;dbl&amp;gt;, liveness &amp;lt;dbl&amp;gt;, valence &amp;lt;dbl&amp;gt;,
## #   tempo &amp;lt;dbl&amp;gt;, duration_ms &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this weekâ€™s tidy Tuesday, I decided to use a somewhat different approach from my previous submissions. Instead of focusing solely on the visualization aspect of my submissions, I tried to use other tools from the tidy model universe for machine learning model development,&lt;/p&gt;
&lt;p&gt;Each song has around 12 columns representing audio features. The Githubâ€™s page for this dataset describes these features as follows:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;73%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;variable&lt;/th&gt;
&lt;th&gt;class&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;danceability&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;energy&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;key&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = Câ™¯/Dâ™­, 2 = D, and so on. If no key was detected, the value is -1.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;loudness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;mode&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;speechiness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g.Â talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;acousticness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;instrumentalness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Predicts whether a track contains no vocals. â€œOohâ€ and â€œaahâ€ sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly â€œvocalâ€. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;liveness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;valence&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.Â happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.Â sad, depressed, angry).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;tempo&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;duration_ms&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Duration of song in milliseconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It would be very helpful to compare songs based on their audio features and have an overall picture of where each song is placed. Unfortunately, we can only visualize 2 or 3 audio features at the same time, and It is not possible to put all these features in a 2D or 3D space. So, I tried to use unsupervised machine learning to visualize songs on a 2D space by transforming their high-dimensional audio features into a more compressed form.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(workflows)
library(gghighlight)
library(hrbrthemes)
library(ggthemes)
library(lubridate)
library(reticulate)
library(ggrepel)
library(plotly)
library(uwot)


theme_update(legend.position = &amp;#39;top&amp;#39;,
   legend.text  = element_text(size = 32,color = &amp;#39;gray75&amp;#39; ),
   legend.key = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   legend.background= element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   plot.title = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;, size = 60,hjust = 0.5,vjust = 0.5,color = &amp;#39;#FFE66D&amp;#39;,margin = ggplot2::margin(40,0,0,0)),
   plot.subtitle = element_text(
   family = &amp;#39;Montserrat&amp;#39;, size = 30, hjust = 0.5),
   strip.background = element_blank(),
   plot.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   panel.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   panel.grid.major.x =element_blank(),
   panel.grid.major.y =element_blank(),
   panel.grid.minor =element_blank(),
   axis.text.x.bottom = element_blank(),
   axis.ticks.x = element_blank(), 
   axis.ticks.y = element_blank(),
   axis.text.x = element_blank(),
   axis.text.y.left = element_blank()) &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;dimensionality-reduction-and-umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dimensionality Reduction and UMAP&lt;/h2&gt;
&lt;p&gt;My initial idea was to use some clustering algorithms to cluster songs based on their audio feature and find songs that are similar to each other. Yet, it was not easy to visualize these clusters in a two-dimensional space. Of course, you can do that by using hierarchal clustering but even then, visualizing a few thousand samples (songs) seems to be impractical. So, I decided to use other unsupervised techniques to compress these high-dimensional audio features and transform them into a more compact 2D space.&lt;/p&gt;
&lt;p&gt;There are several dimensionality reduction algorithms such as PCA, t-SNE UMAP. The primary purpose of these algorithms is to give us a compressed representation of the input data, while preserving the most relevant information in the data. PCA is a linear dimensionality reduction method, while both t-SNE and UMAP are non-linear methods.&lt;/p&gt;
&lt;p&gt;In this post, I will use UMAP and t-SNE, two widely used dimensionality reduction algorithms. When the input dataset is large T-SNE becomes very slow and is not an efficient algorithm anymore. On the other hand, UMAP can handle larger datasets much more easily and quickly. Moreover, UMAP can preserve the underlying local structure present in the data, and it can also represent the global structure of the data more accurately. What do we mean by local and global structure? For example, in the song dataset, persevering local structure means that songs that belong to an artist are clustered together. Similarly, global structure means that songs belonging to more related genres (e.g., hard rock, album rock, and classic rock) will be placed in close proximity to each other on the new projection.&lt;/p&gt;
&lt;p&gt;UMAP achieves this goal by employing some advanced optimization techniques and mathematical concepts. Understanding how UMAP uses these techniques and projects the input data into a more compressed representation is not crucial, but If you are curious to know more about the theory behind UMAP and its difference with T-SNE, I recommend &lt;a href=&#34;https://pair-code.github.io/understanding-umap/&#34;&gt;this excellent blogpost&lt;/a&gt; by Andy Coenen and Adam Pearce.&lt;/p&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Preprocessing&lt;/h3&gt;
&lt;p&gt;Both UMAP and T-SNE compute a distance metric between samples. This distance metric should be meaningful and reasonable. If we do not scale the input features before feeding them to these algorithms, some features might have a stronger (unfair) influence than other features on the computation of the distance between samples. For this reason, it is necessary to normalize input features before implementing them,&lt;/p&gt;
&lt;p&gt;I create a data preprocessing recipe using the &lt;code&gt;recipe&lt;/code&gt; package, and I add a normalization step to scale the audio features. Note that since I implement an unsupervised algorithm, there is no need to split the dataset into a training and testing dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normalized_features &amp;lt;- spotify_songs %&amp;gt;%
 recipe() %&amp;gt;% 
 step_normalize( danceability,
  energy,
  key,
  loudness,
  mode,
  speechiness,
  acousticness,
  instrumentalness,
  liveness,
  valence,
  tempo,
  duration_ms) %&amp;gt;% 
 prep() %&amp;gt;% 
 juice()

head(normalized_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 23
##   track_id track_name track_artist track_popularity track_album_id
##   &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;         
## 1 6f807x0~ I Don&amp;#39;t C~ Ed Sheeran                 66 2oCs0DGTsRO98~
## 2 0r7CVbZ~ Memories ~ Maroon 5                   67 63rPSO264uRjW~
## 3 1z1Hg7V~ All the T~ Zara Larsson               70 1HoSmj2eLcsrR~
## 4 75Fpbth~ Call You ~ The Chainsm~               60 1nqYsOef1yKKu~
## 5 1e8PAfc~ Someone Y~ Lewis Capal~               69 7m7vv9wlQ4i0L~
## 6 7fvUMiy~ Beautiful~ Ed Sheeran                 67 2yiy9cd2QktrN~
## # ... with 18 more variables: track_album_name &amp;lt;fct&amp;gt;,
## #   track_album_release_date &amp;lt;fct&amp;gt;, playlist_name &amp;lt;fct&amp;gt;, playlist_id &amp;lt;fct&amp;gt;,
## #   playlist_genre &amp;lt;fct&amp;gt;, playlist_subgenre &amp;lt;fct&amp;gt;, danceability &amp;lt;dbl&amp;gt;,
## #   energy &amp;lt;dbl&amp;gt;, key &amp;lt;dbl&amp;gt;, loudness &amp;lt;dbl&amp;gt;, mode &amp;lt;dbl&amp;gt;, speechiness &amp;lt;dbl&amp;gt;,
## #   acousticness &amp;lt;dbl&amp;gt;, instrumentalness &amp;lt;dbl&amp;gt;, liveness &amp;lt;dbl&amp;gt;, valence &amp;lt;dbl&amp;gt;,
## #   tempo &amp;lt;dbl&amp;gt;, duration_ms &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;t-sne&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;T-SNE&lt;/h2&gt;
&lt;p&gt;Both UMAP and T-SNE have several hyper-parameters that can influence the resulting embedding output. However, T-SNE is a notoriously slow algorithm and the opportunity for trial and error with different sets of hyper-parameter values are limited. For the sake of simplicity, I stick to default settings for hyper-parameter in T-SNE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
tsne_embedding &amp;lt;- normalized_features %&amp;gt;%
 select(c(12:23)) %&amp;gt;%
 Rtsne(check_duplicates = FALSE)

tsne_embeddings &amp;lt;- spotify_songs %&amp;gt;% 
 select(-c(12:22)) %&amp;gt;% 
 bind_cols(tsne_embedding$Y %&amp;gt;% as_tibble()) %&amp;gt;% = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
 dplyr::rename(tsne_1 = V1, tsne_2 = V2) %&amp;gt;% &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though I managed to transform a high dimensional dataset into a 2D space, still it was very challenging to visualize every song and every artists all at once. So, I just select a few famous artists that I have heard about. Each artist in this list more or less represents at least a genre of music and it can perfectly show that an artist (or a band) made several genres of music and how difficult our task is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_artists &amp;lt;- c(&amp;#39;Queen&amp;#39;,&amp;#39;Drake&amp;#39;,&amp;#39;Rihanna&amp;#39;,&amp;#39;Taylor Swift&amp;#39;,&amp;#39;Eminem&amp;#39;,&amp;#39;Snoop Dogg&amp;#39;,&amp;#39;Katy Perry&amp;#39;,&amp;#39;The Beatles&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings &amp;lt;- tsne_embeddings%&amp;gt;% 
 mutate(
  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
  popular_tracks_selected_artist = if_else(
   track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 65,shorter_names, NULL )) %&amp;gt;%
 distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings %&amp;gt;%
 ggplot(aes(x = tsne_1, y = tsne_2 ,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    title = &amp;#39;The Map of Spotify Songs Based on T-SNE Algorithm\n&amp;#39;,
    subtitle = &amp;#39;Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n&amp;#39;,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see in this projection, songs that belong to the same artists are placed close to each other. It seems that T-SNE is able to preserve the local topological structure of songs. Now I will look at how T-SNE distinguishes different genres of music.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings %&amp;gt;%
 ggplot(aes(x = tsne_1, y = tsne_2 ,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    title = &amp;#39;The Map of Spotify Songs Based on T-SNE Algorithm\n&amp;#39;,
    subtitle = &amp;#39;Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n&amp;#39;,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMAP&lt;/h2&gt;
&lt;p&gt;Just like t-SNE, UMAP is a dimensionality reduction algorithm but it is much more computationally efficient and faster that t-SNE. The UMAP algorithm was &lt;a href=&#34;https://github.com/lmcinnes/umap&#34;&gt;originally implemented in Python&lt;/a&gt;. But there are also several libraries in R such as &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;umapr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/umap/vignettes/umap.html&#34;&gt;umap&lt;/a&gt; and &lt;a href=&#34;https://github.com/jlmelville/uwot&#34;&gt;uwot&lt;/a&gt; that also provide an implementation of the UMAP algorithm. &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;&lt;code&gt;umapr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/umap/vignettes/umap.html&#34;&gt;&lt;code&gt;umap&lt;/code&gt;&lt;/a&gt; use the &lt;a href=&#34;https://cran.r-project.org/web/packages/reticulate/index.html&#34;&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; package and provide a wrapper function around the original &lt;code&gt;umap-learn&lt;/code&gt; python library. Also, &lt;code&gt;umap&lt;/code&gt; and &lt;code&gt;uwot&lt;/code&gt; library have their own R implementation and they do not require the python package to be installed beforehand. For this specific experiment, I will use the &lt;code&gt;uwot&lt;/code&gt; library.&lt;/p&gt;
&lt;p&gt;we can change and tune a few hyper-parameters in the implementation of UMAP in the uwot library, These hyperparameter can change the embedding outcome. However, there are two hyper-parameters that have a much more important impact on the structure of the low-dimensional representation:&lt;code&gt;n_neighbors&lt;/code&gt;, &lt;code&gt;min_dist&lt;/code&gt; and &lt;code&gt;metric&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_neighbors&lt;/code&gt; determines the number of nearest neighbor data points that we use to compute and construct the embedding.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_dist&lt;/code&gt; controls the minimum distance between data points in the low dimensional space (embedding). That means a low value of &lt;code&gt;min_dist&lt;/code&gt; results in a more compact clusters of data points. On the other hand, with larger values of &lt;code&gt;min_dist&lt;/code&gt;, the projection will be less compact and tend to preserve the global structure.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metric&lt;/code&gt;: We can use different metrics (e.g.. cosine or Euclidean) to compute the distance between data points and to find the nearest neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The choice of hyperparameter values can be very important for the final projection. However,choosing the right set of hyper-parameters in UMAP is extremely difficult because UMAP is an unsupervised algorithm and we do not have a baseline to evaluate its performance. Fortunately, UMAP is vary fast and scalable algorithm. It means that we can run UMAP with different hyperparameter settings and decide which set of values best serves our purpose.&lt;/p&gt;
&lt;p&gt;My main goal from running UMAP is to visualize songs and their audio features on a 2D space and I can use a trick to decrease UMAPâ€™s computation time. According to uwotâ€™s documentation, if my only purpose is visualization, I can set the value of &lt;code&gt;fast_sgd&lt;/code&gt; hyper-parameter to &lt;code&gt;TRUE&lt;/code&gt; to speed up UMAPâ€™s convergence and running time.
Next, I create a grid of values for these three hyper-parameters and each time I will learn a new UMAP embedding based on different combinations of these values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_neighbors &amp;lt;- c(15,30,50,100,150)
min_distance &amp;lt;- c( 0.001, 0.003, 0.009,0.03,0.09)
metrics &amp;lt;- c(&amp;quot;euclidean&amp;quot; ,&amp;quot;cosine&amp;quot;,&amp;quot;hamming&amp;quot;)

#make a copy of the dataset
spotify_songs_emb &amp;lt;- spotify_songs

for (nn in n_neighbors) {
 for (md in min_distance) {
  for (metric in metrics) {
  umap_embedding &amp;lt;- normalized_features %&amp;gt;%
  select(c(12:23)) %&amp;gt;%
  umap(n_neighbors = nn,min_dist = md,metric = metric, fast_sgd = TRUE)
  spotify_songs_emb &amp;lt;- spotify_songs_emb %&amp;gt;% 
  bind_cols(umap_embedding[,1]%&amp;gt;% as_tibble() ) %&amp;gt;% 
  bind_cols(umap_embedding[,2] %&amp;gt;% as_tibble() )
  names(spotify_songs_emb)[names(spotify_songs_emb) == &amp;#39;value&amp;#39; ] = paste(&amp;#39;nn_&amp;#39;,nn,&amp;#39;md_&amp;#39;,md,&amp;#39;metric&amp;#39;,metric,&amp;#39;1&amp;#39;,sep = &amp;#39;.&amp;#39;)
  names(spotify_songs_emb)[names(spotify_songs_emb) == &amp;#39;value1&amp;#39; ] = paste(&amp;#39;nn_&amp;#39;,nn,&amp;#39;md_&amp;#39;,md,&amp;#39;metric&amp;#39;,metric,&amp;#39;2&amp;#39;,sep = &amp;#39;.&amp;#39;)
  }
 }
 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like what I did for T-SNE, I will focus on the same list of artists.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb &amp;lt;- spotify_songs_emb%&amp;gt;% 
 mutate(
  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
  point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
  popular_tracks_selected_artist = if_else(
   track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 65,shorter_names, NULL )) %&amp;gt;%
 distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, it was time to plot the results of UMAP embeddings using &lt;code&gt;ggplot&lt;/code&gt; and &lt;a href=&#34;https://github.com/yutannihilation/gghighlight&#34;&gt;&lt;code&gt;gghighlight&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setting-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 50&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.50.md_.0.09.metric.euclidean.1, y = nn_.50.md_.0.09.metric.euclidean.2 ,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 50&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Hamming&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.50.md_.0.09.metric.hamming.1, y = nn_.50.md_.0.09.metric.hamming.2,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 150&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.150.md_.0.09.metric.euclidean.1, y = nn_.150.md_.0.09.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 15&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.15.md_.0.09.metric.euclidean.1, y = nn_.15.md_.0.09.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 5&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 150&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.001&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.150.md_.0.001.metric.euclidean.1, y = nn_.150.md_.0.001.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-6&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 6&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 15&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Hamming&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.15.md_.0.09.metric.hamming.1, y = nn_.15.md_.0.09.metric.hamming.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the most part, both t-SNE and UMAP place songs from the same artists or similar songs close to each other. The UMAP embeddings with Euclidean distance are somehow similar to a real map. In the UMAP representation of the songs, we can see isolated clusters of songs. However, in t-SNE representation, no clear and separate cluster of points can be seen.
We can observe that the most influential hyper-parameter seems to be the distance metric. Additionally, when we decrease the value of &lt;code&gt;min_dist&lt;/code&gt;, the projection becomes less compact, and the global structure emerges. However, we also see that sometimes music genres are not well-separated as we would like them to be. We should take into account that audio features might not be enough to distinguish between genres of music, and We need to incorporate other aspects of songs such as lyrics to differentiate between genres. For instance, Kaylin Pavlik, in her blogpost explained how she based on similar audio features, trained several machine learning models to classify songs into six main categories (EDM, Latin, Pop, R&amp;amp;B, Rap, &amp;amp; Rock). Her best model achieved an accuracy of 54.3%, which is a decent performance but not super accurate. I also tuned and trained a few machine learning models on this dataset, but I could not achieve higher performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Supervised UMAP&lt;/h2&gt;
&lt;p&gt;UMAP is an unsupervised dimensionality reduction algorithm, but we can also feed target labels to UMAP and make it a &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/supervised.html&#34;&gt;supervised algorithm&lt;/a&gt; by specifying the target variable. To make this happen in UWOT, we can give the target column (playlist_genre) as an input to &lt;code&gt;y&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;supervised_umap_embedding_df &amp;lt;- 
  spotify_songs %&amp;gt;% 
  select(-c(12:22)) %&amp;gt;% 
  bind_cols(supervised_umap_embedding %&amp;gt;% as_tibble()) %&amp;gt;% 
  dplyr::rename(umap_1 = V1, umap_2 = V2) %&amp;gt;% 
  mutate(
    selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
    point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
    track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
    genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
    popular_tracks_selected_artist = if_else(
      track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 70,shorter_names, NULL )) %&amp;gt;%
  distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;supervised_umap_embedding_df %&amp;gt;%
  ggplot(aes(x = umap_1, y = umap_2 ,color = playlist_genre )) +
  geom_point(size = 5.3,alpha =0.8 ) +
  gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
  scale_color_tableau() +
  guides(size = FALSE,
    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
    point.padding = 2.2,
    box.padding = .5,
    force = 1,
    min.segment.length = 0.1) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
       color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-the-map-of-spotify-songs/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is no surprise that the results of the supervised UMAP are much better separated than the unsupervised one. We just gave additional information to UMAP to transform input data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Fun with Maps</title>
      <link>/post/2020-01-24-some-fun-with-maps/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-01-24-some-fun-with-maps/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


</description>
    </item>
    
    <item>
      <title>Tidy Tuesday Submissions</title>
      <link>/project/2019-12-29-tidy-tuesday-submissions/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/2019-12-29-tidy-tuesday-submissions/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;For six years, I used python exclusively as the primary tool for carrying out my data science tasks and running my experiments. Recently, I have started using Tidyverse packages and tools in R for my data science activities. I am completely fascinated by how these tools make it easy for me to perform analysis and create nice visualization. Since then, I have tried to participate in the weekly Tidy Tuesday project.
You can find my submissions on this page.&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2020&lt;/h2&gt;
&lt;div id=&#34;week-39---himalayan-peaks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 39 - Himalayan Peaks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;peaks_plot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;week-37---friends&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 37 - Friends&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;My%20TidyTuesday%20Plot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;palmere-penguines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Palmere Penguines&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;palmere.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;week-12&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 12&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;The_Office.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;week-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 3&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;password_quality.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2019&lt;/h2&gt;
&lt;div id=&#34;week-52---christmas-songs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 52 - Christmas Songs&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;img src=&#34;00001d.png&#34; /&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;week-51--adoptable-dogs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Week 51 -Adoptable dogs&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;tags_Akbar.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ccc.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;week-46---code-in-cran-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 46 - Code in CRAN Packages&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;cran_pkg.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;week-36---moores-law&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Week 36 - Mooreâ€™s Law&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;CPU.jpg&#34; /&gt;
&lt;img src=&#34;CPU-Plot.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;week-35---simpsons-guest-stars&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Week 35 - Simpsons Guest Stars&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
