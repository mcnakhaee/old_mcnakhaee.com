<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Muhammad Chenariyan Nakhaee</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Muhammad Chenariyan Nakhaee</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpg</url>
      <title>Muhammad Chenariyan Nakhaee</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/otherwidget/slider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/slider/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic</title>
      <link>/otherwidget/hero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/hero/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Build &lt;strong&gt;Anything&lt;/strong&gt; with Widgets&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Star&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demos</title>
      <link>/otherwidget/demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/demo/</guid>
      <description>&lt;p&gt;Welcome to the &lt;strong&gt;personal demo&lt;/strong&gt; of Academic. Other demos available include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;&lt;strong&gt;Project Demo&lt;/strong&gt; (Academic&amp;rsquo;s actual site)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Over 100,000 &lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34;&gt;Amazing Websites&lt;/a&gt; have Already Been Built with Academic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34;&gt;Join&lt;/a&gt; the Most Empowered Hugo Community&lt;/strong&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>/otherwidget/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accomplish&amp;shy;ments</title>
      <link>/otherwidget/accomplishments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/accomplishments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/otherwidget/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Meet the Team</title>
      <link>/otherwidget/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>/otherwidget/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Featured Publications</title>
      <link>/otherwidget/featured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/featured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Popular Topics</title>
      <link>/otherwidget/tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/otherwidget/tags/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explaining Machine Learning Models Using Contextual Importance and Contextual Utility</title>
      <link>/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/explaining-machine-learning-models-using-contextual-importance-and-contextual-utility/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-kinds-of-explanation-does-ciu-generate&#34;&gt;What Kinds of explanation does CIU generate?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-does-ciiu-work&#34;&gt;How does CIIU work?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-toy-example-predicting-breast&#34;&gt;A toy example: predicting breast&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#permutation-feature-importance&#34;&gt;Permutation feature importance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-tree-classifier&#34;&gt;Decision Tree Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest-classifier&#34;&gt;Random Forest Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-boosting-classifier&#34;&gt;Gradient Boosting Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#explaining-a-single-observation&#34;&gt;Explaining a single observation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generating-textual-explanations&#34;&gt;Generating Textual Explanations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#drawbacks&#34;&gt;Drawbacks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;title: Explaining Machine Learning Models Using Contextual Importance and Contextual Utility
author: Muhammad Chenariyan Nakhaee
date: ‘2020-05-31’
slug: explaining-machine-learning-models-using-contextual-importance-and-contextual-utility
categories:
- XAI
tags:
- XAI
- Machine Learning
- R
- Python
subtitle: ’’
summary: ’’
authors: []
lastmod: ‘2020-01-18T18:46:22+01:00’
featured: no
draft: FALSE
image:
caption: ’’
focal_point: ’’
preview_only: no
projects: []
output:
blogdown::html_page:
toc: true
—
## Introduction
Explainablity is a very hot topics in the machine learning research community these days. over the past few years, many methods have been introduced to just understand how a machine learning model makes prediction. However, explainablity is not an entirely new concept and it was actually started a few decades ago. In this blogpost, I will introduce you to a rather unknown but simple technique which was introduced almost 20 years ago. This technique is called &lt;a href=&#34;https://www.researchgate.net/publication/228897070_Explaining_results_of_neural_networks_by_contextual_importance_and_utility&#34;&gt;Contextual Importance and Utility (CIU)&lt;/a&gt; for explaining ML models and show you how we can explain any types of machine learning.&lt;/p&gt;
&lt;div id=&#34;what-kinds-of-explanation-does-ciu-generate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What Kinds of explanation does CIU generate?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is a &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html&#34;&gt;&lt;strong&gt;&lt;em&gt;model-agnostic&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; methods and it can explain the output of any “black-box” machine learning model.&lt;/li&gt;
&lt;li&gt;It produces &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html&#34;&gt;&lt;strong&gt;&lt;em&gt;local&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; explanations which means that the explanations are generated for individual instances (not the whole model) and they just show which features are more important with respect to an individual observation.&lt;/li&gt;
&lt;li&gt;It gives us &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html&#34;&gt;&lt;strong&gt;&lt;em&gt;post-hoc&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; explanations as it is a method that processes the output of a machine learning model after training.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unlike LIME and many other techniques, CIU does not approximate or transforms what a model predicts but rather directly explain predictions. It can also provide a contrastive explanation. For instance, why did the model predict rainy and not cloudy?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-ciiu-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does CIIU work?&lt;/h2&gt;
&lt;p&gt;CIT estimates two values that aim to explain the context in which a machine learning model predicts:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contextual Importance (CI)&lt;/strong&gt; is a measure of how much of change in the range an output values can be attributed to one (or several) input variables. CU is based on the notion that a variable which results in a wider ranger of output values would be more important. Formally, CIU is defined as follows:&lt;/p&gt;
&lt;p&gt;CI = (Cmax - Cmin)/(absmax - absmin)&lt;/p&gt;
&lt;p&gt;Contextual Utility (CU) indicates how favorable the current value of one (or several) input variables is for a high output value. CU is computed using the following formula.&lt;/p&gt;
&lt;p&gt;CU = (out - Cmin)/(Cmax - Cmin)&lt;/p&gt;
&lt;p&gt;Cmax and Cmin are the highest and lowest values that the output of an ML model &lt;em&gt;can&lt;/em&gt; take by changing the input feature(s). Obtaining Cmax and Cmin is computationally and mathematically is not a trivial task. Also, absmax and absmin indicate the absolute range of values that the output has taken. For example, In classification problems, the absolute minimum and maximum range of values are the predicted probabilities of machine learning models and are between 0 and 1.&lt;/p&gt;
&lt;p&gt;CIU is implemented both in &lt;a href=&#34;https://github.com/TimKam/py-ciu&#34;&gt;&lt;strong&gt;python&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/KaryFramling/ciu&#34;&gt;R&lt;/a&gt;. For simplicity, I will use its python implementation (&lt;em&gt;py-ciu library&lt;/em&gt;) in this blogpost.&lt;/p&gt;
&lt;p&gt;You can install py-ciu using the pip command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pip install py-ciu&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-toy-example-predicting-breast&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A toy example: predicting breast&lt;/h2&gt;
&lt;p&gt;I will use breast cancer dataset in scikit-learn to show how we can use CIU. I will train three different machine learning models including a decision tree, a random forest and a gradient boosting algorithm on this dataset and compute CI and CU values for a single instance from test dataset.&lt;/p&gt;
&lt;p&gt;First we need to load necessary libraries and modules.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from ciu import determine_ciu
from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# for reproducability
np.random.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we split the dataset into a training and test set. We train our machine learning models on the training dataset and evaluate their performance on the test dataset. Note that for explaining ML models, we also use samples from the test dataset and not the training dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X = pd.DataFrame(load_breast_cancer()[&amp;#39;data&amp;#39;])
y = load_breast_cancer()[&amp;#39;target&amp;#39;]
X.columns = load_breast_cancer()[&amp;#39;feature_names&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X_train,X_test, y_train,y_test = train_test_split(X,y,stratify = y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def fit_evaluate_model(clf):
  clf = clf.fit(X_train, y_train)
  print(&amp;#39; Accuracy on test dataset {}&amp;#39;.format(clf.score(X_test,y_test)))
  return clf&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;permutation-feature-importance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Permutation feature importance&lt;/h3&gt;
&lt;p&gt;As I mentioned before, CIU only generates local explanations and doesn’t give us an global overview of how a model makes prediction. To gain a better understanding of the global importance of the model let us compute permutation feature importance scores that is implemented in scikit-learn.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def print_permutation_importance(model):
  imp_features = []
  pi = permutation_importance(model, X_test, y_test,
                            n_repeats=30,
                           random_state=0)
  for i in pi.importances_mean.argsort()[::-1]:
       if pi.importances_mean[i] - 2 * pi.importances_std[i] &amp;gt; 0:
           print(f&amp;quot;{X_test.columns[i]:&amp;lt;8} &amp;quot;
                 f&amp;quot;{pi.importances_mean[i]:.3f} &amp;quot;
                 f&amp;quot; +/- {pi.importances_std[i]:.3f}&amp;quot;)
           imp_features.append(pi.importances_mean[i])
           if len(imp_features) == 0:
                print(&amp;#39;no important features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-tree-classifier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision Tree Classifier&lt;/h3&gt;
&lt;p&gt;Since it is just a toy example, I won’t be very picky about hyper-parameters of my models and leave them to their default values in sklearn.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dt = DecisionTreeClassifier()
dt_fit = fit_evaluate_model(dt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Accuracy on test dataset 0.9370629370629371&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print_permutation_importance(dt_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## worst perimeter 0.173  +/- 0.019
## worst concave points 0.145  +/- 0.023
## worst concavity 0.135  +/- 0.017
## worst area 0.063  +/- 0.014
## radius error 0.036  +/- 0.014
## worst smoothness 0.018  +/- 0.008
## mean area 0.017  +/- 0.006&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-classifier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Forest Classifier&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;rf = RandomForestClassifier(
)
rf_fit = fit_evaluate_model(rf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Accuracy on test dataset 0.972027972027972&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print_permutation_importance(rf_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## worst texture 0.023  +/- 0.004
## mean texture 0.013  +/- 0.006
## worst smoothness 0.010  +/- 0.004
## mean concavity 0.010  +/- 0.005
## worst fractal dimension 0.006  +/- 0.003&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boosting-classifier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Boosting Classifier&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;gb = GradientBoostingClassifier()
gb_fit = fit_evaluate_model(gb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Accuracy on test dataset 0.9790209790209791&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print_permutation_importance(gb_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## worst concave points 0.024  +/- 0.011
## mean concave points 0.021  +/- 0.010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The random forest and gradient boosting classifiers have exactly the same accuracy score, however their most important features are different.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explaining-a-single-observation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explaining a single observation&lt;/h3&gt;
&lt;p&gt;Now lets explain how each model makes prediction on a single example (observation) from the test dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;example = X_test.iloc[1,:]
example_prediction = gb.predict(example.values.reshape(1, -1))
example_prediction_prob = gb.predict_proba(example.values.reshape(1, -1))
prediction_index = 0 if example_prediction &amp;gt; 0.5 else 1
print(f&amp;#39;Prediction {example_prediction}; Probability: {example_prediction_prob}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Prediction [1]; Probability: [[0.10952357 0.89047643]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain CIU score, we need to compute minimum and maximum observed value of each feature in the dataset first.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def min_max_features(X_train):
  min_max = dict()
  for i in range(len(X_train.columns)):
      min_max[X_train.columns[i]] =[X_train.iloc[:,i].min(),X_train.iloc[:,i].max(),False]
  return min_max
  
min_max = min_max_features(X_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def explain_ciu(example,model):
  ciu = determine_ciu(
      example.to_dict(),
      model.predict_proba,
      min_max,
      1000,
      prediction_index,
  )
  return ciu&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dt_ciu = explain_ciu(example,dt_fit)
rf_ciu = explain_ciu(example,rf_fit)
gb_ciu = explain_ciu(example,gb_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-textual-explanations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating Textual Explanations&lt;/h2&gt;
&lt;p&gt;We can obtain a textual explanation of CIU which indicates which feature(s) can be important for our test example&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dt_ciu.text_explain()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;The feature &amp;quot;mean radius&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean texture&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean perimeter&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean area&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean smoothness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean compactness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concavity&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concave points&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean symmetry&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean fractal dimension&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;radius error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;texture error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;perimeter error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;area error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;smoothness error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;compactness error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;concavity error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;concave points error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;symmetry error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;fractal dimension error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst radius&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst texture&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst perimeter&amp;quot;, which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst area&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst smoothness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst compactness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concavity&amp;quot;, which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concave points&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst symmetry&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst fractal dimension&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;rf_ciu.text_explain()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;The feature &amp;quot;mean radius&amp;quot;, which is important (CI=32.26%), is very typical for its class (CU=90.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean texture&amp;quot;, which is important (CI=35.48%), is unlikely for its class (CU=27.27%).&amp;#39;, &amp;#39;The feature &amp;quot;mean perimeter&amp;quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean area&amp;quot;, which is not important (CI=19.35%), is unlikely for its class (CU=33.33%).&amp;#39;, &amp;#39;The feature &amp;quot;mean smoothness&amp;quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean compactness&amp;quot;, which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concavity&amp;quot;, which is not important (CI=16.13%), is not typical for its class (CU=20.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concave points&amp;quot;, which is not important (CI=19.35%), is not typical for its class (CU=16.67%).&amp;#39;, &amp;#39;The feature &amp;quot;mean symmetry&amp;quot;, which is important (CI=38.71%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean fractal dimension&amp;quot;, which is not important (CI=6.45%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;radius error&amp;quot;, which is not important (CI=22.58%), is typical for its class (CU=71.43%).&amp;#39;, &amp;#39;The feature &amp;quot;texture error&amp;quot;, which is not important (CI=22.58%), is very typical for its class (CU=85.71%).&amp;#39;, &amp;#39;The feature &amp;quot;perimeter error&amp;quot;, which is not important (CI=22.58%), is unlikely for its class (CU=42.86%).&amp;#39;, &amp;#39;The feature &amp;quot;area error&amp;quot;, which is important (CI=38.71%), is unlikely for its class (CU=33.33%).&amp;#39;, &amp;#39;The feature &amp;quot;smoothness error&amp;quot;, which is not important (CI=3.23%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;compactness error&amp;quot;, which is not important (CI=12.9%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;concavity error&amp;quot;, which is not important (CI=6.45%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;concave points error&amp;quot;, which is not important (CI=9.68%), is typical for its class (CU=66.67%).&amp;#39;, &amp;#39;The feature &amp;quot;symmetry error&amp;quot;, which is not important (CI=6.45%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;fractal dimension error&amp;quot;, which is not important (CI=19.35%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst radius&amp;quot;, which is very important (CI=51.61%), is very typical for its class (CU=87.5%).&amp;#39;, &amp;#39;The feature &amp;quot;worst texture&amp;quot;, which is very important (CI=67.74%), is unlikely for its class (CU=33.33%).&amp;#39;, &amp;#39;The feature &amp;quot;worst perimeter&amp;quot;, which is very important (CI=70.97%), is typical for its class (CU=63.64%).&amp;#39;, &amp;#39;The feature &amp;quot;worst area&amp;quot;, which is very important (CI=61.29%), is typical for its class (CU=57.89%).&amp;#39;, &amp;#39;The feature &amp;quot;worst smoothness&amp;quot;, which is not important (CI=6.45%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst compactness&amp;quot;, which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concavity&amp;quot;, which is very important (CI=64.52%), is very typical for its class (CU=85.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concave points&amp;quot;, which is important (CI=38.71%), is not typical for its class (CU=16.67%).&amp;#39;, &amp;#39;The feature &amp;quot;worst symmetry&amp;quot;, which is important (CI=25.81%), is typical for its class (CU=50.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst fractal dimension&amp;quot;, which is not important (CI=3.23%), is not typical for its class (CU=0.1%).&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;gb_ciu.text_explain()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;The feature &amp;quot;mean radius&amp;quot;, which is not important (CI=16.49%), is not typical for its class (CU=0.65%).&amp;#39;, &amp;#39;The feature &amp;quot;mean texture&amp;quot;, which is highly important (CI=90.14%), is not typical for its class (CU=3.76%).&amp;#39;, &amp;#39;The feature &amp;quot;mean perimeter&amp;quot;, which is not important (CI=2.63%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean area&amp;quot;, which is not important (CI=3.36%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean smoothness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean compactness&amp;quot;, which is important (CI=37.26%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concavity&amp;quot;, which is not important (CI=4.0%), is not typical for its class (CU=8.92%).&amp;#39;, &amp;#39;The feature &amp;quot;mean concave points&amp;quot;, which is important (CI=38.25%), is not typical for its class (CU=3.57%).&amp;#39;, &amp;#39;The feature &amp;quot;mean symmetry&amp;quot;, which is not important (CI=8.91%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;mean fractal dimension&amp;quot;, which is not important (CI=1.54%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;radius error&amp;quot;, which is not important (CI=10.53%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;texture error&amp;quot;, which is not important (CI=6.53%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;perimeter error&amp;quot;, which is not important (CI=1.48%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;area error&amp;quot;, which is very important (CI=57.97%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;smoothness error&amp;quot;, which is not important (CI=16.51%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;compactness error&amp;quot;, which is not important (CI=4.39%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;concavity error&amp;quot;, which is not important (CI=4.03%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;concave points error&amp;quot;, which is not important (CI=5.76%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;symmetry error&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;fractal dimension error&amp;quot;, which is not important (CI=21.47%), is not typical for its class (CU=17.33%).&amp;#39;, &amp;#39;The feature &amp;quot;worst radius&amp;quot;, which is not important (CI=1.27%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst texture&amp;quot;, which is very important (CI=60.61%), is not typical for its class (CU=13.75%).&amp;#39;, &amp;#39;The feature &amp;quot;worst perimeter&amp;quot;, which is important (CI=41.37%), is not typical for its class (CU=23.17%).&amp;#39;, &amp;#39;The feature &amp;quot;worst area&amp;quot;, which is not important (CI=19.51%), is typical for its class (CU=67.91%).&amp;#39;, &amp;#39;The feature &amp;quot;worst smoothness&amp;quot;, which is not important (CI=18.24%), is unlikely for its class (CU=48.97%).&amp;#39;, &amp;#39;The feature &amp;quot;worst compactness&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concavity&amp;quot;, which is not important (CI=10.79%), is very typical for its class (CU=100.0%).&amp;#39;, &amp;#39;The feature &amp;quot;worst concave points&amp;quot;, which is important (CI=42.94%), is not typical for its class (CU=4.32%).&amp;#39;, &amp;#39;The feature &amp;quot;worst symmetry&amp;quot;, which is not important (CI=5.86%), is not typical for its class (CU=0.1%).&amp;#39;, &amp;#39;The feature &amp;quot;worst fractal dimension&amp;quot;, which is not important (CI=0.0%), is not typical for its class (CU=0.1%).&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;drawbacks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Drawbacks&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In regression problems, the range of possible values for the target variable can be infinite, which somehow does not make sense when we want to compute CIU. The authors said that they have put a limit on the range of values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Computing the range of values can be a little bit misleading especially when we have outliers in the dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is not clear how we can get a global explanation for the model using CIU.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Happiest, Saddest, Most Energetic and Fastet Persian Singers on Spotify</title>
      <link>/post/happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/happiest-saddest-most-energetic-and-fastet-persian-singers-on-spotify/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gargle)
library(tidyverse)
library(googlesheets4)
library(tidymodels)
library(gghighlight)
library(hrbrthemes)
library(ggthemes)
library(ggrepel)
library(ggalt)
library(extrafont)
library(ggtext)
library(ggforce)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop &amp;lt;- read_csv(&amp;#39;https://raw.githubusercontent.com/mcnakhaee/datasets/master/Persian_Songs_Spotify.csv&amp;#39;)
songs_audio_plus_pop &amp;lt;- songs_audio_plus_pop %&amp;gt;%
  filter(
    !artist_name %in% c(
      &amp;#39;Hatam Asgari&amp;#39;,
      &amp;#39;Kaveh Deylami&amp;#39;,
      &amp;#39;Nasser Abdollahi&amp;#39;,
      &amp;#39;Peyman Yazdanian&amp;#39;,
      &amp;#39;Abbas Ghaderi&amp;#39;,
      &amp;#39;Mohammad Golriz&amp;#39;,
      &amp;#39;Hamid Hami&amp;#39;,
      &amp;#39;Koveyti Poor&amp;#39;,
      &amp;#39;Mohsen Sharifian&amp;#39;
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;overall-song-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overall Song Features&lt;/h3&gt;
&lt;p&gt;This plot was inspired by the
inspired me to create a similar plot for Persian singers&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;artists &amp;lt;-
  c( &amp;#39;Sirvan Khosravi&amp;#39;,
     &amp;#39;Hesameddin Seraj&amp;#39;,
     &amp;#39;Rastak&amp;#39;,
     &amp;#39;Shahram Nazeri&amp;#39;,
    &amp;#39;Hossein Alizadeh&amp;#39;,
    &amp;#39;Reza Sadeghi&amp;#39;,
    &amp;#39;Alireza Eftekhari&amp;#39;,
    &amp;#39;Mohammadreza Shajarian&amp;#39; ,
    &amp;#39;Salar Aghili&amp;#39;,
    &amp;#39;Morteza Pashaei&amp;#39;,
    &amp;#39;Alireza Ghorbani&amp;#39;,
    &amp;#39;Homayoun Shajarian&amp;#39;,
    &amp;#39;Mohsen Yeganeh&amp;#39; ,
    &amp;#39;Morteza Pashaei&amp;#39;,
    &amp;#39;Moein&amp;#39;,
     &amp;#39;Farzad Farzin&amp;#39;,
     &amp;#39;Babak Jahanbakhsh&amp;#39;,
    &amp;#39;Ehsan Khajeh Amiri&amp;#39;,
    &amp;#39;Siavash Ghomayshi&amp;#39;,
    &amp;#39;Xaniar Khosravi&amp;#39;,
    &amp;#39;Tohi&amp;#39; ,
    &amp;#39;Mohsen Chavoshi&amp;#39;,
    &amp;#39;Abbas Ghaderi&amp;#39;,
    &amp;#39;Amir Tataloo&amp;#39;,
    &amp;#39;Hamed Homayoun&amp;#39;,
    &amp;#39;Kayhan Kalhor&amp;#39;
 )

order &amp;lt;- c(
  &amp;quot;valence&amp;quot;,
  &amp;quot;energy&amp;quot;,
  &amp;quot;tempo&amp;quot;,
  &amp;quot;loudness&amp;quot;,
  &amp;quot;acousticness&amp;quot;,
  &amp;quot;instrumentalness&amp;quot;,
  &amp;quot;danceability&amp;quot;
)

normalized_features_long &amp;lt;- songs_audio_plus_pop %&amp;gt;%
  mutate_at(order, scales::rescale, to = c(0, 7)) %&amp;gt;%
  filter(!is.na(popularity)) %&amp;gt;%
  filter(artist_name %in% artists) %&amp;gt;%
  mutate(artist_name = factor(artist_name, levels = levels))  %&amp;gt;%
  pivot_longer(
    names_to = &amp;#39;metric&amp;#39;,
    cols = c(
      &amp;quot;valence&amp;quot;,
      &amp;quot;energy&amp;quot;,
      &amp;quot;tempo&amp;quot;,
      &amp;quot;loudness&amp;quot;,
      &amp;quot;acousticness&amp;quot;,
      &amp;quot;danceability&amp;quot;),
    values_to = &amp;#39;value&amp;#39;
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Every singer or composer has his/her own distinct musical style and audio charactristics. Based on 8 audio features extracted from Spotify’s API, this plot compares several well-known Persian singers and musicians.These artists are compared by the minimum (red) , the average (orange) and maximum (yellow) values of each audio features in their songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_main &amp;lt;- ggplot() +
  geom_polygon(
    data = normalized_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), mean) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name, ),
    alpha = .54,
    size = 1.5,
    show.legend = T,
    fill = &amp;#39;#FF1654&amp;#39;
  ) +
  geom_polygon(
    data = normalized_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), max) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name, ),
    alpha = .44,
    size = 1.5,
    show.legend = T,
    fill = &amp;#39;#FFE066&amp;#39;
  ) +
  geom_polygon(
    data = normalized_features_long %&amp;gt;%  group_by(artist_name, metric) %&amp;gt;%
      summarise_at(c(&amp;quot;value&amp;quot;), min) %&amp;gt;%
      arrange(factor(metric, levels = order)) %&amp;gt;%
      ungroup(),
    aes(x = metric, y = value, group = artist_name, ),
    alpha = .84,
    size = 1.5,
    show.legend = T,
    fill =  &amp;quot;#EF476F&amp;quot;
  ) +
  scale_x_discrete(
    limits = order,
    labels = c(
      &amp;quot;Happy&amp;quot;,
      &amp;quot;Energy&amp;quot;,
      &amp;quot;Fast&amp;quot;,
      &amp;quot;Loud&amp;quot;,
      &amp;quot;Acoustic&amp;quot;,
      &amp;quot;Instrumental&amp;quot;,
      &amp;quot;Danceable&amp;quot;
    )
  ) +
  coord_polar(clip = &amp;#39;off&amp;#39;) +
  theme_minimal() +
  labs(title = &amp;quot;Persian Singers and Their Audio Characteristics&amp;quot;,
       caption = &amp;#39;Source: Spotify \n Visualization: mcnakhaee&amp;#39;) +
  
  ylim(0, 8) +
  facet_wrap(~ artist_name, ncol = 4) +
  theme(
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_blank(),
    axis.text.x = element_text(
      family =  &amp;#39;Montserrat&amp;#39;,
      size = 13.5,
      margin = ggplot2::margin(30, 0, 20, 0)
    ),
    
    plot.caption = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 11,
      color = &amp;#39;grey80&amp;#39;
    ) ,
    text = element_text(family =  &amp;#39;Montserrat&amp;#39;),
    strip.text = element_text(family =  &amp;#39;Montserrat&amp;#39;, size = 18),
    strip.text.x = element_text(margin = ggplot2::margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;)),
    panel.spacing = unit(3.5, &amp;quot;lines&amp;quot;),
    panel.grid = element_blank(),
    plot.title = element_text(
      family = &amp;#39;Montserrat&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 32,
      color = &amp;#39;gray10&amp;#39;
    ),
    plot.background = element_rect(fill = &amp;#39;#FCF0E1&amp;#39;)
  ) 

p_main&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;jitter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Jitter&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(  theme_void() +
  theme(
    text = element_text(family =  &amp;#39;B Mitra&amp;#39;),
    axis.text.x = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      color = &amp;#39;gray80&amp;#39;,
      size = 20
    ),
    axis.text.y = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 20),
      color = &amp;#39;gray80&amp;#39;,
      size = 20
    ),
    axis.title.x = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 27,
      color = &amp;#39;gray80&amp;#39;
    ),
    plot.title = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(40, 0, 40, 0),
      size = 35,
      color = &amp;#39;gray80&amp;#39;
    ),
    plot.caption = element_text(family =&amp;#39;B Mitra&amp;#39;,
                                  margin = ggplot2::margin(30, 0, 20, 20),
                                      size = 20,
                                  color = &amp;#39;gray70&amp;#39;) ,
    legend.position = &amp;#39;none&amp;#39;,
    plot.background = element_rect(fill = &amp;quot;#516869&amp;quot;)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter &amp;lt;- songs_audio_plus_pop %&amp;gt;% 
  filter(artist_name %in% artists) %&amp;gt;% 
  mutate(is_popular = !is.na(popularity)) %&amp;gt;%
  distinct(artist_name_farsi,track_name,.keep_all = T) %&amp;gt;% 
  mutate(is_popular_size = if_else(!is.na(popularity),popularity,25),
         is_popular_alpha = if_else(!is.na(popularity),0.8,0.5)) %&amp;gt;% 
  mutate(track_name_farsi = str_wrap(track_name_farsi, width = 15)) %&amp;gt;% 
  mutate(track_farsi_avail = if_else(!is.na(track_name_farsi)&amp;amp; !is.na(popularity) &amp;amp; nchar(track_name_farsi) &amp;lt; 20 &amp;amp; !explicit,track_name_farsi,&amp;#39;&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name_farsi, y = valence)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
    
  ) +
  geom_text_repel(
    aes(label = track_farsi_avail , x = artist_name_farsi , y = valence),
    family = &amp;#39;B Mitra&amp;#39;,
    color = &amp;#39;gray99&amp;#39;,
    size = 7,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name_farsi)
  ) +
  scale_color_manual(values = c(&amp;#39;#FFD166&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;شادی&amp;#39;,
    title = &amp;#39;مقایسه ای بین شادی در آهنگ های برخی از خوانندگان (نوازندگان) ایرانی&amp;#39;,
    subtitle = &amp;#39;&amp;#39;,
     caption = &amp;#39;منبع: اسپاتیفای\n  مصورسازی: محمد چناریان نخعی&amp;#39;) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name_farsi, y = energy)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
    
  ) +
  geom_text_repel(
    aes(label = track_farsi_avail , x = artist_name_farsi , y = energy),
    family = &amp;#39;B Mitra&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name_farsi)
  ) +
  scale_color_manual(values = c(&amp;#39;#EF476F&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;انرژی&amp;#39;,
    title = &amp;#39;مقایسه ای بین انرژی در آهنگ های برخی از خوانندگان (نوازندگان) ایرانی&amp;#39;,
    subtitle = &amp;#39;&amp;#39;,
     caption = &amp;#39;منبع: اسپاتیفای\n  مصورسازی: محمد چناریان نخعی&amp;#39;) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name_farsi, y = danceability)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
    
  ) +
  geom_text_repel(
    aes(label = track_farsi_avail , x = artist_name_farsi , y = danceability),
    family = &amp;#39;B Mitra&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name_farsi)
  ) +
  scale_color_manual(values = c(&amp;#39;#A5668B&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;رقص آوری&amp;#39;,
    title = &amp;#39;مقایسه ای بین میزان رقص آوری در آهنگ های برخی از خوانندگان (نوازندگان) ایرانی&amp;#39;,
    subtitle = &amp;#39;&amp;#39;,
    caption = &amp;#39;منبع: اسپاتیفای\n  مصورسازی: محمد چناریان نخعی&amp;#39;
  ) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop_jitter %&amp;gt;%
  ggplot(aes(x = artist_name_farsi, y = loudness)) +
  geom_jitter(
    aes(
      color = is_popular,
      size = is_popular_size,
      alpha = is_popular_alpha
    ),
    size = 6,
    width = 0.2,
    
  ) +
  geom_text_repel(
    aes(label = track_farsi_avail , x = artist_name_farsi , y = loudness),
    family = &amp;#39;B Mitra&amp;#39;,
    color = &amp;#39;gray90&amp;#39;,
    size = 6,
    force = 0.6,
    max.iter = 2000,
    box.padding = 0.4,
    point.padding = 0.6,
    min.segment.length = 0.15,
    nudge_y      = 0.001,
    hjust = 0.5,
    segment.alpha = 0.6,
    segment.size = 0.6
  ) +
  stat_summary(
    fun = mean,
    geom = &amp;#39;point&amp;#39;,
    color = &amp;#39;#FF9F1C&amp;#39;,
    size = 5,
    aes(group = artist_name_farsi)
  ) +
  #&amp;#39;#EF476F&amp;#39;
  scale_color_manual(values = c(&amp;#39;#06D6A0&amp;#39;, &amp;#39;#EF476F&amp;#39;)) +
  labs(
    x = &amp;#39;&amp;#39;,
    y = &amp;#39;بلندی&amp;#39;,
    title = &amp;#39;مقایسه ای بین میزان بلندی آهنگ های برخی از خوانندگان (نوازندگان) ایرانی&amp;#39;,
    subtitle = &amp;#39;&amp;#39;,
    caption = &amp;#39;منبع: اسپاتیفای\n  مصورسازی: محمد چناریان نخعی&amp;#39;
  ) +
  scale_y_continuous(sec.axis = dup_axis()) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;most-popular-songs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Popular Songs&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs_audio_plus_pop &amp;lt;- songs_audio_plus_pop %&amp;gt;%
  filter(
    !artist_name %in% c(
      &amp;#39;Hatam Asgari&amp;#39;,
      &amp;#39;Kaveh Deylami&amp;#39;,
      &amp;#39;Nasser Abdollahi&amp;#39;,
      &amp;#39;Peyman Yazdanian&amp;#39;,
      &amp;#39;Abbas Ghaderi&amp;#39;,
      &amp;#39;Mohammad Golriz&amp;#39;,
      &amp;#39;Hamid Hami&amp;#39;,
      &amp;#39;Koveyti Poor&amp;#39;,
      &amp;#39;Mohsen Sharifian&amp;#39;,
      &amp;#39;Soheil Nafissi&amp;#39;
    )
  )
s0 &amp;lt;-
  &amp;quot;با های آهنگ اسپاتیفای در زبان فارسی نوازندگان و خوانندگان برتر آهنگ 10 میان از&amp;quot;
s1 &amp;lt;-
  &amp;quot;&amp;lt;br/&amp;gt; دارند اسپاتیفای کاربران میان در را محبوبیت میانگین بالاترین مانکن ساسی های آهنگ ،خاص طور به .اند شده مرتب (نوازنده) خواننده هر محبوبیت میانگین اساس بر اسامی.&amp;quot;
s2 &amp;lt;- &amp;quot; اند شده نمایش نمودار این در محبوبیت مقدار&amp;quot;
s3 &amp;lt;- &amp;quot;&amp;lt;span style=&amp;#39;color:#118ab2&amp;#39;&amp;gt; بیشترین&amp;lt;/span&amp;gt; &amp;quot;
s4 &amp;lt;- &amp;quot;&amp;lt;span style=&amp;#39;color:#ef476f&amp;#39;&amp;gt; کمترین و &amp;lt;/span&amp;gt;&amp;quot;
s_ &amp;lt;- paste0(s2, s4, s3, s0, collapse = &amp;#39; &amp;#39;)
s_ &amp;lt;- paste0(s1, s_, collapse = &amp;#39;&amp;lt;/br&amp;gt;&amp;#39;)
songs_audio_plus_pop %&amp;gt;%
  filter(!is.na(popularity)) %&amp;gt;%
  mutate(track_name = if_else(!is.na(track_name), track_name, track_name)) %&amp;gt;%
  group_by(artist_name) %&amp;gt;%
  
  filter(artist_name != &amp;#39;سایر&amp;#39;) %&amp;gt;%
  summarize(
    avg_pop = mean(popularity),
    min_pop = min(popularity),
    max_pop = max(popularity),
    most_popular = track_name[which.max(popularity)],
    least_popular = track_name[which.min(popularity)]
  ) %&amp;gt;%
  mutate(
    artist_name = fct_reorder(artist_name, avg_pop),
    size_text = if_else(str_detect(least_popular, &amp;#39;When&amp;#39;), 12, 12)
  ) %&amp;gt;%
  
  ggplot(aes(x = min_pop , xend = max_pop, y = artist_name)) +
  geom_dumbbell(
    colour_x = &amp;#39;#ef476f&amp;#39;,
    colour_xend = &amp;#39;#118ab2&amp;#39;,
    size_x = 7,
    size_xend = 7
  ) +
  geom_text(
    aes(x = min_pop - 1, y = artist_name, label = least_popular),
    size = 7,
    family = &amp;#39;B Tehran&amp;#39;,
    hjust = 1
  ) +
  geom_text(
    aes(x = max_pop + 1, y = artist_name, label = most_popular),
    size = 7,
    family = &amp;#39;B Tehran&amp;#39;,
    hjust = 0
  ) +
  labs(title = &amp;#39;محبوب ترین آهنگ ها و خواننده های ایرانی در اسپاتیفای&amp;#39;,
       subtitle = s_,
       x = &amp;#39;محبوبیت&amp;#39;,
       caption = &amp;#39;منبع: اسپاتیفای\n  مصورسازی: محمد چناریان نخعی&amp;#39;) +
  scale_x_continuous(sec.axis = dup_axis()) +
  theme_tufte() +
  theme(
    plot.title = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      hjust = .5,
      margin = ggplot2::margin(0, 0, 40, 0),
      size = 45
    ),
    plot.subtitle = element_markdown(
      family = &amp;#39;B Mitra&amp;#39;,
      size = 15,
      margin = ggplot2::margin(20, 0, 40, 0),
      hjust = 1
      
    ),
    axis.text.x = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 20
    ),
    
    axis.text.y = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 20
    ),
    axis.title.x = element_text(
      family = &amp;#39;B Mitra&amp;#39;,
      margin = ggplot2::margin(30, 0, 20, 0),
      size = 30
    ),
    plot.caption = element_text(family =&amp;#39;B Mitra&amp;#39;,
                                  margin = ggplot2::margin(30, 0, 20, 20),
                                      size = 20,
                                  color = &amp;#39;gray20&amp;#39;) ,
    axis.title.y = element_blank(),
    plot.background = element_rect(fill = &amp;#39;#FCF0E1&amp;#39;),
    plot.margin = unit(c(1, 1, 1.5, 1.2), &amp;quot;cm&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;over-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Over Time&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;years &amp;lt;- normalized_features %&amp;gt;% 
  distinct(album_release_year) %&amp;gt;% 
  filter(album_release_year &amp;gt;1984) %&amp;gt;% 
  pull()

normalized_features %&amp;gt;%
  select(album_release_year, danceability, energy, loudness, speechiness, liveness, valence, tempo, duration_ms,popularity
         ) %&amp;gt;% 
  mutate_at(vars(-album_release_year),scale) %&amp;gt;% 
  group_by(album_release_year) %&amp;gt;% 
  summarise_at(vars(everything()),mean,na.rm = TRUE) %&amp;gt;% 
  pivot_longer(names_to = &amp;#39;metric&amp;#39;,cols =c(danceability, energy, loudness, speechiness, liveness, valence, tempo, duration_ms,popularity
                                           ),
           values_to = &amp;#39;value&amp;#39;) %&amp;gt;% 
  ggplot(aes(album_release_year,y= value,color = metric)) +
  geom_line(color = &amp;#39;indianred&amp;#39;,size = 1.5,alpha = 1) + 
  gghighlight( use_direct_label = FALSE,unhighlighted_params = list(size = 1.5,width = 0.5,color =&amp;#39;#F6DAB4&amp;#39;,alpha  = 0.7)) +
  scale_x_continuous(breaks = years,labels = years,limits = c(1985,2020)) +
  facet_wrap(~ metric,
          #   scales = &amp;#39;free_y&amp;#39;
           ncol = 2 ) +
  theme_fivethirtyeight()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jakelawlor/TidyTuesday_JL/blob/master/CodeFiles/Jan21.20.Spotify.Rmd&#34; class=&#34;uri&#34;&gt;https://github.com/jakelawlor/TidyTuesday_JL/blob/master/CodeFiles/Jan21.20.Spotify.Rmd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jakelawlor/TidyTuesday_JL/blob/master/CodeFiles/Feb.18.20.CO2Food.R&#34; class=&#34;uri&#34;&gt;https://github.com/jakelawlor/TidyTuesday_JL/blob/master/CodeFiles/Feb.18.20.CO2Food.R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nagvayeasheghi.blogfa.com/post/1417/اینسترومنتال-(Instrumental)-چیست-&#34; class=&#34;uri&#34;&gt;http://nagvayeasheghi.blogfa.com/post/1417/اینسترومنتال-(Instrumental)-چیست-&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the 2020 Democratic Presidential Debates - Part 2</title>
      <link>/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;It wasn’t possible for many of us to watch every 2020 Democratic Primary debate. It was important for some of us to know what happened during the debates. In my case, I was reading about what happened in debates in some kind of online newspapers or I watched a highlight of a debate on Youtube the next day. However, they only give a short summary of a debate or just broadcast a portion of debates that includes a heated exchange of opinions between candidates. As a result, many important issues raised by candidates will be &lt;strong&gt;ignored and forgotten in the&lt;/strong&gt; aftermath of a debate. So, it is important to summarize the content of a debate in such a way that everyone could understand what went on in a debate and what issues each candidate addressed during his/her speech. In this blog post, I will show you how I used some NLP techniques for exploring the content of debates and give you a comprehensive overview of topics that each candidate discussed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mcnakhaee.com/post/2020-02-23-the-most-eloeuent-democratic-candidate/&#34;&gt;In my last blog post&lt;/a&gt;, I explained that I had the three following goals in mind when I started exploring the 2020 Democratic Debates :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To know how eloquent presidential candidate are.&lt;/li&gt;
&lt;li&gt;To find out who used more positive or more negative words in his/her speech by performing sentiment analysis.&lt;/li&gt;
&lt;li&gt;A map of topics, individuals and entities that each candidate mentioned in his/her speech by using named entity recognition and network analysis..&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I only discussed how I approached the first two aspects of my experiment in my last blogpost. Now it is time to investigate the third and last one.&lt;/p&gt;
&lt;p&gt;Initially, my aim from using network analysis was to determine potential allies and enemies on the debate stage. For example, &lt;a href=&#34;https://www.theguardian.com/us-news/2020/mar/04/mike-bloomberg-out-60-second-attack-elizabeth-warren-destroyed-campaign&#34;&gt;Elizabeth Warren mentioned Mike Bloomberg several times and attacked him harshly in the 9th debate&lt;/a&gt;. During the same debate, &lt;a href=&#34;https://www.independent.co.uk/news/world/americas/us-election/amy-klobuchar-pete-buttigieg-handshake-democratic-debate-video-a9348621.html&#34;&gt;Amy Klobuchar and Pete Buttigieg clashed bitterly with each other&lt;/a&gt;. These are just two instances of many other heated exchanges between the candidates that happend over the course of 10 debates.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To make things more clear, I transformed my objective into two questions that I would like to answer:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How many times did a candidate address (mention) other candidates during a debate?&lt;/li&gt;
&lt;li&gt;How did he/she refer to a candidate(in a friendly or unfriendly manner)?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;A simple approach to answer these questions is to store the names of all candidates in a variable (for example a vector in R or a list in Python), iterate over the transcript, compute the sentiment, count and store the number of times that a candidate’s name was brought up by another candidate.&lt;/p&gt;
&lt;p&gt;However, this approach is a little bit challenging and requires a lot of manual data pre-processing efforts. For each democratic candidate, one must compile a comprehensive combinations of ways that may be used to call a candidate and preparing such a list seems to be a very time-consuming task. For example, other candidates mentioned Bernie Sanders in many different ways including Bernie, Bernie Sanders or Senator Sanders.&lt;/p&gt;
&lt;p&gt;I realized that I can use Named Entity Recognition (NER), a technique from Natural Language Processing (NLP) literature, to extract the name of candidates from the transcript and solve this problem more efficiently. By using this approach, not only I can find candidates’ names form the transcript, but I can also find the names of other politicians, individuals and even organizations and further extend my analysis to include much more topics and issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Workflow&lt;/h2&gt;
&lt;p&gt;I made use of both python and R in my analysis. My workflow includes the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I access the transcript of debates using this package.&lt;/li&gt;
&lt;li&gt;I use tidytext to split the transcript into multiple sentences and also for sentiment analysis.&lt;/li&gt;
&lt;li&gt;I extract several types of Named Entities from each sentence, using Spacy,&lt;/li&gt;
&lt;li&gt;I compute the sentiment of each sentence using TextBlob library in Python.&lt;/li&gt;
&lt;li&gt;I transferred the results to R for visualization. There, I visualize the network of mentions and entities using ggraph and ggplot library.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that I could have implemented all the steps in R. For instance, Spacy has an R wrapper called Spacyr which gives the same functionality that I need for this analysis. However, I’d like to increase the number of tools that I can use. Particularly, using Python and R side by side is an interesting challenge for me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(demdebates2020)
library(tidytext)
library(tidygraph)
library(tidyverse)
library(ggraph)
library(gghighlight)
library(ggthemes)
library(kableExtra)
library(reticulate)
library(magrittr)
library(pluralize)
theme_set(
  theme_graph(base_family = &amp;#39;Montserrat&amp;#39;)  +
    theme(
      panel.border = element_blank(),
      plot.title = element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#540b0e&amp;#39;,
        size = 42,
        margin = ggplot2::margin(40, 40, 20, 10),
        hjust = 0
      ),
      plot.subtitle =  element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#7d4f50&amp;#39;,
        size = 30,
        margin = ggplot2::margin(20, 40, 80, 10),
        hjust = 0
      ),
      plot.caption =  element_text(
        family = &amp;#39;Montserrat&amp;#39;,
        face = &amp;quot;bold&amp;quot;,
        colour = &amp;#39;#540b0e&amp;#39;,
        size = 16,
        margin = ggplot2::margin(0, 0, 20, 20),
      ),
      legend.position = &amp;#39;none&amp;#39;,
      plot.background = element_rect(fill = &amp;#39;#FCF0E1&amp;#39;),
      
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;loading-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.2 Loading the dataset&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speech
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
All right. So with that business out of the way, we want to get to it. And we’ll start this evening with Senator Elizabeth Warren.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Senator, good evening to you.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thank you. Good to be here.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Savannah Guthrie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
You have many plans - free college, free child care, government health care, cancellation of student debt, new taxes, new regulations, the breakup of major corporations. But this comes at a time when 71 percent of Americans say the economy is doing well, including 60 percent of Democrats. What do you say to those who worry this kind of significant change could be risky to the economy?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
So I think of it this way. Who is this economy really working for? It’s doing great for a thinner and thinner slice at the top. It’s doing great for giant drug companies. It’s just not doing great for people who are trying to get a prescription filled.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
It’s doing great for people who want to invest in private prisons, just not for the African Americans and Latinx whose families are torn apart, whose lives are destroyed, and whose communities are ruined.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;tokenization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.1. Tokenization&lt;/h3&gt;
&lt;p&gt;As I mentioned before, I use tidytext to tokenize the transcript dataset based on sentences. For sentence tokenization, you need to set &lt;code&gt;token = &#39;sentences&#39;&lt;/code&gt; in &lt;code&gt;unnest_tokens()&lt;/code&gt; function. I think sentence tokenization is a reasonable choice because candidates might change the subject or the tone of their speech in each sentence.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debates &amp;lt;- debates %&amp;gt;%
 unnest_tokens(sentence, speech, token = &amp;#39;sentences&amp;#39;,to_lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(debates) %&amp;gt;%
 kable() %&amp;gt;%
 kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentence
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
222
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
285
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
328
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(APPLAUSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
576
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Abby Phillip
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(COMMERCIAL BREAK)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moderator
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
284
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;named-entity-recognition-using-spacy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Named Entity Recognition using Spacy&lt;/h3&gt;
&lt;p&gt;Now we change to python for NER but before starting the analysis, we need to install and import a few python libraries.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import spacy
from textblob import TextBlob&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In RStudio and Rmarkdown notebooks, with the help of &lt;code&gt;reticulate&lt;/code&gt; library, we can easily load the debate dataset in our R environment to our Python environment, .&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;debates = r.debates&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in in the transcript dataset there are rows for both the candidates and the moderators who asked questions from candidates. However, we are particularly interested in what the candidates said, so we only filter rows corresponding to candidates.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates = debates[(debates[&amp;#39;type&amp;#39;] == &amp;#39;Candidate&amp;#39;) &amp;amp; (pd.notnull(debates[&amp;#39;sentence&amp;#39;])) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are almost ready for extracting the named entities. But to use Spacy’s NLP features such as NER, we first need to download and load a pre-trained English language model. There are &lt;a href=&#34;https://spacy.io/usage/models&#34;&gt;several English language models&lt;/a&gt; with different sizes available in Spacy. I used the largest language model available as it might be better and more accurate.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nlp = spacy.load(&amp;#39;en_core_web_lg&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spacy’s NER model is trained on the &lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2013T19&#34;&gt;OntoNotes 5&lt;/a&gt; corpus and it can detect several types of named entities including:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;TYPE&lt;/th&gt;
&lt;th&gt;DESCRIPTION&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;PERSON&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;People, including fictional.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;NORP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Nationalities or religious or political groups.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;FAC&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Buildings, airports, highways, bridges, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;ORG&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Companies, agencies, institutions, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;GPE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Countries, cities, states.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LOC&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Non-GPE locations, mountain ranges, bodies of water.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;PRODUCT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Objects, vehicles, foods, etc. (Not services.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;EVENT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Named hurricanes, battles, wars, sports events, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;WORK_OF_ART&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Titles of books, songs, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LAW&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Named documents made into laws.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;LANGUAGE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Any named language.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;DATE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Absolute or relative dates or periods.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;TIME&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Times smaller than a day.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;PERCENT&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Percentage, including ”%“.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;MONEY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Monetary values, including unit.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;QUANTITY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Measurements, as of weight or distance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;ORDINAL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;“first”, “second”, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;CARDINAL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Numerals that do not fall under another type.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, there are many types of named entities but I narrow down my analysis to just a handful of them including &lt;code&gt;PERSON&lt;/code&gt;, &lt;code&gt;ORG&lt;/code&gt;, &lt;code&gt;GPE&lt;/code&gt;, &lt;code&gt;NORP&lt;/code&gt;, &lt;code&gt;LAW&lt;/code&gt; and &lt;code&gt;LOC&lt;/code&gt;.
The named entity labels are stored in &lt;code&gt;label_&lt;/code&gt; attribute. To do so, we need to create &lt;code&gt;Doc&lt;/code&gt; object using &lt;code&gt;nlp()&lt;/code&gt; method. When we call &lt;code&gt;nlp()&lt;/code&gt; on the input text, spacy uses the language model to tokenize the document first. Then, spacy applies a tagger, parser and named entity recognizer steps as the next components of its processing pipeline. The named entities can be accessed by &lt;code&gt;ents&lt;/code&gt; attribute of the document object.&lt;/p&gt;
&lt;p&gt;If you are interested to learn more about Spacy and how it works, I have provided some links at the end of this post.&lt;/p&gt;
&lt;p&gt;I define a python function that iterates over all named entities and see to which class of named entities (by default &lt;code&gt;PERSON&lt;/code&gt;) they belong. I apply this function to the transcript column in the original dataset and store each extracted type of entities as a separate column.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def extract_entities_delim(text,type_ent = &amp;#39;PERSON&amp;#39;):
  ent_text = &amp;#39;&amp;#39;
  doc = nlp(text)
  for e in doc.ents:
    if e.label_ == type_ent:
      ent_text = e.text+ &amp;#39;;&amp;#39; + ent_text 
  return ent_text&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates[&amp;#39;PERSON&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x))
candidates[&amp;#39;ORG&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;ORG&amp;#39;))
candidates[&amp;#39;GPE&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;GPE&amp;#39;))
candidates[&amp;#39;NORP&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;NORP&amp;#39;))
candidates[&amp;#39;LAW&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;LAW&amp;#39;))
candidates[&amp;#39;LOC&amp;#39;] = candidates[&amp;#39;sentence&amp;#39;].apply(lambda x:extract_entities_delim(x,&amp;#39;LOC&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;Next, I use TextBlob to compute the sentiment of each sentence and store store its polarity score in a separate column called &lt;code&gt;polarity_sentiment&lt;/code&gt; (TextBlob also returns a &lt;code&gt;subjectivity&lt;/code&gt; score but for simplicity, I will not use this score in my analysis). The polarity sentiment score is a value between -1 and 1. If the value is larger than 0, it means that the sentence has a positive sentiment. On the other hand, if the returned value is smaller than 0, it indicates that the sentiment of the sentence is negative.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def polarity_sentiment(text):
  blob = TextBlob(text)
  return blob.sentiment.polarity
  
candidates[&amp;#39;polarity_sentiment&amp;#39;] = candidates.sentence.apply(lambda x:polarity_sentiment(x))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5.Network Visualization&lt;/h2&gt;
&lt;p&gt;A network (graph) can nicely represent how individuals and entities were mentioned by candidates in their speeches. We have two types of nodes in this network:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The first set of nodes represent candidates on the debate stage (&lt;strong&gt;from nodes)&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The second set of nodes represent named entities (including the name of candidates themselves) that the candidates referred to in their speeches (&lt;strong&gt;to&lt;/strong&gt; &lt;strong&gt;nodes&lt;/strong&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If a candidate mentions a named entity in his/her speech, we connect the candidate node and the named entity node via an edge in our network. Also, It is fair to assume that the candidate-entity network should be a weighted network because candidates tend to place a varying level of importance on different issues, topics and people (named entities).&lt;/p&gt;
&lt;p&gt;We have two options for specifying weights for edges in the network:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can use the number of times that a candidate mentioned a named entity in his/her speech. This shows how much a named entity was important to that candidate.&lt;/li&gt;
&lt;li&gt;We can group by candidates and named entities and compute their average sentiment score. By doing so, we can measure how each candidate described these named entities. However, this approach might not be as accurate as we want.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having said that, it is time to go back to R and visualize the network of candidates and named entities using the &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;&lt;code&gt;ggraph&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;tidygraph&lt;/code&gt; libraries. For each class of named entities, I use &lt;code&gt;as_tbl_graph()&lt;/code&gt; function and create a unique graph table dataset, and visualize the network.&lt;/p&gt;
&lt;p&gt;So, let’s load the sentiment-entity dataset that I created in the Python environment to the R environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates &amp;lt;- py$candidates&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(candidates) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
X1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentence
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PERSON
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
ORG
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
GPE
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
NORP
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
LAW
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
LOC
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
polarity_sentiment
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
370
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Well, first, the economy.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
371
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
We know that not everyone is sharing in this prosperity.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
372
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
And Donald Trump just sits in the White House and gloats about what’s going on, when you have so many people that are having trouble affording college and having trouble affording their premiums.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Donald Trump;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
373
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
So I do get concerned about paying for college for rich kids.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.375
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
374
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I do.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
375
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
But I think my plan is a good one.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.700
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;the-candidateperson-network&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5.1 The Candidate/Person Network&lt;/h4&gt;
&lt;p&gt;First, I will visualize the candidate/person network. However, I should remind you of the fact that in the beginning of the democratic primary many democratic candidates were competing against each other in the race and also on the debate stage. If I were to visualize every individual that each candidate had ever in the network, the results would become unreadable. So, just like my last blog post, I selected a few democratic candidates to show my analysis.&lt;/p&gt;
&lt;p&gt;Furthermore, I will only highlight nodes corresponding to the top 6 democratic candidates and other interesting individuals including Donald Trump and Barack Obama.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interesting_individuals &amp;lt;-
 c(
  &amp;quot;Bernie Sanders&amp;quot; ,
  &amp;quot;Elizabeth Warren&amp;quot; ,
  &amp;quot;Mike Bloomberg&amp;quot;  ,
  &amp;quot;Pete Buttigieg&amp;quot; ,
  &amp;quot;Amy Klobuchar&amp;quot; ,
  &amp;quot;Joe Biden&amp;quot;,
  &amp;#39;Donald Trump&amp;#39;,
  &amp;#39;Barack Obama&amp;#39;
 )



custom_palette &amp;lt;-
  c(
    &amp;#39;Mike Bloomberg&amp;#39; = &amp;#39;#EDC948&amp;#39;,
    &amp;#39;Amy Klobuchar&amp;#39; = &amp;#39;#59A14F&amp;#39; ,
    &amp;#39;Joe Biden&amp;#39; = &amp;#39;#E15759&amp;#39;,
    &amp;#39;Pete Buttigieg&amp;#39; = &amp;#39;#B07AA1&amp;#39;,
    &amp;#39;Elizabeth Warren&amp;#39; =  &amp;#39;#F28E2B&amp;#39;,
    &amp;#39;Bernie Sanders&amp;#39; =  &amp;#39;#4E79A7&amp;#39; ,
    &amp;#39;Donald Trump&amp;#39; = &amp;#39;#BC3908&amp;#39;,
    &amp;#39;Barack Obama&amp;#39; = &amp;#39;#00afb9&amp;#39;,
    &amp;#39;Others&amp;#39; = &amp;#39;#540b0e&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persons_graph_table &amp;lt;- candidates %&amp;gt;%
 separate_rows(PERSON, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
 filter(speaker %in% interesting_individuals,PERSON != &amp;#39;&amp;#39;, debate %in% c(8, 9, 10)) %&amp;gt;%
 mutate(from = speaker, to = PERSON) %&amp;gt;%
 group_by(from, to) %&amp;gt;%
 summarize(n_mentions = n(),
      mean_sent = mean(polarity_sentiment),
      sent =case_when(mean_sent &amp;lt; -0.01 ~ &amp;#39;Negative&amp;#39;,
               mean_sent &amp;gt; 0.01 ~ &amp;#39;Positive&amp;#39;,
              TRUE ~ &amp;#39;Neutral&amp;#39; )
               ) %&amp;gt;%
 ungroup() %&amp;gt;%
 as_tbl_graph() %&amp;gt;%
 mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edge_cols &amp;lt;- c(&amp;#39;#e63946&amp;#39;,&amp;#39;#f1faee&amp;#39;,&amp;#39;#457B9D&amp;#39;)
ggraph(persons_graph_table, layout = &amp;#39;kk&amp;#39;) + 
  geom_edge_link(aes(edge_width = n_mentions,colour = sent )) +
  geom_node_point(aes(color = interesting_individuals ),size = 5) + 
  geom_node_label(aes(label = name,color = interesting_individuals),repel = TRUE,size= 8) + 
 scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Who Mentioned Whom in the 2020 Democratic Debates?&amp;#39;) +
  scale_edge_colour_manual(values = edge_cols) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look at the graph carefully, you will notice that there are three issues with this network. First of all, the sentiment scores does not necessarily indicate how a candidate thinks about that person. For instance, the edge between Bernie Sanders and Khashoggi is red (i.e. negative sentiment) but Bernie Sanders did not talk negatively about Khashoggi at all but rather how he was murdered. Secondly, there are several nodes in the network that belong to the same individual. For example, Bernie Sanders tends to address other candidates by their first names, but other (younger) candidates usually use the last name to address each other.&lt;/p&gt;
&lt;p&gt;The third issue is that some nodes do not represent a person. The transcript dataset is full of errors and many names are misspelled. Also, although Spacy is a very powerful library for NER, sometimes it gives us wrong results and its detected named entities are not always correct. For this reason, we also need to perform a post-processing step in which we remove some incorrectly spelled words or replaced them with their correct forms. I found two ways to deal with these issues: 1. or we can use a name matching algorithm to match the partial names of a person with its full name. This approach can be a bit challenging because we need to have a list of all possible full names, which is only available for the candidates.2. we can manually find undesirable names and replace them with what we want.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-candidate-names&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5.1 Matching candidate names&lt;/h4&gt;
&lt;p&gt;There is a python library called &lt;code&gt;fuzzywuzzy&lt;/code&gt; that can help us match two strings based on different similarity criteria. However, before using this library, I transform the original dataset into a longer dataset where each row belongs to a pair of candidate-person (from-to) and I move it back to our python environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- candidates %&amp;gt;%
 filter(PERSON != &amp;#39;&amp;#39;) %&amp;gt;%
 separate_rows(PERSON, sep = &amp;#39;;&amp;#39;) 
head(candidates_long)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-tibble-6-x-16&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A tibble: 6 x 16&lt;/h1&gt;
&lt;p&gt;X1 speaker background type gender debate day order sentence PERSON ORG&lt;br /&gt;
&lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
1 372 Amy Kl~ NA Cand~ female 1 1 11 And Don~ “Dona~ &lt;NA&gt;
2 372 Amy Kl~ NA Cand~ female 1 1 11 And Don~”&#34; &lt;NA&gt;
3 384 Amy Kl~ NA Cand~ female 1 1 99 It’s so~ “Bara~ &lt;NA&gt;
4 384 Amy Kl~ NA Cand~ female 1 1 99 It’s so~”&#34; &lt;NA&gt;
5 422 Amy Kl~ NA Cand~ female 1 1 329 But the~ “Dona~ &lt;NA&gt;
6 422 Amy Kl~ NA Cand~ female 1 1 329 But the~”&#34; &lt;NA&gt;
# … with 5 more variables: GPE &lt;chr&gt;, NORP &lt;chr&gt;, LAW &lt;chr&gt;, LOC &lt;chr&gt;,
# polarity_sentiment &lt;dbl&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;candidates_long = r.candidates_long&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the full names of democratic candidates.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
candidate_lists = pd.unique(candidates_long.speaker)
print(candidate_lists)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;Amy Klobuchar&amp;#39; &amp;#39;Andrew Yang&amp;#39; &amp;#39;Bernie Sanders&amp;#39; &amp;quot;Beto O&amp;#39;Rourke&amp;quot;
##  &amp;#39;Bill de Blasio&amp;#39; &amp;#39;Cory Booker&amp;#39; &amp;#39;Elizabeth Warren&amp;#39; &amp;#39;Eric Swalwell&amp;#39;
##  &amp;#39;Jay Inslee&amp;#39; &amp;#39;Joe Biden&amp;#39; &amp;#39;John Delaney&amp;#39; &amp;#39;John Hickenlooper&amp;#39;
##  &amp;#39;Julian Castro&amp;#39; &amp;#39;Kamala Harris&amp;#39; &amp;#39;Kirsten Gillibrand&amp;#39;
##  &amp;#39;Marianne Williamson&amp;#39; &amp;#39;Michael Bennet&amp;#39; &amp;#39;Mike Bloomberg&amp;#39; &amp;#39;Pete Buttigieg&amp;#39;
##  &amp;#39;Steve Bullock&amp;#39; &amp;#39;Tim Ryan&amp;#39; &amp;#39;Tom Steyer&amp;#39; &amp;#39;Tulsi Gabbard&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I define a python function called &lt;code&gt;match_names&lt;/code&gt; which uses &lt;code&gt;process.extractOne&lt;/code&gt; function to select the first matched named entity that has at least 80 percent similarity to a candidate’s full name. The matched names are stored in a separate columns called &lt;code&gt;dem_candidate_full_name&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from fuzzywuzzy import fuzz 
from fuzzywuzzy import process &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def match_names(name):
  try:
    return process.extractOne(name, candidate_lists,score_cutoff = 80)[0] 
  except:
    return None

candidates_long[&amp;#39;dem_candidate_full_name&amp;#39;] = candidates_long.PERSON.apply(lambda x: match_names(x) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can return to R and continue our analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- py$candidates_long
glimpse(candidates_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 3,238
## Variables: 16
## $ X1                 &amp;lt;dbl&amp;gt; 372, 372, 384, 384, 422, 422, 437, 437, 440, 440...
## $ speaker            &amp;lt;chr&amp;gt; &amp;quot;Amy Klobuchar&amp;quot;, &amp;quot;Amy Klobuchar&amp;quot;, &amp;quot;Amy Klobuchar...
## $ background         &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...
## $ type               &amp;lt;chr&amp;gt; &amp;quot;Candidate&amp;quot;, &amp;quot;Candidate&amp;quot;, &amp;quot;Candidate&amp;quot;, &amp;quot;Candidat...
## $ gender             &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;...
## $ debate             &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
## $ day                &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
## $ order              &amp;lt;dbl&amp;gt; 11, 11, 99, 99, 329, 329, 426, 426, 427, 427, 56...
## $ sentence           &amp;lt;chr&amp;gt; &amp;quot;And Donald Trump just sits in the White House a...
## $ PERSON             &amp;lt;chr&amp;gt; &amp;quot;Donald Trump&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;Barack Obama&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;Donald ...
## $ ORG                &amp;lt;chr&amp;gt; &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, ...
## $ GPE                &amp;lt;chr&amp;gt; &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, ...
## $ NORP               &amp;lt;chr&amp;gt; &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, ...
## $ LAW                &amp;lt;chr&amp;gt; &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;the Affordable Care Act;&amp;quot;, &amp;quot;the Aff...
## $ LOC                &amp;lt;chr&amp;gt; &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;NA&amp;quot;, ...
## $ polarity_sentiment &amp;lt;dbl&amp;gt; 0.0250000, 0.0250000, 0.0000000, 0.0000000, 0.50...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a-word-of-caution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A word of caution&lt;/h4&gt;
&lt;p&gt;We need to be very careful with the results of the name matching algorithm. There are too many politicians with the name ‘John’ and a John might refer to “John McCain” or “John Bolton” not the candidate “John Hickenlooper”. So, as a post-processing step, I manually explore the dataset to correct the few mistakes that the matching algorithm had made.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidates_long &amp;lt;- candidates_long %&amp;gt;%
  mutate(PERSON = if_else(
    !is.na(dem_candidate_full_name),
    dem_candidate_full_name,
    PERSON),
  PERSON = case_when(PERSON == &amp;#39;John Hickenlooper&amp;#39; &amp;amp; str_detect(sentence,&amp;#39;McCain&amp;#39;) ~ &amp;#39;John McCain&amp;#39;,
                      PERSON == &amp;#39;John Hickenlooper&amp;#39; &amp;amp; str_detect(sentence,&amp;#39;Bolton&amp;#39;) ~ &amp;#39;John Bolton&amp;#39;,
                      PERSON == &amp;#39;John Delaney&amp;#39; &amp;amp; speaker == &amp;#39;Joe Biden&amp;#39; ~&amp;#39;John McCain&amp;#39;,
                      PERSON == &amp;#39;John Delaney&amp;#39; &amp;amp; speaker == &amp;#39;   Amy Klobuchar&amp;#39; ~&amp;#39;John McCain&amp;#39;,
                      TRUE ~  PERSON)
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-name-correction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.2 Manual name correction&lt;/h3&gt;
&lt;p&gt;We have a better dataset now, but there are still a lot of inaccurate named entities or inconsistencies in the dataset. Let’s start by removing named entities that do not correspond with a real person.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_person &amp;lt;-
  c(
    &amp;#39;y adema&amp;#39; ,
    &amp;#39;Appalachia&amp;#39; ,
    &amp;#39;AUMF&amp;#39; ,
    &amp;#39;bias&amp;#39;,
    &amp;#39;nondisclosur&amp;#39; ,
    &amp;#39;Mathew 25&amp;#39;,
    &amp;#39;Idlib&amp;#39;,
    &amp;#39;ye&amp;#39;,
    &amp;#39;Everytown&amp;#39;,
    &amp;#39;Kurd&amp;#39;,
    &amp;#39;Roe V.&amp;#39;,
    &amp;#39;Wade&amp;#39;,
    &amp;#39;Trumpism&amp;#39;,
    &amp;#39;Casey&amp;#39;,
    &amp;#39;brown&amp;#39;,
    &amp;#39;Grandpa&amp;#39;,
    &amp;#39;Dad&amp;#39;,
    &amp;quot;Josh&amp;quot;,
    &amp;#39;Uighurs&amp;#39;,
    &amp;#39;Roe&amp;#39;,
    &amp;#39;PolitiFact&amp;#39;,
    &amp;#39;Latinx&amp;#39;,
    &amp;#39;Brady&amp;#39;,
    &amp;#39;pre-K.&amp;#39;,
    &amp;#39;Brady Bill&amp;#39;,
    &amp;#39;pro-Israel&amp;#39;,
    &amp;#39;ho&amp;#39;,
    &amp;#39;Dreamer&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have another problem left. Some individuals were mentioned in different ways and we have several nodes for them in the graph. To solve this issue, I simply use &lt;code&gt;str_detect&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt; package to manually modify the them names. I must say this was the most boring and time-consuming part of my analysis!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persons_graph_table &amp;lt;-  candidates_long %&amp;gt;%
  filter(speaker %in% interesting_individuals,
         !PERSON %in% non_person,
         nchar(PERSON)&amp;gt;1) %&amp;gt;%
  dplyr::rowwise() %&amp;gt;%
  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %&amp;gt;%
  mutate(from = speaker, to = PERSON) %&amp;gt;%
  mutate(
    to = case_when(
      to %in% c(
        &amp;#39;Donald&amp;#39;,
        &amp;#39;Donald Trump&amp;#39;,
        &amp;#39;Donald trump&amp;#39;,
        &amp;#39;Trump&amp;#39;,
        &amp;#39;President Trump&amp;#39;,
        &amp;quot;Donald Trump&amp;#39;s&amp;quot;
      ) ~ &amp;#39;Donald Trump&amp;#39;,
      to %in% c(&amp;#39;Hillar&amp;#39;,
                &amp;#39;Clinton&amp;#39;,
                &amp;#39;Hillary&amp;#39;) ~ &amp;#39;Hillary Clinton&amp;#39;,
      to %in% c(&amp;#39;Obama&amp;#39;,
                &amp;#39;Barack&amp;#39;) ~ &amp;#39;Barack Obama&amp;#39;,
      str_detect(to, &amp;#39;Trump&amp;#39;) ~ &amp;#39;Donald Trump&amp;#39;,
      str_detect(to, &amp;#39;Vind&amp;#39;) ~ &amp;#39;Vindman&amp;#39;,
      str_detect(to, &amp;#39;Assad&amp;#39;) ~ &amp;#39;Assad&amp;#39;,
      str_detect(to, &amp;#39;McCarthy&amp;#39;) ~ &amp;#39;McCarthy&amp;#39;,
      str_detect(to, &amp;#39;Trudeau&amp;#39;) ~ &amp;#39;Justin Trudeau&amp;#39;,
      str_detect(to, &amp;#39;Bannon&amp;#39;) ~ &amp;#39;Steve Bannon&amp;#39;,
      str_detect(to, &amp;#39;Netanyahu&amp;#39;) ~ &amp;#39;Netanyahu&amp;#39;,
      str_detect(to, &amp;#39;Martin Luther&amp;#39;) ~ &amp;#39;Martin Luther King&amp;#39;,
      str_detect(to, &amp;#39;Mandela&amp;#39;) ~ &amp;#39;Mandela&amp;#39;,
      str_detect(to, &amp;#39;Xi&amp;#39;) ~ &amp;#39;Xi Jinping&amp;#39;,
      str_detect(to, &amp;#39;Putin&amp;#39;) ~ &amp;#39;Putin&amp;#39;,
      str_detect(to, &amp;#39;Mitch&amp;#39;) ~ &amp;#39;Mitch Mcconnell&amp;#39;,
      str_detect(to, &amp;#39;Lindsey&amp;#39;) ~ &amp;#39;Lindsey Graham&amp;#39;,
      str_detect(to, &amp;#39;Romney&amp;#39;) ~ &amp;#39;Mitt Romney&amp;#39;,
      str_detect(to, &amp;#39;George&amp;#39;) ~ &amp;#39;George Bush&amp;#39;,
      str_detect(to, &amp;#39;Bush&amp;#39;) ~ &amp;#39;George Bush&amp;#39;,
      str_detect(to, &amp;#39;Turner&amp;#39;) ~ &amp;#39;Nina Turner&amp;#39;,
      str_detect(to, &amp;#39;Clyburn&amp;#39;) ~ &amp;#39;Jim Clyburn&amp;#39;,
      str_detect(to, &amp;#39;Cheney&amp;#39;) ~ &amp;#39;Dick Cheney&amp;#39;,
      str_detect(to, &amp;#39;Shaheen&amp;#39;) ~ &amp;#39;Jeanne Shaheen&amp;#39;,
      str_detect(to, &amp;#39;Hart&amp;#39;) ~ &amp;#39;Quentin Hart&amp;#39;,
      str_detect(to, &amp;#39;Cokie&amp;#39;) ~ &amp;#39;Cokie Roberts&amp;#39;,
      str_detect(to, &amp;#39;Kelly&amp;#39;) ~ &amp;#39;Laura Kelly&amp;#39;,
      str_detect(to, &amp;#39;Berry&amp;#39;) ~ &amp;#39;Seth Berry&amp;#39;,
      str_detect(to, &amp;#39;Grassley&amp;#39;) ~ &amp;#39;Chuck Grassley&amp;#39;,
      str_detect(to, &amp;#39;Tommy&amp;#39;) ~ &amp;#39;Tom Steyer&amp;#39;,
      str_detect(to, &amp;#39;Pelosi&amp;#39;) ~ &amp;#39;Nancy Pelosi&amp;#39;,
      str_detect(to, &amp;#39;Kim&amp;#39;) ~ &amp;#39;Kim Jong-un&amp;#39;,
      str_detect(to, &amp;#39;Pence&amp;#39;) ~ &amp;#39;Mike Pence&amp;#39;,
      str_detect(to, &amp;#39;Schatz&amp;#39;) ~ &amp;#39;Brian Schatz&amp;#39;,
      str_detect(to, &amp;#39;Gates&amp;#39;) ~  &amp;#39;Robert Gates&amp;#39;,
      str_detect(to, &amp;#39;Jill&amp;#39;) ~ &amp;#39;Jill Biden&amp;#39;,
      str_detect(to, &amp;#39;Casey Jo&amp;#39;) ~ &amp;#39;Casey Jo&amp;#39;,
      str_detect(to, &amp;#39;Franklin&amp;#39;) |
      str_detect(to, &amp;#39;FDR&amp;#39;) ~ &amp;#39;Franklin D. Roosevelt&amp;#39;,
      str_detect(to, &amp;#39;Welch&amp;#39;) ~ &amp;#39;Joseph Welch&amp;#39;,
      str_detect(to, &amp;#39;Beau&amp;#39;) ~ &amp;#39;Beau Biden&amp;#39;,
      str_detect(to, &amp;#39;Rudy Giuliani&amp;#39;) ~ &amp;#39;Rudy Giuliani&amp;#39;,
      str_detect(to, &amp;#39;Bolton&amp;#39;) ~ &amp;#39;John Bolton&amp;#39;,
      str_detect(to, &amp;#39;McCain&amp;#39;) ~ &amp;#39;John McCain&amp;#39;,
      str_detect(to, &amp;#39;Truman&amp;#39;) ~ &amp;#39;Harry Truman&amp;#39;,
      str_detect(to, &amp;#39;Dunford&amp;#39;) ~ &amp;#39;Joe Dunford&amp;#39;,
      str_detect(to, &amp;#39;Breyer&amp;#39;) ~ &amp;#39;Justice Breyer&amp;#39;,
      str_detect(to, &amp;#39;Cindy&amp;#39;) ~ &amp;#39;Cindy McCain&amp;#39;,
      to == &amp;#39;Dick&amp;#39; ~ &amp;#39;Uncle Dick&amp;#39;,
      to == &amp;#39;Charles&amp;#39; ~ &amp;#39;Charles Fried&amp;#39;,
      to == &amp;#39;JFK&amp;#39; |
        (to == &amp;#39;Kennedy&amp;#39; &amp;amp; speaker == &amp;#39;Joe Biden&amp;#39;) ~ &amp;#39;John F. Kennedy&amp;#39;,
      to == &amp;#39;Kennedy&amp;#39; &amp;amp; speaker == &amp;#39;Amy Klobuchar&amp;#39; ~ &amp;#39;Ted Kennedy&amp;#39;,
      to %in% c(&amp;#39;Joey&amp;#39;) ~ &amp;#39;Himself&amp;#39;,
      to %in% c(
        &amp;#39;Ady&amp;#39;,
        &amp;#39;Carl&amp;#39;,
        &amp;#39;Ady Barkan&amp;#39;,
        &amp;#39;Derek&amp;#39;,
        &amp;#39;Mark&amp;#39;,
        &amp;#39;Salvador&amp;#39;,
        &amp;#39;Rachael&amp;#39;,
        &amp;#39;Nicole&amp;#39;
      ) ~ &amp;#39;American Constituents&amp;#39;,
      to %in% c(
        &amp;#39;David&amp;#39;,
        &amp;#39;Chuck&amp;#39;,
        &amp;#39;Wolf&amp;#39;,
        &amp;#39;   Wolf&amp;#39;,
        &amp;#39;Margaret&amp;#39;,
        &amp;#39;Brianne&amp;#39;,
        &amp;#39;Adam&amp;#39;,
        &amp;#39;Jake&amp;#39;,
        &amp;#39;Norah&amp;#39;,
        &amp;#39;Judy&amp;#39;,
        &amp;#39;Gayle&amp;#39;,
        &amp;#39;Dana&amp;#39;,
        &amp;#39;Jorge - it&amp;#39;,
        &amp;#39;Lester&amp;#39;,
        &amp;#39;Rachel&amp;#39;
      ) ~ &amp;#39;Moderator&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can visualize the network with modified node names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edge_cols &amp;lt;- c(&amp;#39;#e63946&amp;#39;, &amp;#39;#f1faee&amp;#39;, &amp;#39;#457B9D&amp;#39;)
ggraph(persons_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), color = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals),size = 6) +
  geom_node_label(
    aes(label = name, color = interesting_individuals),
    repel = TRUE,
    size = 8,
    label.r = 0.4,
    check_overlap = TRUE
  ) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Individuals Mentioned by Top Democratic Candidates During the Democratic Primary Debates&amp;#39;,
       subtitle = &amp;#39;This graph shows which individuals or politicians were mentioned by top 6 democratic candidates over the course of first ten priamary debates.&amp;#39;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;candidates-interaction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.3 Candidates interaction&lt;/h3&gt;
&lt;p&gt;In the last sections, I explained how the top 6 remaining candidates mentioned other individuals during their speeches on the debate stage. However, with a little bit of modification to our previous chunk of code, we can extend the analysis and investigate how all democratic candidates interacted with each other over the course of 10 debates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The name of all candidaes
all_candidates &amp;lt;- candidates_long %&amp;gt;%
  distinct(speaker) %&amp;gt;%
  pull()

candidates_graph_table &amp;lt;- candidates_long %&amp;gt;%
  filter(!is.na(dem_candidate_full_name),
         dem_candidate_full_name == PERSON) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(debate = as.factor(debate)) %&amp;gt;%
  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %&amp;gt;%
  mutate(from = speaker, to = dem_candidate_full_name) %&amp;gt;%
  group_by(from, to, debate) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph(directed = TRUE) %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;)) %&amp;gt;%
  activate(nodes) %&amp;gt;%
  mutate(bet_cent = centrality_betweenness(),
         deg_cent = centrality_degree())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preparing the circular layout for the network
# Credit to https://www.timlrx.com/2018/10/14/visualising-networks-in-asoiaf-part-ii/ for helping me with the circular layout
full_layout &amp;lt;-
  create_layout(graph = candidates_graph_table,
                layout = &amp;quot;linear&amp;quot;,
                circular = T)

xmin &amp;lt;- min(full_layout$x)
xmax &amp;lt;- max(full_layout$x)
ymin &amp;lt;- min(full_layout$y)
ymax &amp;lt;- max(full_layout$y)

ggraph(
  full_layout,
  layout = &amp;#39;manual&amp;#39;,
  x = x,
  y = y,
  circular = TRUE
) +
  geom_edge_arc(aes(edge_width = n_mentions,
                    alpha = n_mentions,),
                colour = &amp;#39;#540b0e&amp;#39;,) +
  geom_node_point(aes(color = interesting_individuals, size = deg_cent + 40)) +
  geom_node_text(
    aes(
      label = name,
      color = interesting_individuals,
      x = x * 1.15,
      y = y * 1.15,
      angle = ifelse(
        atan(-(x / y)) * (180 / pi) &amp;lt; 0,
        90 + atan(-(x / y)) * (180 / pi),
        270 + atan(-x / y) * (180 / pi)
      )
    ),
    size = 8
  ) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;The Network of Interactions Among Democratic Candidates During Democratic Primary Debates&amp;#39;,
       #subtitle = &amp;#39;This graph shows how democtratic candidates mentioned other candidates on the debate stage.&amp;#39;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;) +
  expand_limits(x = c(xmin - 0.2, xmax + 0.2),
                y = c(ymin - 0.2, ymax + 0.2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are self-explanatory and satisfying. One also can make an animation and show the network over time.&lt;/p&gt;
&lt;p&gt;Now, let’s repeat the same steps and visualize the network for other types of named entities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;organization-and-companies-named-entities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.4 Organization and companies named entities&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_org &amp;lt;- c(&amp;#39;Trump&amp;#39;,&amp;#39;Vindmen&amp;#39;,&amp;#39;a New Yorker&amp;#39;,&amp;#39;Title&amp;#39;,&amp;#39;Obama&amp;#39;,&amp;quot;Donald Trump&amp;#39;s&amp;quot;,&amp;#39;Bernie&amp;#39;,&amp;#39;state&amp;#39;,&amp;#39;Court&amp;#39;,&amp;#39;Ours&amp;#39;,&amp;#39;Education&amp;#39;)
non_org_laws &amp;lt;- c(&amp;#39;Green New Deal&amp;#39;,&amp;#39;Federal Controlled Substance Act&amp;#39;)

org_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(ORG, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(ORG)) %&amp;gt;%
  mutate(ORG = str_remove_all(ORG, &amp;#39;the &amp;#39;),
         ORG = str_remove_all(ORG, &amp;#39;this &amp;#39;),) %&amp;gt;%
  filter(
    
    speaker %in% interesting_individuals,!ORG %in% non_org,!ORG %in% non_org_laws,
    debate %in% c(6, 7, 8, 9, 10),
    nchar(ORG)&amp;gt;1
  ) %&amp;gt;%
  mutate(from = speaker, to = ORG) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      to %in% c(&amp;#39;United Nations&amp;#39;,
                &amp;#39;U.N.&amp;#39;,
                &amp;#39;UN&amp;#39;) ~ &amp;#39;United Nations&amp;#39;,
      str_detect(to, &amp;#39;Department&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;State&amp;#39;) ~ &amp;#39;Department of State&amp;#39;,
      str_detect(to, &amp;#39;Department&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;Defence&amp;#39;) ~ &amp;#39;Department of State&amp;#39;,
      str_detect(to, &amp;#39;Supreme&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;Court&amp;#39;) ~ &amp;#39;Supreme Court&amp;#39;,
      str_detect(to, &amp;#39;Treasury&amp;#39;) ~ &amp;#39;Department of the Treasury&amp;#39;,
      str_detect(to, &amp;#39;Unitetd&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;State&amp;#39;) ~ &amp;#39;United State&amp;#39;,
      str_detect(to, &amp;#39;Yale&amp;#39;) ~ &amp;#39;Yale&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(org_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions,alpha=n_mentions),
    colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 7) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Organizations and Institutions Mentioned by Top Democratic Candidates During the Debates&amp;#39;,
       subtitle = &amp;#39;This plot shows which organizations and institutions were mentioned by top democratic candidates during the last 5 debates.&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-countries-and-cities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.5 The network of named entities for countries and cities&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gpe_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(GPE, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(GPE)) %&amp;gt;%
  mutate(GPE = str_remove_all(GPE, &amp;#39;the &amp;#39;),
         GPE = str_remove_all(GPE, &amp;#39;this &amp;#39;)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,
         debate %in% c(6, 7, 8, 9, 10)) %&amp;gt;%
  mutate(from = speaker, to = GPE) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(gpe_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions),
                 colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 9) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Countries, Cities and States Mentioned by Top Democratic Candidates During the Last Five Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-laws&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.6 The network of named entities for laws&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_law &amp;lt;-
  c(&amp;#39;the ZIP Code&amp;#39;, &amp;quot;&amp;quot;)

law_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(LAW, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(LAW)) %&amp;gt;%
  mutate(LAW = str_remove_all(LAW, &amp;#39;the &amp;#39;)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,!LAW %in% non_law) %&amp;gt;%
  mutate(from = speaker, to = LAW) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      str_detect(to , &amp;#39;Constitution&amp;#39;) ~ &amp;#39;Constitution&amp;#39;,
      str_detect(to , &amp;#39;Roe&amp;#39;) ~ &amp;#39;Roe V. Wade&amp;#39;,
      str_detect(to , &amp;#39;War Powers Act&amp;#39;) ~ &amp;#39;War Powers Act&amp;#39;,
      str_detect(to , &amp;#39;New START&amp;#39;) ~ &amp;#39;New START Treaty&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(law_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 7) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Laws Mentioned by Top Democratic Candidates During the First Ten Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;2880&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-network-of-named-entities-for-nationalities-religious-or-political-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5.7 The network of named entities for nationalities, religious or political groups&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_norp &amp;lt;- c(&amp;#39;Coronavirus&amp;#39;, &amp;#39;&amp;#39;)
norp_graph_table &amp;lt;- candidates %&amp;gt;%
  separate_rows(NORP, sep = &amp;#39;;&amp;#39;) %&amp;gt;%
  filter(!is.na(NORP)) %&amp;gt;%
  mutate(NORP = singularize(NORP)) %&amp;gt;%
  filter(speaker %in% interesting_individuals,!NORP %in% non_norp,
         debate %in% c(6, 7, 8, 9, 10)) %&amp;gt;%
  mutate(from = speaker, to = NORP) %&amp;gt;%
  group_by(from, to) %&amp;gt;%
  summarize(n_mentions = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    to = case_when(
      str_detect(to, &amp;#39;African&amp;#39;) &amp;amp;
        str_detect(to, &amp;#39;American&amp;#39;) ~ &amp;#39;African-American&amp;#39;,
      str_detect(to, &amp;#39;republican&amp;#39;) ~ &amp;#39;Republican&amp;#39;,
      str_detect(to, &amp;#39;Democrat&amp;#39;) ~ &amp;#39;Democrat&amp;#39;,
      str_detect(to, &amp;#39;Jew&amp;#39;) ~ &amp;#39;Jew&amp;#39;,
      str_detect(to, &amp;#39;Palestinian&amp;#39;) ~ &amp;#39;Palestinian&amp;#39;,
      TRUE ~ to
    )
  ) %&amp;gt;%
  as_tbl_graph() %&amp;gt;%
  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, &amp;#39;Others&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggraph(norp_graph_table, layout = &amp;#39;nicely&amp;#39;) +
  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = &amp;#39;#540b0e&amp;#39;) +
  geom_node_point(aes(color = interesting_individuals), size = 5) +
  geom_node_label(aes(label = name, color = interesting_individuals),
                  repel = TRUE,
                  size = 10) +
  scale_color_manual(values = custom_palette) +
  labs(title = &amp;#39;Nationalities, religious or Political Groups Mentioned by Top Democratic Candidates During the Last Five Primary Debates&amp;#39;,
       subtitle = &amp;#39;&amp;#39;,
       caption = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-08-analayzing-the-2020-democratic-presidential-debates-part-2/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;p&gt;##Resources:
A very useful place to learn more how spacy works the spacy’s online course by one of its founders and developers.
[1] &lt;a href=&#34;https://course.spacy.io/en/&#34; class=&#34;uri&#34;&gt;https://course.spacy.io/en/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;amp;t=676s&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=IqOJU1-_Fi0&amp;amp;t=676s&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is
&lt;a href=&#34;http://www.favstats.eu/post/demdebates/&#34; class=&#34;uri&#34;&gt;http://www.favstats.eu/post/demdebates/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the 2020 Democratic Party Presidential Debates - Part 1</title>
      <link>/post/2020-02-23-the-most-eloquent-democratic-candidate/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-23-the-most-eloquent-democratic-candidate/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I am not a US citizen, nor have I been to the United States, but that does not mean that I should not care about the result of the US presidential election. The outcome of the election plays an important role in my life and almost everyone’s else around the world. So, I have been following the US politics for a few years.&lt;/p&gt;
&lt;p&gt;I consider everything and every issue around me as a data science problem and an opportunity to use data science. The US presidential election is not an exception, and a few weeks ago, I was wondering how I can use data science techniques to analyze the presidential election in the U.S and the Democratic Primary elections. Luckily, a few days later, I found &lt;a href=&#34;https://github.com/favstats/demdebates2020&#34;&gt;an amazing R package&lt;/a&gt; on Twitter which contains the transcripts of speeches given by all candidates in the Democratic Party’s debates.&lt;/p&gt;
&lt;p&gt;Now that I had access to a dataset, it was time to think about how I should use it and how I can extract useful knowledge from it. There are many possibilities for studying the debates’ transcript, but I was interested in investigating 3 aspects of the debates and candidates:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Determining the most eloquent presidential candidate.&lt;/li&gt;
&lt;li&gt;Sentiment analysis of the transcripts to find out who used positive or most negative words on the stage.&lt;/li&gt;
&lt;li&gt;Network analysis of the transcripts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post, I will explain how I used R and the tidytext package to investigate the first two points. Discussing how I approached needs a much longer and separate post, so I will write about here.&lt;/p&gt;
&lt;div id=&#34;who-is-the-most-eloquent-candidate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Who is the most eloquent candidate?&lt;/h3&gt;
&lt;p&gt;Everyone agrees that Donald Trump only uses a basic English vocabulary in his speeches and tweets, and he is not the most eloquent president in the US history. But what about his future challenger from the democratic party and how skillful his opponents are with words? For this reason, I analyzed the transcripts from this point of view to determine how good they are with words.&lt;/p&gt;
&lt;p&gt;An eloquent person has gained a rich vocabulary and uses a wide range of complex words in his/her speeches. An inarticulate person has a limited vocabulary and uses simple and everyday words in his/her writings or conversations. Ideally, I think we can measure eloquence by counting the number of unique words and the number of sophisticated words that a person uses.&lt;/p&gt;
&lt;p&gt;However, I could not find a dataset of English words along with their perceived complexity. So, to measure the eloquence of the presidential candidates, I defined two other metrics that I hope can serve as an approximation to the truth:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Vocabulary size&lt;/strong&gt;: The ratio of unique words that a candidate used in his/her debate speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vocabulary complexity&lt;/strong&gt;: The ratio of stop-words (words that are very common and rarely add much value to the content). Intuitively, a lower ratio of stopword usage by a candidate shows that the candidate is more articulate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I use the &lt;code&gt;Tidytext&lt;/code&gt; library and its stopword list to compute my defined metrics. Note that there are several stopword lexicons out there, and the choice of lexicon can slightly change the outcome.&lt;/p&gt;
&lt;p&gt;It’s time to start the analysis itself, but before that I need to import a few libraries and set and customize the theme that I am going to use for visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(demdebates2020)
library(tidytext)
library(tidyverse)
library(gghighlight)
library(ggthemes)
library(kableExtra)
#set a default theme for visualization
theme_set(theme_fivethirtyeight())
#customize the default theme 
theme_update(legend.position = &amp;#39;none&amp;#39;,
             text = element_text(family = &amp;#39;Montserrat&amp;#39;),
      plot.title = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;,size = 25, margin = margin(0, 0, 20, 0)),
      axis.text.x = element_blank(),
      axis.text.y = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;,size = 15, margin = margin(0, 0, 20, 0)),
      panel.spacing = unit(2, &amp;quot;points&amp;quot;),
      axis.title.x = element_blank(),
      axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The field of the democratic primary election is full of candidates. So, for the sake of simplicity and clarity, I am going to analyze candidates that are still in the race (as of February 23rd) and were present in the last two democratic debate. It means that I will compare six democratic candidates including &lt;em&gt;Bernie Sanders&lt;/em&gt;, &lt;em&gt;Elizabeth Warren&lt;/em&gt;, &lt;em&gt;Mike Bloomberg&lt;/em&gt;, &lt;em&gt;Pete Buttigieg&lt;/em&gt;, &lt;em&gt;Amy Klobuchar&lt;/em&gt; and &lt;em&gt;Joe Biden&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;speakers &amp;lt;- debates %&amp;gt;%
  filter(!is.na(speech), type == &amp;#39;Candidate&amp;#39; ,debate == 9) %&amp;gt;%
  distinct(speaker) %&amp;gt;%
  pull(speaker)
speakers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Bernie Sanders&amp;quot;   &amp;quot;Elizabeth Warren&amp;quot; &amp;quot;Mike Bloomberg&amp;quot;   &amp;quot;Pete Buttigieg&amp;quot;  
## [5] &amp;quot;Amy Klobuchar&amp;quot;    &amp;quot;Joe Biden&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before computing my desirable metrics, I should tokenize and transform the transcript into a &lt;em&gt;&lt;a href=&#34;https://www.tidytextmining.com/tidytext.html&#34;&gt;tidy format&lt;/a&gt;&lt;/em&gt; (one word per row). After that, I will create a logical variable called &lt;code&gt;is_stop_word&lt;/code&gt; to determine whether a word is stopword or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_vocab_df &amp;lt;- debates %&amp;gt;%
  filter(!is.na(speech), type == &amp;#39;Candidate&amp;#39;,speaker %in% speakers) %&amp;gt;%
  unnest_tokens(word, speech) %&amp;gt;%
  mutate(is_stop_word = word %in% stop_words$word) %&amp;gt;%
  group_by(speaker) %&amp;gt;%
  summarize(stop_word_ratio = sum(is_stop_word) / n(),
            vocab_size = n_distinct(word)/ n())  %&amp;gt;% 
  arrange(stop_word_ratio) 

#show the output  
head(debate_vocab_df)  %&amp;gt;% 
kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
stop_word_ratio
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
vocab_size
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bernie Sanders
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6588343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0882325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6942596
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0911883
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pete Buttigieg
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6978231
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1147498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7189251
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1027148
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe Biden
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7195672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0842458
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mike Bloomberg
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7415574
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2083830
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After computing my metrics it is time to visualize them with &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-word-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stop word ratio&lt;/h3&gt;
&lt;p&gt;I will start by visualizing the stop word ratio for each candidate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_palette &amp;lt;-
  c(
    &amp;#39;Mike Bloomberg&amp;#39; = &amp;#39;#EDC948&amp;#39;,
    &amp;#39;Amy Klobuchar&amp;#39; = &amp;#39;#59A14F&amp;#39; ,
    &amp;#39;Joe Biden&amp;#39; = &amp;#39;#4E79A7&amp;#39;,
    &amp;#39;Pete Buttigieg&amp;#39; = &amp;#39;#B07AA1&amp;#39;,
    &amp;#39;Elizabeth Warren&amp;#39; =  &amp;#39;#F28E2B&amp;#39;,
    &amp;#39;Bernie Sanders&amp;#39; = &amp;#39;#E15759&amp;#39; 
  )

debate_vocab_df %&amp;gt;%
  mutate(speaker = fct_reorder(speaker,stop_word_ratio,.desc =  TRUE)) %&amp;gt;% 
  ggplot(aes(x = speaker , y = stop_word_ratio,fill = speaker)) +
  geom_col(show.legend = FALSE) +
  geom_label(aes(label = round(stop_word_ratio,digits = 3)) ,size = 5) +
  coord_flip() +
  scale_fill_manual(values = custom_palette) +
  labs(title = &amp;quot;The ratio of stopwords used by Democratic canidates in the debates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that Bernie Sanders had used the lowest percentage of stopwords in his speeches. On the other hand, Mike Bloomberg had used the largest ratio of stopwords in his first and only debates so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vocabulary-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vocabulary size&lt;/h3&gt;
&lt;p&gt;The vocabulary size metric shows a different trend as Mike Bloomberg has the highest score among the rest of the candidates. Of course and as I mentioned before, Bloomberg has appeared only once on the debate stage, and it might be too soon to draw a conclusion about his eloquence.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; debate_vocab_df %&amp;gt;%
  mutate(speaker = fct_reorder(speaker,vocab_size,.desc =  FALSE)) %&amp;gt;% 
  ggplot(aes(x = speaker , y = vocab_size,fill = speaker)) +
  geom_col(show.legend = FALSE) +
  geom_label(aes(label = round(vocab_size,digits = 3) ,size = 8)) +
  coord_flip() +
  scale_fill_manual(values = custom_palette) +
  labs(title = &amp;quot;The ratio of unique Words used by Democratic candidates in the Debates&amp;quot;,
       caption = &amp;#39;Visualization: @m_cnakhaee\n\n Source: https://github.com/favstats/demdebates2020&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, there is no outright winner in terms of language skills among Democratic candidates. Bernie Sanders had the best score in terms of vocabulary complexity, but he has the least ratio of unique words among his competitors. Also, one can argue that being eloquent might not be advantage to a candidate and win them an election ( look at the person who is the current president). Finally, I must emphasize that my metrics are rather arbitrary and should be taken with a grain of salt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment analysis&lt;/h3&gt;
&lt;p&gt;In this part of my blog post, I examine how the language used by each top candidate had changed over the course of debates. I will use the tidy text approach to measure sentiment in the text. There are four main sentiment lexicons in the tidytext library, but in this experiment I am just using the &lt;a href=&#34;https://rdrr.io/cran/textdata/man/lexicon_loughran.html&#34;&gt;Loughran lexicon&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment &amp;lt;- debates %&amp;gt;%
  filter(type == &amp;#39;Candidate&amp;#39;, is.na(background)) %&amp;gt;%
  unnest_tokens(word, speech) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;loughran&amp;quot;))

head(debate_senteneces_sentiment,3) %&amp;gt;% 
kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
speaker
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
background
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
debate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
day
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sentiment
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
destroyed
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elizabeth Warren
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
corruption
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amy Klobuchar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Candidate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prosperity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment %&amp;gt;%
group_by(speaker,debate,sentiment)%&amp;gt;%
summarise(sentiment_score = n())%&amp;gt;%
ungroup() %&amp;gt;%
filter(speaker %in% speakers) %&amp;gt;%
ggplot(aes(x = debate,y= sentiment_score,color = speaker)) +
geom_line(size = 3,alpha = 0.8) +
  geom_point(size  = 4) +
scale_color_manual(values = custom_palette) +
  scale_x_continuous(breaks = seq(1,10),labels = seq(1,10)) +
  labs(title = &amp;#39;What kinds of Language Have the Top Deomocratic Candidates Used in the Debates?&amp;#39;,
       color = &amp;#39;&amp;#39;,
       x = &amp;#39;&amp;#39;) +
facet_wrap(sentiment ~ . ,ncol = 1,scales = &amp;#39;free&amp;#39;) +
theme(strip.text = element_text(size = 20),
      strip.background = element_rect(fill = &amp;#39;gray80&amp;#39;) ,
     legend.text = element_text(size = 15),
     title = element_text(size = 25),
     legend.position = &amp;#39;top&amp;#39;,
     axis.text.y = element_blank(),
     axis.text.x = element_text(size = 15),
     axis.ticks.y = element_blank(),
     axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An the overall sentiment score for each candidate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;debate_senteneces_sentiment %&amp;gt;%
group_by(speaker,sentiment)%&amp;gt;%
summarise(sentiment_score = n())%&amp;gt;%
ungroup() %&amp;gt;%
filter(speaker %in% speakers) %&amp;gt;%
  ggplot(aes(speaker,sentiment_score,fill = speaker)) +
  geom_col(alpha =0.8) +
scale_fill_manual(values = custom_palette) +
  coord_flip() +
  facet_wrap(~sentiment, ncol = 5) +
  theme(strip.text = element_text(size = 20),
      strip.background = element_rect(fill = &amp;#39;gray80&amp;#39;) ,
     title = element_text(size = 25),
     legend.position = &amp;#39;none&amp;#39;,
     axis.title.y = element_blank(),
     axis.title.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-23-the-most-eloquent-democratic-candidate/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that negativity has been the most predominant emotion during Democratic debates (which absolutely makes sense since they want to unseat a president). Bernie Sanders used the highest number of words with negative sentiment in his remarks.&lt;/p&gt;
&lt;p&gt;Performing sentiment analysis in tidytext is straightforward and easy, but sometimes the results are not what we hope and what the candidate actually meant. So, also take these results with a grain of salt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;If you are interested to learn more about tidy text mining in r, the following links can be helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tidytextmining.com/tidytext.html&#34; class=&#34;uri&#34;&gt;https://www.tidytextmining.com/tidytext.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://education.rstudio.com/blog/2020/02/conf20-tidytext/&#34; class=&#34;uri&#34;&gt;https://education.rstudio.com/blog/2020/02/conf20-tidytext/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel for Scikit-Learn Users and Vise Versa</title>
      <link>/post/2020-02-14-tidymodel-for-scikit-learn-users-and-vise-versa/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-14-tidymodel-for-scikit-learn-users-and-vise-versa/</guid>
      <description>


&lt;p&gt;Advantages
There are many ways to do one thing
The output is a table which you can use as an input to everything that works with a table&lt;/p&gt;
&lt;p&gt;Disadvantages&lt;/p&gt;
&lt;p&gt;##Classification Models&lt;/p&gt;
&lt;div id=&#34;regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Models&lt;/h2&gt;
&lt;div id=&#34;making-prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Prediction&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model selection&lt;/h3&gt;
&lt;p&gt;reasonable defaults for tidymodel&lt;/p&gt;
&lt;p&gt;tidymodel by default tuning paramters are set for us. We can also specify them ourselves.&lt;/p&gt;
&lt;p&gt;you can even tune the preprocessing steps in Tidymodel.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pipelines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pipelines&lt;/h2&gt;
&lt;p&gt;pipelines are handy:
they make your code much shorter
data leakage&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unsupervised Learning&lt;/h2&gt;
&lt;div id=&#34;pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-Processing&lt;/h2&gt;
&lt;p&gt;inverse transform&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;p&gt;My preferable way&lt;/p&gt;
&lt;p&gt;Automatic machine learning&lt;/p&gt;
&lt;p&gt;parellal processing&lt;/p&gt;
&lt;p&gt;Things that are unique to Scikit-learn&lt;/p&gt;
&lt;p&gt;Things that are unique to Tidymodels&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://juliasilge.com/blog/best-hip-hop/&#34; class=&#34;uri&#34;&gt;https://juliasilge.com/blog/best-hip-hop/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Football Data</title>
      <link>/post/2020-02-12-analyzing-football-data/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-12-analyzing-football-data/</guid>
      <description>


&lt;p&gt;names(html_table)&amp;lt;- col_names
names(table_wiki)
View(as_tibble(table_wiki,.name_repair = “minimal”)[1,1])&lt;/p&gt;
&lt;p&gt;t &amp;lt;- as_tibble(table_wiki,.name_repair = “minimal”)[1,1]
as.vector(t)
table_wiki[1:10]&lt;/p&gt;
&lt;p&gt;col_names &amp;lt;- url %&amp;gt;%
read_html() %&amp;gt;%
html_nodes(xpath = paste(’//*&lt;span class=&#34;citation&#34;&gt;[@id=&#34;stats_standard_ks_3260&#34;]&lt;/span&gt;’)) %&amp;gt;%
html_table(header = NA,fill = TRUE) %&amp;gt;%
as.data.frame() %&amp;gt;%
slice(1)&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Makes a Song Popular? An Explainable Machine Learning Approach</title>
      <link>/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_chunk$set(warning = FALSE, message = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tune)
library(rsample)
library(yardstick)
library(dials)
library(workflows)
library(parsnip)
library(infer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#devtools::install_github(&amp;quot;tidymodels/tidymodels&amp;quot;)
#remotes::install_github(&amp;quot;wilkelab/ggtext&amp;quot;,build = &amp;#39;binary&amp;#39;)
library(tidyverse)
library(tidymodels)
library(lubridate)
library(corrr)
library(pins)
library(genius)
library(reticulate)

spotify_songs &amp;lt;- pin(read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&amp;#39;))
head(spotify_songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 23
##   track_id track_name track_artist track_popularity track_album_id
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         
## 1 6f807x0~ I Don&amp;#39;t C~ Ed Sheeran                 66 2oCs0DGTsRO98~
## 2 0r7CVbZ~ Memories ~ Maroon 5                   67 63rPSO264uRjW~
## 3 1z1Hg7V~ All the T~ Zara Larsson               70 1HoSmj2eLcsrR~
## 4 75Fpbth~ Call You ~ The Chainsm~               60 1nqYsOef1yKKu~
## 5 1e8PAfc~ Someone Y~ Lewis Capal~               69 7m7vv9wlQ4i0L~
## 6 7fvUMiy~ Beautiful~ Ed Sheeran                 67 2yiy9cd2QktrN~
## # ... with 18 more variables: track_album_name &amp;lt;chr&amp;gt;,
## #   track_album_release_date &amp;lt;chr&amp;gt;, playlist_name &amp;lt;chr&amp;gt;, playlist_id &amp;lt;chr&amp;gt;,
## #   playlist_genre &amp;lt;chr&amp;gt;, playlist_subgenre &amp;lt;chr&amp;gt;, danceability &amp;lt;dbl&amp;gt;,
## #   energy &amp;lt;dbl&amp;gt;, key &amp;lt;dbl&amp;gt;, loudness &amp;lt;dbl&amp;gt;, mode &amp;lt;dbl&amp;gt;, speechiness &amp;lt;dbl&amp;gt;,
## #   acousticness &amp;lt;dbl&amp;gt;, instrumentalness &amp;lt;dbl&amp;gt;, liveness &amp;lt;dbl&amp;gt;, valence &amp;lt;dbl&amp;gt;,
## #   tempo &amp;lt;dbl&amp;gt;, duration_ms &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- spotify_songs %&amp;gt;%  
  dplyr::rowwise() %&amp;gt;% 
  mutate(shorter_names = unlist(str_split(track_name,&amp;#39;-&amp;#39;))[1]) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- py$spotify_songs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have you ever wondered why some songs from an artist become so popular and others are just total failure?
Look at the next plot. Even The Beetles had a few not so popular songs.&lt;/p&gt;
&lt;p&gt;So what can be the recipe for popularity? Or why does a song become(un)popular? Is it solely releated to the artists that play a song? Can it be the song’s audio features?&lt;/p&gt;
&lt;p&gt;##Exploratory Data Analysis&lt;/p&gt;
&lt;div id=&#34;machine-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;Another more complex way to look at this problem is to use machine learning algorithms. We can train a machine learning model to accurately predict the popularity of a song. Now if we look inside the patterns that this model learning model has learned, we might be able to find out why a song has become popular or unpopular.
In the second part of this post, I will demonstrate how I designed a machine learning workflow to predict the popularity of songs based on several audio features. Here my goal from using an ML model is not just to predict popularity but rather to figure out which factors contribute to it.&lt;/p&gt;
&lt;p&gt;However, peeking inside an ML algorithm and discovering how it makes prediction is not alwayse straighforward. Only the inner-workings of a few ML algorithms such as decision trees and linear modelsare transparent. These algorithms are very simple and might be powerful enough to model the complexities and the common knowledge is that they are not accurate. Of course you can make a decision tree fairly accurate by increasing its depth but the resulting tree would become exteremly messy and hard to understand.In addition, deeper tree are more likely to overfit.&lt;/p&gt;
&lt;p&gt;There are also more powerful and more accurate algorithms such as random forests, xgboost or deep neural networks but understanding how they make predictions is very challenging (sometimes they are called black-box models). That is translated to a widespread belief amont ML community that there is a trade-off between the accuracy and the interpretability of an ML algorithm. However, a few other researchers reject this claim and believe it is just a popular myth and you can indeed find an interpretable and accurate ML algorithm.&lt;/p&gt;
&lt;p&gt;Anyways, over the past five years a lot of methods have been proposed to some “approximate” how an ML algorithm predicts an outcome. Two popular methods that are widely used to interpret machine learning algorithms are LIME and SHAP.&lt;/p&gt;
&lt;p&gt;We can look inside a machine learning algorithms from two aspects:&lt;/p&gt;
&lt;p&gt;Based on what feature values, an ML algorithm has made prediction about the popularity of “a particular” song. Local explanations
Global explanations such as feature importance scores to understand to the popularity of songs.&lt;/p&gt;
&lt;p&gt;In this part of my post, first I will train a random forest and an XGBoost model to predict song popularity and then I will discover how they make prediction using SHAP, LIME and feature importance scores. Hopefully, these patterns can help us better understand which factors might contribute to a song’s popularity.&lt;/p&gt;
&lt;p&gt;To use a explainable machine learning methods for this purpose, it is important to obtain a reasonable performance on the prediction task. Otherwise, the result would be unreliable and useless.&lt;/p&gt;
&lt;p&gt;I won’t use common pre-processing steps such as normalization because random forest and XGBoost are not sensitive to non-normalized data. Also, some pre-processing steps might make the interpretation of the results less intuitive.&lt;/p&gt;
&lt;p&gt;I have mainly used scikit-learn for training ML models but recently I have become passionately interested in the Tidymodels ecosystem. So, here I have decided to use Tidymodels and its features for model development.
Workflows are similar to &lt;code&gt;pipelines&lt;/code&gt; in scikit-learn.
My designed workflow consists of the following step:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;I create a preprocessing recipe using the &lt;code&gt;recipe&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I split the input dataset into a training and testing set&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I build a random forest and an XGBoost model using the &lt;code&gt;parsnip&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I use &lt;code&gt;tune&lt;/code&gt; package and tuning the hyper-paramters of the random forest and the XGBoost model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I use&lt;code&gt;rsample&lt;/code&gt; package to perform cross-validation and train both models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I evaluate the performance of the trained models based on metrics from the &lt;code&gt;yardstick&lt;/code&gt; package and select the best model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://tidymodels.github.io/yardstick/reference/index.html&#34; class=&#34;uri&#34;&gt;https://tidymodels.github.io/yardstick/reference/index.html&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Finally, I explain the predictions of the machine learning model in hope of finding interesting patterns that might tell us something about why a song becomes popular. Not that this step is not implemented as a part of the Tidymodel workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;machine-learning-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pre-processing&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;bulding-the-ml-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bulding the ML models&lt;/h3&gt;
&lt;p&gt;first we need to specify the type of the model that we want to train and if necessary its hyper-paramters. Then we have to determine the mode of the ML task that we would like to solve. Our problem is a regression problem, so we set the mode as &lt;code&gt;regression&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;set the engine or the implementation of the model (Ranger)&lt;/p&gt;
&lt;p&gt;4.set the mode of the ML task (Regression)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs %&amp;gt;% 
 select(track_popularity, c(12:23)) %&amp;gt;% 
 correlate() %&amp;gt;% 
  network_plot(min_cor = 0.1,color = c(&amp;#39;#1a535c&amp;#39;,&amp;#39;#4ecdc4&amp;#39;,&amp;#39;#f7fff7&amp;#39;,&amp;#39;#ff6b6b&amp;#39;,&amp;#39;#ffe66d&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs %&amp;gt;% 
 ggplot(aes(track_popularity)) +
 geom_histogram(fill = &amp;#39;indianred&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-what-makes-a-song-popular-an-explainable-machine-learning-approach/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shap&lt;/h2&gt;
&lt;div id=&#34;machine-learning-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#rf &amp;lt;- rand_forest(trees = 100, mode = &amp;#39;regression&amp;#39;) %&amp;gt;% 
 #set_engine(&amp;quot;randomForest&amp;quot;) %&amp;gt;% 
 #fit(Species ~. ,data = iris_training)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references-and-further-readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References and further readings&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaylinpavlik.com/classifying-songs-genres/&#34; class=&#34;uri&#34;&gt;https://www.kaylinpavlik.com/classifying-songs-genres/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://konradsemsch.netlify.com/2019/10/testing-the-tune-package-from-tidymodels-analysing-the-relationship-between-the-upsampling-ratio-and-model-performance/&#34; class=&#34;uri&#34;&gt;https://konradsemsch.netlify.com/2019/10/testing-the-tune-package-from-tidymodels-analysing-the-relationship-between-the-upsampling-ratio-and-model-performance/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Going Back to the Roots! How Much Influence Did Arabic Have on Persian Literature?</title>
      <link>/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/</guid>
      <description>


&lt;p&gt;Since the &lt;a href=&#34;https://en.wikipedia.org/wiki/Muslim_conquest_of_Persia&#34;&gt;conquest of Persia (now Iran) by the Muslim forces in the 7th century&lt;/a&gt;, Arabic culture and language have had a huge influence on Iran and Iranians. Although Iran had never fully adapted Arabic as its main language, but the new Persian (Farsi) language is a mix of Arabic and the old Persian (Pahlavi) and almost use the same alphabet for writing. Also, in some parts of Iran, Arabic is the daily-life language.
Over the past 100 years, a very few (narrowly-minded and mostly racist) scholars have tried to erase Arabic words from the Persian literature. Since I was a kid I have always wanted to I put my data science skills and tools to&lt;/p&gt;
&lt;p&gt;I decided to start a small project and determine how much influence Arabic has had on the Persian Lieterature and poetry over time. To put it simply, my goal is to look at every word used in poems ad determind wether it comes from Arabic or it is originally a Persian word. Then I count the occurance of each of them and compute their ratio.&lt;/p&gt;
&lt;p&gt;However, this is not an easy task for several reasons. Although determing the origin of a word is not difficult for a well-educated person, there are millions of words in Persian literary works.So, labaling each word manually is not feasible and I tried smarter ways (but less accurate ). Similar to many other languages, Persian poems are different from daily written or spoken Persian and therefore common NLP methods are not as effective as before.&lt;/p&gt;
&lt;p&gt;Ideally, we need a complete dataset of word with Araabic roots that are used in Persian to solve this task. But this dataset does not exist and I must use other approaches:
1. There are a number of rules and exceptions that can be used to distinguish Persian words from Arabic words. For example, unlike Persian, Arabic does not have four letters representing “p”, “j” such as Japan, “g” such as game and “ch” in its alphabet. It means that any word that consists of one of these letters it is definitely a non-Arabic word. On the other hand, we do not have any letter in the Persian alphabet for representing the ‘th’ letter (and a few other letters) in Arabic. Therefore, words that consist of these letters are likely to be Arabic words.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# fa --&amp;gt; Farsi (Persian)
# ar ---&amp;gt; Arabic
# un ----&amp;gt; Unkown
def arabic_word(word):
    if &amp;#39;ث&amp;#39; in word:
        return &amp;#39;ar&amp;#39;
    elif &amp;#39;ح&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;ص&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    if &amp;#39;ض&amp;#39; in word:
        return &amp;#39;ar&amp;#39;
    elif &amp;#39;ظ&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;ع&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;ط&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;ق&amp;#39; in word:
        return &amp;#39;ar&amp;#39; 
    elif &amp;#39;ژ&amp;#39; in word:
        return &amp;#39;fa&amp;#39;
    elif &amp;#39;گ&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    elif &amp;#39;چ&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    elif &amp;#39;پ&amp;#39; in word:
        return &amp;#39;fa&amp;#39; 
    else:
        return &amp;#39;un&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Unfortunately, the rules mentioned above are not comprehensive and the origin of many words cannot be determined by them. So,I turned to the python port of the popular &lt;a href=&#34;https://github.com/Mimino666/langdetect&#34;&gt;&lt;code&gt;Langdetect&lt;/code&gt;&lt;/a&gt; library for help. Here, if the origin of a word can not be determined by the above rules, I will ask this library to identify the language. I should mention that langdetect can be sometimes wrong so the final results might not be 100% accurate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I must also mention that I performed a few preprocessing steps such as removing stopwords on the poetry corpus. A few other operations such as stemming could have been performed but my initial assessment was that they might not significantly change the final results.
After preprocessing, I stored all the information about the ratio of Arabic and Persian words for each poet in a separate dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lang_ratio_df &amp;lt;- read_csv(&amp;#39;lang_ratio_df.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   poet = col_character(),
##   century = col_double(),
##   ar = col_double(),
##   fa = col_double(),
##   ratio = col_double(),
##   period = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(lang_ratio_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   poet               century    ar    fa ratio period         
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          
## 1 Abusaeid Abolkheir       5  3014  8277 0.364 Khorasani Style
## 2 Ahmad Shamlou           14  8232 28862 0.285 Contemporary   
## 3 Akhavan-Sales           14  3338 14937 0.223 Contemporary   
## 4 Amir Khusrow             8 10582 41997 0.252 Iraqi Style    
## 5 Anvari                   6 29430 67188 0.438 Iraqi Style    
## 6 Artimani                10  2616  7706 0.339 Indian Style&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I visualized the ratio of words for each poet using the &lt;code&gt;ggplot&lt;/code&gt; library in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; lang_ratio_df %&amp;gt;%
    mutate(
    poet = fct_reorder(poet, ratio),

    period = factor(
      period,
      levels = c(&amp;#39;Khorasani Style&amp;#39;,&amp;#39;Iraqi Style&amp;#39;,&amp;#39;Indian Style&amp;#39;,&amp;#39;Contemporary&amp;#39; )
    )) %&amp;gt;% 
  ggplot(aes(x = poet, y = ratio , color = period)) +
  geom_point(size = 4) +
  geom_segment(aes(
    y = 0, yend = ratio, x = poet, xend = poet), size = 1) +
  geom_text(
    aes(x = poet,  y = ratio,label = scales::percent(ratio)), size = 5, nudge_y = .2,family = &amp;#39;Montserrat&amp;#39;) +
  labs( x = &amp;#39;&amp;#39;, y = &amp;#39;&amp;#39;, title = &amp;#39;The Estimated Ratio of Arabic Words Used by Famous Persion Poets&amp;#39;) +
  scale_color_tableau() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  coord_flip() +
  facet_wrap( ~ period, scales = &amp;quot;free_y&amp;quot;, ncol = 2) +
  theme_tufte() +
  theme(
    text = element_text(family = &amp;#39;Montserrat&amp;#39;),
    legend.title =  element_text(size = 20),
    axis.ticks.x = element_blank(),
    legend.text = element_text(
      size = 15,
    margin = ggplot2::margin(0, 20, 0, 0)),
    plot.title = element_text(
      face = &amp;quot;bold&amp;quot;,
      color = &amp;#39;gray&amp;#39;,
      size = 22,
      margin = ggplot2::margin(0, 20, 20, 0),
      hjust = 0.5,
      vjust = 0.5),
        strip.text = element_text(
      color = &amp;#39;gray80&amp;#39;,
      size = 18 ,
      margin = ggplot2::margin(1, 0, 1, 0)),
    legend.position = &amp;#39;none&amp;#39;,
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(size = 12, color = &amp;#39;gray&amp;#39;),
    plot.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
    panel.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
    panel.border = element_rect(fill = NA, color = NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see above, every poet used at least a sizable number of Arabic words in his/her work. Most notably, Ferdowsi who wrote &lt;a href=&#34;https://en.wikipedia.org/wiki/Shahnameh&#34;&gt;Shahname (the Book of Kings)&lt;/a&gt;, which recount the myths and legends of Persian Kings and Heroes and is the oldest piece of poetry analyzed in my experiment also includes a considerable number of Arabic words. Other top Persian poets such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Hafez&#34;&gt;Hafez&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Saadi_Shirazi&#34;&gt;Saadi&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Rumis&#34;&gt;Rumi&lt;/a&gt; used Arabic words in almost 40%-50% of their works.&lt;/p&gt;
&lt;p&gt;Nothing better can show this than the following plot which made by the &lt;a href=&#34;https://emilhvitfeldt.github.io/ggpage/&#34;&gt;&lt;code&gt;ggpage&lt;/code&gt;&lt;/a&gt; package in R. The plot shows the distribution of words and their origins for several top Persian poets. Note that in this plot I used a only random subset of words from the works of each poet and not their whole works of poetry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_poets_df &amp;lt;- read_csv(&amp;#39;sample_poets.csv&amp;#39;)
head(sample_poets_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   word  lang  poet   century
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 &amp;lt;U+0628&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0645&amp;gt;   fa    Rudaki       3
## 2 &amp;lt;U+0627&amp;gt;&amp;lt;U+0634&amp;gt;&amp;lt;U+06A9&amp;gt;   fa    Rudaki       3
## 3 &amp;lt;U+063A&amp;gt;&amp;lt;U+0645&amp;gt;    ar    Rudaki       3
## 4 &amp;lt;U+0647&amp;gt;&amp;lt;U+0645&amp;gt;&amp;lt;U+06CC&amp;gt;   fa    Rudaki       3
## 5 &amp;lt;U+0628&amp;gt;&amp;lt;U+0631&amp;gt;&amp;lt;U+0645&amp;gt;   fa    Rudaki       3
## 6 &amp;lt;U+0646&amp;gt;&amp;lt;U+0647&amp;gt;&amp;lt;U+0627&amp;gt;&amp;lt;U+0646&amp;gt;&amp;lt;U+06CC&amp;gt; fa    Rudaki       3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpage_df %&amp;gt;%
  mutate(poet = fct_reorder(poet, century)) %&amp;gt;%
  ggpage_plot(aes(fill = lang)) +
  
  labs(title = &amp;#39;Distribution of Persian and Arabic Words Used by Top Persian Poets&amp;#39;, fill = &amp;#39;&amp;#39;) +
  scale_fill_manual(values = plotcolors,
                    guide = &amp;#39;legend&amp;#39; ,
                    labels = c(&amp;#39;Arabic&amp;#39;,&amp;#39;Persian&amp;#39;)) +
  
  facet_wrap(~ poet, nrow = 3) +
  theme(
    strip.text = element_text(
      size = 15,
      face = &amp;quot;bold&amp;quot;,
      margin = ggplot2::margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;),
      color = &amp;#39;white&amp;#39;
    ),
        text = element_text(family = &amp;#39;Montserrat&amp;#39;),

    legend.position = &amp;#39;top&amp;#39;,
    legend.text = element_text(
      size = 15,
      margin = ggplot2::margin(10, 10, 10, 10)
    ),
    panel.spacing = unit(1, &amp;quot;points&amp;quot;),
    plot.title = element_text(
      face = &amp;quot;bold&amp;quot;,
      size = 22,
      margin = ggplot2::margin(30, 0, 30, 0),
      hjust = 0.5
    ),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = &amp;#39;#000F2B&amp;#39;),
    panel.border = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-08-going-back-to-the-roots-how-much-influence-did-arabic-have-on-persian-literature/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;How Much Influence Did Arabic Have on Persian Literature has been one of my questions since I started to read and study literature. Nobody had been able to answer this question and I could not have answered it without the help of data science.&lt;/p&gt;
&lt;p&gt;My analysis shows that the Arabic language has contributed significantly to our literature and culture. In fact, the golden era of Persian poetry can be seen as a result of its integration with Arabic. Persian also made its contribution to the Arabic language and Arabic poetry. So, talking about erasing one language from the other is not helpful or wise and I hope everyone realizes that.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Map of Spotify Songs</title>
      <link>/post/2020-02-01-what-makes-a-song-popular/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-01-what-makes-a-song-popular/</guid>
      <description>


&lt;p&gt;In the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md&#34;&gt;4th week of the Tidy Tuesday project&lt;/a&gt;, a very interesting and fun dataset was proposed to the data science community. The dataset contains information about thousands of songs on Spotify’s platform and along with their metadata and audio features. You can download the dataset can using the following piece of code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md&#34;&gt;4th week of the Tidy Tuesday project&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&amp;#39;)
head(spotify_songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 23
##   track_id track_name track_artist track_popularity track_album_id
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         
## 1 6f807x0~ I Don&amp;#39;t C~ Ed Sheeran                 66 2oCs0DGTsRO98~
## 2 0r7CVbZ~ Memories ~ Maroon 5                   67 63rPSO264uRjW~
## 3 1z1Hg7V~ All the T~ Zara Larsson               70 1HoSmj2eLcsrR~
## 4 75Fpbth~ Call You ~ The Chainsm~               60 1nqYsOef1yKKu~
## 5 1e8PAfc~ Someone Y~ Lewis Capal~               69 7m7vv9wlQ4i0L~
## 6 7fvUMiy~ Beautiful~ Ed Sheeran                 67 2yiy9cd2QktrN~
## # ... with 18 more variables: track_album_name &amp;lt;chr&amp;gt;,
## #   track_album_release_date &amp;lt;chr&amp;gt;, playlist_name &amp;lt;chr&amp;gt;, playlist_id &amp;lt;chr&amp;gt;,
## #   playlist_genre &amp;lt;chr&amp;gt;, playlist_subgenre &amp;lt;chr&amp;gt;, danceability &amp;lt;dbl&amp;gt;,
## #   energy &amp;lt;dbl&amp;gt;, key &amp;lt;dbl&amp;gt;, loudness &amp;lt;dbl&amp;gt;, mode &amp;lt;dbl&amp;gt;, speechiness &amp;lt;dbl&amp;gt;,
## #   acousticness &amp;lt;dbl&amp;gt;, instrumentalness &amp;lt;dbl&amp;gt;, liveness &amp;lt;dbl&amp;gt;, valence &amp;lt;dbl&amp;gt;,
## #   tempo &amp;lt;dbl&amp;gt;, duration_ms &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this week’s tidy Tuesday, I decided to use a rather different approach from my previous submission. Instead of focusing entirely on the visualization aspect of my submission, I tried to use other tools from tidy model universe for machine learning model development.&lt;/p&gt;
&lt;p&gt;Each song has around 12 columns representing audio features. The Github’s page for this dataset describes these features as follows:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;73%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;variable&lt;/th&gt;
&lt;th&gt;class&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;danceability&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;energy&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;key&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;loudness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;mode&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;speechiness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;acousticness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;instrumentalness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;liveness&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;valence&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;tempo&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;duration_ms&lt;/td&gt;
&lt;td&gt;double&lt;/td&gt;
&lt;td&gt;Duration of song in milliseconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It would be very helpful to compare songs based on the combination of their audio features and have an overall picture of where each song is placed. Unfortunately, we can only visualize 2 or 3 audio features at the same time and It is not possible to put all these features on a 2D or 3D space. So, I tried to use unsupervised machine learning for the purpose of visualizing songs on a 2D space by transforming their high-dimensional audio features into a more compressed form.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(workflows)
library(gghighlight)
library(hrbrthemes)
library(ggthemes)
library(lubridate)
library(reticulate)
library(ggrepel)
library(plotly)
library(uwot)


theme_update(legend.position = &amp;#39;top&amp;#39;,
   legend.text  = element_text(size = 32,color = &amp;#39;gray75&amp;#39; ),
   legend.key = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   legend.background= element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   plot.title = element_text(family = &amp;#39;Montserrat&amp;#39;, face = &amp;quot;bold&amp;quot;, size = 60,hjust = 0.5,vjust = 0.5,color = &amp;#39;#FFE66D&amp;#39;,margin = ggplot2::margin(40,0,0,0)),
   plot.subtitle = element_text(
   family = &amp;#39;Montserrat&amp;#39;, size = 30, hjust = 0.5),
   strip.background = element_blank(),
   plot.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   panel.background = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
   panel.grid.major.x =element_blank(),
   panel.grid.major.y =element_blank(),
   panel.grid.minor =element_blank(),
   axis.text.x.bottom = element_blank(),
   axis.ticks.x = element_blank(), 
   axis.ticks.y = element_blank(),
   axis.text.x = element_blank(),
   axis.text.y.left = element_blank()) &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;dimensionality-reduction-and-umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dimensionality Reduction and UMAP&lt;/h2&gt;
&lt;p&gt;My initial idea was to use some clustering algorithms to cluster songs based on their audio feature and find songs that are similar to each other. Yet, it was difficult to visualize these clusters in a two dimensional space. Of course you can do that by using hierarchal clustering but even then visualizing a few thousands samples (songs) seems to be impractical. So, I decided to to use other unsupervised techniques to compress these high-dimensional audio features and transform them into a more compact 2D space.&lt;/p&gt;
&lt;p&gt;There are a number of dimensionality reduction algorithms such as PCA, t-SNE UMAP. The main purpose of these algorithm is to give us a compressed representation of the input data, which is obtained with the least possible information loss. PCA is a linear dimensionality reduction method while both t-SNE and UMAP are non-linear methods.&lt;/p&gt;
&lt;p&gt;In this post, I will use UMAP and t-SNE, two widely used dimensionality reduction algorithms. When the input dataset is large T-SNE becomes very slow and is not an efficient algorithm anymore. On the other hand, UMAP can handle larger datasets much more easily. Moreover, not only UMAP can preserve the underlying local structure present in the data, but it can also represent the global structure of the data more accurately. What do we mean by local and global structure? For example, in the song dataset, persevering local structure means that songs that belong to an artist are clustered together. Similarly, global structure means that songs belonging to more related genres (e.g. hard rock, album rock and classic rock) will be placed in a close proximity of each other on the new projection&lt;/p&gt;
&lt;p&gt;UMAP achieves this goal by employing some advanced optimization techniques and mathematical concept. Understanding how UMAP uses these techniques and projects the input data into a more compressed representation is not crucial, but If you are curios to know more about the theory behind UMAP and its difference with T-SNE, I recommend &lt;a href=&#34;https://pair-code.github.io/understanding-umap/&#34;&gt;this wonderful blogpost&lt;/a&gt; by Andy Coenen and Adam Pearce.&lt;/p&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Preprocessing&lt;/h3&gt;
&lt;p&gt;Both UMAP and T-SNE compute a distance metric between samples. This distance metric should be meaningful and reasonable. If we do not scale the input features before ru some features might have higher (unfair) influence than other features on the computation of distance between samples. For this reason, it is necessary to normalize input features before implementing a them,&lt;/p&gt;
&lt;p&gt;I create a data preprocessing recipe using the &lt;code&gt;recipe&lt;/code&gt; package and I add a normalization step to scale the audio features. Note that since I implement an unsupervised algorithm, there is no need to split the dataset into a training and testing dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normalized_features &amp;lt;- spotify_songs %&amp;gt;%
 recipe() %&amp;gt;% 
 step_normalize( danceability,
  energy,
  key,
  loudness,
  mode,
  speechiness,
  acousticness,
  instrumentalness,
  liveness,
  valence,
  tempo,
  duration_ms) %&amp;gt;% 
 prep() %&amp;gt;% 
 juice()

head(normalized_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 23
##   track_id track_name track_artist track_popularity track_album_id
##   &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;         
## 1 6f807x0~ I Don&amp;#39;t C~ Ed Sheeran                 66 2oCs0DGTsRO98~
## 2 0r7CVbZ~ Memories ~ Maroon 5                   67 63rPSO264uRjW~
## 3 1z1Hg7V~ All the T~ Zara Larsson               70 1HoSmj2eLcsrR~
## 4 75Fpbth~ Call You ~ The Chainsm~               60 1nqYsOef1yKKu~
## 5 1e8PAfc~ Someone Y~ Lewis Capal~               69 7m7vv9wlQ4i0L~
## 6 7fvUMiy~ Beautiful~ Ed Sheeran                 67 2yiy9cd2QktrN~
## # ... with 18 more variables: track_album_name &amp;lt;fct&amp;gt;,
## #   track_album_release_date &amp;lt;fct&amp;gt;, playlist_name &amp;lt;fct&amp;gt;, playlist_id &amp;lt;fct&amp;gt;,
## #   playlist_genre &amp;lt;fct&amp;gt;, playlist_subgenre &amp;lt;fct&amp;gt;, danceability &amp;lt;dbl&amp;gt;,
## #   energy &amp;lt;dbl&amp;gt;, key &amp;lt;dbl&amp;gt;, loudness &amp;lt;dbl&amp;gt;, mode &amp;lt;dbl&amp;gt;, speechiness &amp;lt;dbl&amp;gt;,
## #   acousticness &amp;lt;dbl&amp;gt;, instrumentalness &amp;lt;dbl&amp;gt;, liveness &amp;lt;dbl&amp;gt;, valence &amp;lt;dbl&amp;gt;,
## #   tempo &amp;lt;dbl&amp;gt;, duration_ms &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;t-sne&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;T-SNE&lt;/h2&gt;
&lt;p&gt;Both UMAP and T-SNE have several hyper-parameters that can influence the resulting embedding output. However, T-SNE is a notoriously slow algorithm and the opportunity for trial and error with different sets of hyper-parameter values are limited. For the sake of simplicity, I stick to default settings for hyper-parameter in T-SNE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
tsne_embedding &amp;lt;- normalized_features %&amp;gt;%
 select(c(12:23)) %&amp;gt;%
 Rtsne(check_duplicates = FALSE)

tsne_embeddings &amp;lt;- spotify_songs %&amp;gt;% 
 select(-c(12:22)) %&amp;gt;% 
 bind_cols(tsne_embedding$Y %&amp;gt;% as_tibble()) %&amp;gt;% = element_rect(fill = &amp;quot;black&amp;quot;, color = &amp;quot;black&amp;quot;),
 dplyr::rename(tsne_1 = V1, tsne_2 = V2) %&amp;gt;% &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though I managed to transform a high dimensional dataset into a 2D space, still it was very challenging to visualize every song and every artists all at once. So, I just select a few famous artists that I have heard about. Each artist in this list more or less represents at least a genre of music and it can perfectly show that an artist (or a band) made several genres of music and how difficult our task is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_artists &amp;lt;- c(&amp;#39;Queen&amp;#39;,&amp;#39;Drake&amp;#39;,&amp;#39;Rihanna&amp;#39;,&amp;#39;Taylor Swift&amp;#39;,&amp;#39;Eminem&amp;#39;,&amp;#39;Snoop Dogg&amp;#39;,&amp;#39;Katy Perry&amp;#39;,&amp;#39;The Beatles&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings &amp;lt;- tsne_embeddings%&amp;gt;% 
 mutate(
  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
  popular_tracks_selected_artist = if_else(
   track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 65,shorter_names, NULL )) %&amp;gt;%
 distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings %&amp;gt;%
 ggplot(aes(x = tsne_1, y = tsne_2 ,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    title = &amp;#39;The Map of Spotify Songs Based on T-SNE Algorithm\n&amp;#39;,
    subtitle = &amp;#39;Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n&amp;#39;,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see in this projection, songs that belong to the same artists are placed close to each other. It seems that T-SNE is able to preserve the local topological structure of songs. Now I will look at how T-SNE distinguishes different genres of music.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne_embeddings %&amp;gt;%
 ggplot(aes(x = tsne_1, y = tsne_2 ,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    title = &amp;#39;The Map of Spotify Songs Based on T-SNE Algorithm\n&amp;#39;,
    subtitle = &amp;#39;Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\n Each point represents a unique song and the most popular songs of several known artist are also shown\n&amp;#39;,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMAP&lt;/h2&gt;
&lt;p&gt;Just like t-SNE, UMAP is a dimensionality reduction algorithm but it is much more computationally efficient and faster that t-SNE. The UMAP algorithm was &lt;a href=&#34;https://github.com/lmcinnes/umap&#34;&gt;originally implemented in Python&lt;/a&gt;. But there are also several libraries in R such as &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;umapr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/umap/vignettes/umap.html&#34;&gt;umap&lt;/a&gt; and &lt;a href=&#34;https://github.com/jlmelville/uwot&#34;&gt;uwot&lt;/a&gt; that also provide an implementation of the UMAP algorithm. &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;&lt;code&gt;umapr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/umap/vignettes/umap.html&#34;&gt;&lt;code&gt;umap&lt;/code&gt;&lt;/a&gt; use the &lt;a href=&#34;https://cran.r-project.org/web/packages/reticulate/index.html&#34;&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; package and provide a wrapper function around the original &lt;code&gt;umap-learn&lt;/code&gt; python library. Also, &lt;code&gt;umap&lt;/code&gt; and &lt;code&gt;uwot&lt;/code&gt; library have their own R implementation and they do not require the python package to be installed beforehand. For this specific experiment, I will use the &lt;code&gt;uwot&lt;/code&gt; library.&lt;/p&gt;
&lt;p&gt;we can change and tune a few hyper-parameters in the implementation of UMAP in the uwot library, These hyperparameter can change the embedding outcome. However, there are two hyper-parameters that have a much more important impact on the structure of the low-dimensional representation:&lt;code&gt;n_neighbors&lt;/code&gt;, &lt;code&gt;min_dist&lt;/code&gt; and &lt;code&gt;metric&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_neighbors&lt;/code&gt; determines the number of nearest neighbor data points that we use to compute and construct the embedding.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_dist&lt;/code&gt; controls the minimum distance between data points in the low dimensional space (embedding). That means a low value of &lt;code&gt;min_dist&lt;/code&gt; results in a more compact clusters of data points. On the other hand, with larger values of &lt;code&gt;min_dist&lt;/code&gt;, the projection will be less compact and tend to preserve the global structure.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metric&lt;/code&gt;: We can use different metrics (e.g.. cosine or Euclidean) to compute the distance between data points and to find the nearest neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The choice of hyperparameter values can be very important for the final projection. However,choosing the right set of hyper-parameters in UMAP is extremely difficult because UMAP is an unsupervised algorithm and we do not have a baseline to evaluate its performance. Fortunately, UMAP is vary fast and scalable algorithm. It means that we can run UMAP with different hyperparameter settings and decide which set of values best serves our purpose.&lt;/p&gt;
&lt;p&gt;My main goal from running UMAP is to visualize songs and their audio features on a 2D space and I can use a trick to decrease UMAP’s computation time. According to uwot’s documentation, if my only purpose is visualization, I can set the value of &lt;code&gt;fast_sgd&lt;/code&gt; hyper-parameter to &lt;code&gt;TRUE&lt;/code&gt; to speed up UMAP’s convergence and running time.
Next, I create a grid of values for these three hyper-parameters and each time I will learn a new UMAP embedding based on different combinations of these values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_neighbors &amp;lt;- c(15,30,50,100,150)
min_distance &amp;lt;- c( 0.001, 0.003, 0.009,0.03,0.09)
metrics &amp;lt;- c(&amp;quot;euclidean&amp;quot; ,&amp;quot;cosine&amp;quot;,&amp;quot;hamming&amp;quot;)

#make a copy of the dataset
spotify_songs_emb &amp;lt;- spotify_songs

for (nn in n_neighbors) {
 for (md in min_distance) {
  for (metric in metrics) {
  umap_embedding &amp;lt;- normalized_features %&amp;gt;%
  select(c(12:23)) %&amp;gt;%
  umap(n_neighbors = nn,min_dist = md,metric = metric, fast_sgd = TRUE)
  spotify_songs_emb &amp;lt;- spotify_songs_emb %&amp;gt;% 
  bind_cols(umap_embedding[,1]%&amp;gt;% as_tibble() ) %&amp;gt;% 
  bind_cols(umap_embedding[,2] %&amp;gt;% as_tibble() )
  names(spotify_songs_emb)[names(spotify_songs_emb) == &amp;#39;value&amp;#39; ] = paste(&amp;#39;nn_&amp;#39;,nn,&amp;#39;md_&amp;#39;,md,&amp;#39;metric&amp;#39;,metric,&amp;#39;1&amp;#39;,sep = &amp;#39;.&amp;#39;)
  names(spotify_songs_emb)[names(spotify_songs_emb) == &amp;#39;value1&amp;#39; ] = paste(&amp;#39;nn_&amp;#39;,nn,&amp;#39;md_&amp;#39;,md,&amp;#39;metric&amp;#39;,metric,&amp;#39;2&amp;#39;,sep = &amp;#39;.&amp;#39;)
  }
 }
 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like what I did for T-SNE, I will focus on the same list of artists.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb &amp;lt;- spotify_songs_emb%&amp;gt;% 
 mutate(
  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
  point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
  popular_tracks_selected_artist = if_else(
   track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 65,shorter_names, NULL )) %&amp;gt;%
 distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, it was time to plot the results of UMAP embeddings using &lt;code&gt;ggplot&lt;/code&gt; and &lt;a href=&#34;https://github.com/yutannihilation/gghighlight&#34;&gt;&lt;code&gt;gghighlight&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setting-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 50&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.50.md_.0.09.metric.euclidean.1, y = nn_.50.md_.0.09.metric.euclidean.2 ,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 50&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Hamming&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.50.md_.0.09.metric.hamming.1, y = nn_.50.md_.0.09.metric.hamming.2,color = selected_artist )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_manual(values = c(&amp;#39;#5BC0EB&amp;#39;,&amp;#39;#FDE74C&amp;#39;,&amp;#39;#7FB800&amp;#39;,&amp;#39;#E55934&amp;#39;,&amp;#39;#FA7921&amp;#39;,&amp;#39;#1A936F&amp;#39; ,&amp;#39;#F0A6CA&amp;#39;,&amp;#39;#B8BEDD&amp;#39;))+
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 150&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.150.md_.0.09.metric.euclidean.1, y = nn_.150.md_.0.09.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 15&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.15.md_.0.09.metric.euclidean.1, y = nn_.15.md_.0.09.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 5&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 150&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.001&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Euclidean&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.150.md_.0.001.metric.euclidean.1, y = nn_.150.md_.0.001.metric.euclidean.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-6&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting 6&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nearest neighbors: 15&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum distance: 0.09&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance metric: Hamming&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spotify_songs_emb %&amp;gt;%
 ggplot(aes(x = nn_.15.md_.0.09.metric.hamming.1, y = nn_.15.md_.0.09.metric.hamming.2,color = playlist_genre )) +
 geom_point(size = 5.3,alpha =0.8) +
 gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size=0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
 scale_color_tableau() +
 guides(size = FALSE,
  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = &amp;#39;Montserrat&amp;#39;,
  point.padding = 2.2,
  box.padding = .5,
  force = 1,
  min.segment.length = 0.1) +
 labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
    color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the most part, both t-SNE and UMAP place songs from the same artists or similar songs close to each other. The UMAP embeddings with Euclidean distance are somehow similar to a real map. In the UMAP representation of the songs, we can see isolated clusters of songs. However, in t-SNE representation, no clear and separate cluster of points can be seen.
We can observe that the most influential hyper-parameter seems to be distance metric. Additionally, when we decrease the value of &lt;code&gt;min_dist&lt;/code&gt;, the projection becomes less compact and the global structure emerges. However, we also see that sometimes music genres are not well-separated as we would like them to be. We should take into account that audio features might not be enough to distinguish between genres of music and We need to incorporate other aspects of songs such as lyrics to be able to differentiate between genres. For instance, Kaylin Pavlik in her blogpost explained how she based on similar audio features trained several machine learning models to classify songs into 6 main categories (EDM, Latin, Pop, R&amp;amp;B, Rap, &amp;amp; Rock). Her best model achieved an accuracy of 54.3%, which is a decent performance but not super accurate. I also tuned and trained a few machine learning models on this dataset but I couldn’t achieve higher performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Supervised UMAP&lt;/h2&gt;
&lt;p&gt;UMAP is an unsupervised dimensionality reduction algorithm but we can also feed target labels to UMAP and make it a &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/supervised.html&#34;&gt;supervised algorithm&lt;/a&gt; by specifying the target variable. To make this happen in UWOT, we can just give the target column (playlist_genre) as an input to &lt;code&gt;y&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;supervised_umap_embedding_df &amp;lt;- 
  spotify_songs %&amp;gt;% 
  select(-c(12:22)) %&amp;gt;% 
  bind_cols(supervised_umap_embedding %&amp;gt;% as_tibble()) %&amp;gt;% 
  dplyr::rename(umap_1 = V1, umap_2 = V2) %&amp;gt;% 
  mutate(
    selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), &amp;quot;&amp;quot;),
    point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),
    track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),
    genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),
    popular_tracks_selected_artist = if_else(
      track_artist %in% selected_artists &amp;amp; track_popularity &amp;gt; 70,shorter_names, NULL )) %&amp;gt;%
  distinct(track_name, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;supervised_umap_embedding_df %&amp;gt;%
  ggplot(aes(x = umap_1, y = umap_2 ,color = playlist_genre )) +
  geom_point(size = 5.3,alpha =0.8 ) +
  gghighlight(selected_artist != &amp;quot;&amp;quot;,unhighlighted_params = list(alpha = 0.3,size = 0.8, color = &amp;#39;#FFE66D&amp;#39;)) +
  scale_color_tableau() +
  guides(size = FALSE,
    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +
    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = &amp;#39;Montserrat&amp;#39;,
    point.padding = 2.2,
    box.padding = .5,
    force = 1,
    min.segment.length = 0.1) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot; ,
       color = &amp;#39;&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-what-makes-a-song-popular/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;3360&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is no surprise that the results of the supervised UMAP are much better separated than the unsupervised one. We just gave additional information to UMAP to transform input data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Fun with Maps</title>
      <link>/post/2020-01-24-some-fun-with-maps/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-01-24-some-fun-with-maps/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Optimal Rule Lists</title>
      <link>/post/2020-01-18-optimal-rule-lists/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-01-18-optimal-rule-lists/</guid>
      <description>


&lt;p&gt;Axiom
There is an inverse relationship between
model accuracy and model interpretability.&lt;/p&gt;
&lt;p&gt;This post is heavily inspired by the Decision Rules chapter from the Interpretable Machine Learning book by Christoph Molnar.&lt;/p&gt;
&lt;p&gt;Some machine learning researchers argue that we should pay more attention to interpretable machine learning instead of trying to design methods to explain black box models.
While reading almost any paper the field of explainable machine learning, you will notice that in that every paper almost always starts by arguing that there is a trade-off between accuracy and interpretability. It means that a more interpretable is less accurate and vice versa and for this reason we need to use more complex and black box models and then design methods to peek into them. However, Cynthia Rudin argues that actually there is no trade-off between these two concepts. On the contrary, interpretability can even help us increase the accuracy of a model becuase with an interpretable algorithm we better understand how the predictive performance of a model can be improved.&lt;/p&gt;
&lt;p&gt;Cynthia Rudin encourages machine learning practitioners and researchers to rather than trying to make black-box algorithms more build and use accurate interpretable machine learning models .
There are already a number of interpretable machine learning algorithms in the literature.
Decision trees and linear models are the two most popular classes of interpretable algorithms. Rule learning algorithms also belong to the class of interpretable algorithms. The aim of these algorithms is to learn decision rules from input data.&lt;/p&gt;
&lt;p&gt;Decision rules are expressed as IF-THEN statement.&lt;/p&gt;
&lt;p&gt;If the condition of the IF part holds true, we will make the prediction based on output of the THEN part.&lt;/p&gt;
&lt;p&gt;Decision rules are considered to be probably the most human-understandable prediction model.&lt;/p&gt;
&lt;p&gt;In many ways, decision rules resemble decision trees. In fact, we can write down a decision tree as a set of decision rules.&lt;/p&gt;
&lt;p&gt;Decision trees are highly scalable and powerful algorithms. But a decision tree is a greedy algorithm. For instance, the split at each node in a decision tree is determined by a greedy process. It means that the decision tree does not find an optimal solution and therefore, the optimal rule lists.&lt;/p&gt;
&lt;div id=&#34;how-to-bin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to bin&lt;/h1&gt;
&lt;p&gt;optbin
santokura&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(OneR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;OneR&amp;#39; was built under R version 3.6.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_binned &amp;lt;- optbin(iris)
iris_binned&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Sepal.Length Sepal.Width Petal.Length    Petal.Width    Species
## 1     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 2     (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 3     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 4     (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 5     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 6     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 7     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 8     (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 9     (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 10    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 11    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 12    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 13    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 14    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 15   (5.41,6.25]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 16   (5.41,6.25]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 17    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 18    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 19   (5.41,6.25]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 20    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 21    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 22    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 23    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 24    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 25    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 26    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 27    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 28    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 29    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 30    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 31    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 32    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 33    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 34   (5.41,6.25]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 35    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 36    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 37   (5.41,6.25]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 38    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 39    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 40    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 41    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 42    (4.3,5.41]    (2,2.87] (0.994,2.46] (0.0976,0.791]     setosa
## 43    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 44    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 45    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 46    (4.3,5.41] (2.87,3.19] (0.994,2.46] (0.0976,0.791]     setosa
## 47    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 48    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 49    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 50    (4.3,5.41]  (3.19,4.4] (0.994,2.46] (0.0976,0.791]     setosa
## 51    (6.25,7.9]  (3.19,4.4]  (2.46,4.86]   (0.791,1.63] versicolor
## 52    (6.25,7.9]  (3.19,4.4]  (2.46,4.86]   (0.791,1.63] versicolor
## 53    (6.25,7.9] (2.87,3.19]  (4.86,6.91]   (0.791,1.63] versicolor
## 54   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 55    (6.25,7.9]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 56   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 57    (6.25,7.9]  (3.19,4.4]  (2.46,4.86]   (0.791,1.63] versicolor
## 58    (4.3,5.41]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 59    (6.25,7.9] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 60    (4.3,5.41]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 61    (4.3,5.41]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 62   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 63   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 64   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 65   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 66    (6.25,7.9] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 67   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 68   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 69   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 70   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 71   (5.41,6.25]  (3.19,4.4]  (2.46,4.86]     (1.63,2.5] versicolor
## 72   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 73    (6.25,7.9]    (2,2.87]  (4.86,6.91]   (0.791,1.63] versicolor
## 74   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 75    (6.25,7.9] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 76    (6.25,7.9] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 77    (6.25,7.9]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 78    (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5] versicolor
## 79   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 80   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 81   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 82   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 83   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 84   (5.41,6.25]    (2,2.87]  (4.86,6.91]   (0.791,1.63] versicolor
## 85    (4.3,5.41] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 86   (5.41,6.25]  (3.19,4.4]  (2.46,4.86]   (0.791,1.63] versicolor
## 87    (6.25,7.9] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 88    (6.25,7.9]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 89   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 90   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 91   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 92   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 93   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 94    (4.3,5.41]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 95   (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 96   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 97   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 98   (5.41,6.25] (2.87,3.19]  (2.46,4.86]   (0.791,1.63] versicolor
## 99    (4.3,5.41]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 100  (5.41,6.25]    (2,2.87]  (2.46,4.86]   (0.791,1.63] versicolor
## 101   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 102  (5.41,6.25]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 103   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 104   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 105   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 106   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 107   (4.3,5.41]    (2,2.87]  (2.46,4.86]     (1.63,2.5]  virginica
## 108   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 109   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 110   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 111   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 112   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 113   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 114  (5.41,6.25]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 115  (5.41,6.25]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 116   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 117   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 118   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 119   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 120  (5.41,6.25]    (2,2.87]  (4.86,6.91]   (0.791,1.63]  virginica
## 121   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 122  (5.41,6.25]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 123   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 124   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 125   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 126   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 127  (5.41,6.25]    (2,2.87]  (2.46,4.86]     (1.63,2.5]  virginica
## 128  (5.41,6.25] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 129   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 130   (6.25,7.9] (2.87,3.19]  (4.86,6.91]   (0.791,1.63]  virginica
## 131   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 132   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 133   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 134   (6.25,7.9]    (2,2.87]  (4.86,6.91]   (0.791,1.63]  virginica
## 135  (5.41,6.25]    (2,2.87]  (4.86,6.91]   (0.791,1.63]  virginica
## 136   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 137   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 138   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 139  (5.41,6.25] (2.87,3.19]  (2.46,4.86]     (1.63,2.5]  virginica
## 140   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 141   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 142   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 143  (5.41,6.25]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 144   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 145   (6.25,7.9]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 146   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 147   (6.25,7.9]    (2,2.87]  (4.86,6.91]     (1.63,2.5]  virginica
## 148   (6.25,7.9] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica
## 149  (5.41,6.25]  (3.19,4.4]  (4.86,6.91]     (1.63,2.5]  virginica
## 150  (5.41,6.25] (2.87,3.19]  (4.86,6.91]     (1.63,2.5]  virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the XAI point of view, we are interested in measuring two metrics for rule lists:&lt;/p&gt;
&lt;p&gt;Accuracy&lt;/p&gt;
&lt;p&gt;Parsimony: Shorter rules are more preferable&lt;/p&gt;
&lt;div id=&#34;corels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CORELS&lt;/h2&gt;
&lt;p&gt;Finding an optimal DT (or a set of rule lists) is an NP-hard problem. The CORELS algorithms developed by aims to find the optimal set of rules. To achieve this goal, CORLES uses pre-mined frequent patterns and optimization techniques.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disadvantaged of rule lists&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rule learning algorithms by design can be only trained on datasets with a discrete target variable. It means that they are only capable of dealing with classification problem and not regression. We can tackle this issue by discretizing the continuous target variable in regression problems. However, doing that results in information loss. Moreover, the input features to a rule learning algorithm must be categorical. Again we can solve this problem by binning continouose features but the same information loss will persist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further Readings and Resources&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;Refrences&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiring.github.io/machine_learning/2017/04/23/one_r&#34; class=&#34;uri&#34;&gt;https://shiring.github.io/machine_learning/2017/04/23/one_r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>EuADS Summer School 2019, Explanations in Philosophy and Psychology Talk by Christos Bechlivanidis</title>
      <link>/post/2020-01-05-euads-summer-school-explanations-in-philosophy-and-psychology/</link>
      <pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-01-05-euads-summer-school-explanations-in-philosophy-and-psychology/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;https://mcnakhaee.com/post/2019-12-31-explainable-data-science-summer-school/&#34;&gt;my last post&lt;/a&gt;, I shared my notes from two talks at Explainable Data Science summer school in Luxembourg.  Although every talk in the summer school was interesting  and taught me new things but I particularly liked the &amp;ldquo;Explanations in Philosophy and Psychology&amp;rdquo; talk by Christos Bechlivanidis. I learned a lot of new things from this this talk  specially because what I had focused by them was mainly about the more algorithmic aspect of explainability. In this post I am going to share my notes from this talk. The slides for this talk can be downloaded from this &lt;a href=&#34;https://euads.org/wp-content/uploads/2019/09/Explanations-in-Philosophy-and-Psychology-2.pdf&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;producers-and-consumers-of-explanations-in-ai&#34;&gt;Producers and consumers of explanations in AI&lt;/h3&gt;
&lt;p&gt;The developer who produces the explanation and evaluates it or as Miller et al (2017), the inmates are running the asylum phenomena. But what makes the good explanation for the developer is not necessarily good for other users of the system. The developer has a deeper understanding of the system. He/she might be cursed by his/her knowledge. In addition, However, his/her understanding, perspective, or goals may be different from the end-user. When producing explanations we need to carefully assess the complexity of the explanation and the knowledge and beliefs of the audience. Fred is a simple person and does not know anything about how neural networks work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-is-an-explanation&#34;&gt;What is an explanation?&lt;/h3&gt;
&lt;p&gt;Explanations are always expressed in  a contrastive manner and this contrast is usually implied by the context. Also, an explanations is not a description.&lt;/p&gt;
&lt;p&gt;Nevertheless, answering &amp;ldquo;what is an explanation? &amp;quot; depends on who we ask.&lt;/p&gt;
&lt;p&gt;Philosopher care about the &lt;strong&gt;normative&lt;/strong&gt; side of an explanation and consider a (good) explanations to be a scientific explanation. But psychologists are interested in the &lt;strong&gt;descriptive&lt;/strong&gt; side or what people consider as an explanation such as everyday explanations and what makes a good explanation.&lt;/p&gt;
&lt;p&gt;Different philosophers and scientists have proposed different definition for an explanation:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aristotle&lt;/strong&gt;: Citing the &lt;em&gt;function&lt;/em&gt;, &lt;em&gt;the material&lt;/em&gt;, &lt;em&gt;the category&lt;/em&gt; or the (efficient) cause of X.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hempel&lt;/strong&gt;:  Producing an (logical) argument whose conclusion is the explanandum X. 
&lt;strong&gt;Salmon&lt;/strong&gt;: Stating everything that affects the probability of X. In other words, If P(Β|Α) ≠ P(Β) then A is explanatory relevant to B (e.g. P(pregnant | male &amp;amp; contraceptives) = P(pregnant | male)). One downside of this definition is that A does not need to have a high probability to be explanatory. For example,if P(Β|Α) - P(Β) = 0.000001, A is still explanatory  relevant to B.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kitcher&lt;/strong&gt;: Showing how X fits a more general state of affairs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Salmon:&lt;/strong&gt; Stating the causal history of X. An explanation of X will trace the causal processes and interactions that lead to X. But, in general, not all causal events in the past of X are explanatory relevant to X. The causal model  presented by Salmon has limitation in dealing with certain type of causation such as Double Prevention. Take the following image where a pink plane wants to drop a bomb on a city. A red plane with an alligator has a mission to shoot the pink plane and prevent the bombing. Also, A blue plane is there to prevent the red plane from shooting the pink plane. Now if the the  blue plane successfully shoot the red plane and the pink plane successfully drops its bomb on the city, does the blue plane cause the bombardment of the city.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;types-of-explanations&#34;&gt;&lt;strong&gt;TYPES OF EXPLANATIONS&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-are-we-interested-in-explanations&#34;&gt;Why are we interested in explanations?&lt;/h3&gt;
&lt;p&gt;We seek explanations because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;To prepare ourselves for similar events in the future :&lt;/strong&gt; Why is the phone turned off? because it has low battery.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Just to explain, understand and assign responsibilities or blames in one-off events&lt;/strong&gt;: why did the assassination of Duke Ferdinand lead to WWI?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To rationalize actions that we take:&lt;/strong&gt; Why didn&amp;rsquo;t you vote?  Because it does not make any difference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To find meaning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To become satisfied from explanations&lt;/strong&gt;. The explanations are like orgasms.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, the explanations may not fulfill their    function or objective.&lt;/p&gt;
&lt;h4 id=&#34;a-preference-for-teleology&#34;&gt;&lt;strong&gt;A PREFERENCE FOR TELEOLOGY&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;However, some people and children are often biased toward teleological explanations and they are looking for the purpose not the cause in their explanations:  Mountains were created to be climbed.&lt;/p&gt;
&lt;h3 id=&#34;explanations-virtues&#34;&gt;Explanations virtues&lt;/h3&gt;
&lt;p&gt;The following properties have been proposed as criteria of a good explanation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Circularity&lt;/strong&gt;: &amp;ldquo;This diet pill works because it helps people lose weight&amp;rdquo; and &amp;ldquo;People lose weight because they use  this diet pill&amp;rdquo; are circular arguments. Although we can detect circularity from childhood, it is not always easy to identify it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Coherence&lt;/strong&gt; : Different elements of explanation must have internal consistency. It means that an explanation must be consistent with prior knowledge and current evidence.&lt;/p&gt;
&lt;p&gt;However, explaining the full set of elements (relations) may not be simple. For instance, to fully explain how a bicycle works we need to say how its various mechanical elements interact and constrain each other.&lt;/p&gt;
&lt;p&gt;Explanations are incomplete (even in science) because our mental representations are skeletal and incomplete. We tend to overestimate the depth of our own
understanding. But the moment we start writing down our understanding, it becomes clear that our understanding is (mostly) shallow and incomplete.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tiger example&lt;/strong&gt;: Tigers have dark vertical stripes on their bodies but  can we tell without looking at an image of a tiger whether these stripes are vertical or horizontal on the tiger&amp;rsquo;s tail and legs?!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bicycle example&lt;/strong&gt;: How much do you know how a bicycle work?  Draw one!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;But sometimes compression is also needed.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We rely heavily in expertise of other minds. In an experiment carried out by Zemla et al. (2017), it was observed that compared to other measures such as complexity, articulation, coherence, generality and truth, one of the most important measures of explanation quality for the participants was “perceived expertise”.  Perceived expertise indicates whether participants believed the explanation was written by an expert. The more we we trust in the expertise of the explainer, the more likely we accept the explanation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relevance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From philosophical point of view,  only factors that make a difference to the explanandum or have a causal role should be included to generate a good explanation.  This level of details is ideal but not attainable and we usually find a trade-off by abstraction. Moreover, hyper-concrete explanations are too true  to be good (e.g. extremely detailed maps).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Strevens (2007) proposed that in order to create an optimal explanation first we need to include every imaginable event and then we remove and abstract every event that makes no difference to the occurrence of the explanandum.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But is it true for non-experts? Philosophical view is different from non-expert view. Weisberg et al (2008) showed that adding irrelevant neuroscientific information (e.g. jargons) to an explanation increased its perceived quality by non-experts (naïve adults and neuroscience students).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another experiment  was carried out in which participant were asked to rate 3 types of explanations:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;8.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The average ratings for the abstract explanation was significantly lower than the irrelevant explanation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the other hand, the causal ratings for the abstract explanation was higher that the other two types of explanations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Match the epistemic status of the audience&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When producing explanations we need to carefully asses the knowledge and beliefs of the audience.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Everyone agrees that explanations should be simple. But what do we mean by simplicity? (number of entities,number of entity types, shortest description? )&lt;/li&gt;
&lt;li&gt;Paul Thagard (1989): simplicity is determined by the number of special required assumptions.  People prefer these kinds of simpler explanations because fewer assumptions means fewer unexplained causes .&lt;/li&gt;
&lt;li&gt;However, when probabilities of assumptions are also included in the explanation, people choose the most probable explanation not the simplest. In case of equal probabilities, simpler explanations are preferred.&lt;/li&gt;
&lt;li&gt;Zemla et al (2017) it showed that  the quality of explanations was positively correlated both with the number of unexplained causes and its length (level of detail)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;12.png&#34; alt=&#34;s&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generality (Breadth– Scope - Coverage)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is it better to explain more things but less precisely or fewer things but more precisely?&lt;/li&gt;
&lt;li&gt;Thagard (1992) argues that an explanations that explains more pieces of evidence should be favored.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;evaluating-explanations&#34;&gt;EVALUATING EXPLANATIONS&lt;/h3&gt;
&lt;p&gt;To evaluate our explanations, we need to ask a public audience. This audience can be our friends, family members and colleagues who may not have any knowledge of the system. However, we should take into account that the these evaluations can be biased and noisy. For this reason, we need to take a larger sample of people.&lt;/p&gt;
&lt;p&gt;Alternatively, we can also collaborate with HCI experts,  psychologist and behavioral scientist to evaluate explanations.&lt;/p&gt;
&lt;p&gt;Different groups of users see different explanation and same group of users see different explanation. We need to compare different versions of our explanations (like A/B testing) .&lt;/p&gt;
&lt;p&gt;We can also utilize online crowdsourcing tools and run our analysis through them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;•Amazon Mechanical Turk – mturk.co&lt;/li&gt;
&lt;li&gt;Prolific Academic - prolific.c&lt;/li&gt;
&lt;li&gt;Gorilla - gorilla.sc&lt;/li&gt;
&lt;li&gt;Testable - testable.org&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;what-do-we-need-to-ask&#34;&gt;&lt;strong&gt;What do we need to ask?&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Do they think that the provided explanation is a good explanation?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How well do they understand this explanation?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Behavioral measures such as what did they expect from the explanation?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;13.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EuADS Summer School 2019: SUBJECTIVITY and VISUALIZATION</title>
      <link>/post/2020-01-04-euads-summer-school-2019-subjectivity-and-visualization/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-01-04-euads-summer-school-2019-subjectivity-and-visualization/</guid>
      <description>


&lt;p&gt;This talk was mainly focused on the importance of visualization and other techniques for exploring and explaining data. In other words, one approach to explainability is to first explore the input data and find interesting patterns. This can be done via:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Association analysis ()&lt;/li&gt;
&lt;li&gt;Dimensionality reduction techniques&lt;/li&gt;
&lt;li&gt;Graph embeddings&lt;/li&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Community detection&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However interesting pattern is a subjective term and there are a huge number of “interesting measures” out there. What you find interesting as a data mining researcher may be different from another researcher. For example, in the case of community detection what we can define as an interesting community can vary. An interesting community might be a densely connected set of nodes or a set of nodes that have few outside neighbors.&lt;/p&gt;
&lt;p&gt;Why interestingness measures are subjective? Because as a user we compare patterns with our prior beliefs or expectations and in if a pattern &lt;span style=&#34;color:blue&#34;&gt; contrasts&lt;/span&gt; with them and &lt;span style=&#34;color:blue&#34;&gt; can be described easily&lt;/span&gt;, we consider it as interesting.&lt;/p&gt;
&lt;p&gt;The challenge is to come up with a metric or formalize a metric that measures ’true interestingness&#34; for us.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Do People Share on Telegram?</title>
      <link>/project/2020-01-01-how-do-people-share-on-telegram/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/project/2020-01-01-how-do-people-share-on-telegram/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Explainable Data Science Summer School</title>
      <link>/post/2019-12-31-explainable-data-science-summer-school/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-12-31-explainable-data-science-summer-school/</guid>
      <description>&lt;p&gt;Last September, I had the opportunity to participate in the  &lt;strong&gt;EXPLAINABLE DATA SCIENCE&lt;/strong&gt;  summer school in Kirchberg, Luxembourg. the summer school was organized by the European Association for Data Science (&lt;strong&gt;EuADS&lt;/strong&gt;) and was held during 10-13 September.&lt;/p&gt;
&lt;p&gt;What I specifically liked about this summer school ( of course besides enjoying the the beautiful city of Luxembourg ) was the fact that it covered a vast variety of topics in the explainable machine learning (AI) literature, ranging from visualization, XAI techniques, causality to psychological aspects of explainability.  In addition, the summer school has a special guest, the legendary &lt;strong&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/people/cmbishop/&#34;&gt;Christopher Bishop&lt;/a&gt;&lt;/strong&gt; who gave the &lt;strong&gt;inaugural&lt;/strong&gt; lecture.&lt;/p&gt;
&lt;p&gt;You can find the complete program and the presentations in the &lt;a href=&#34;https://euads.org/summer-school-2019/&#34;&gt;EuADS&amp;rsquo;s website&lt;/a&gt;. Nevertheless during some presentations in the summer school, I took notes and I summarized them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes it is not easy to keep up with the speaker and take notes. Also, it is possible that what I wrote down is just my interpretation and not what the speaker intened to say.  For this reason, I do not guarantee that all details in this post are accurate or what the speakers wanted to communicate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;from-data-mining-to-data-science---peter-flach-euads-president&#34;&gt;From Data Mining to Data Science - Peter Flach (EuADS President)&lt;/h2&gt;
&lt;h3 id=&#34;1-what-is-data-science&#34;&gt;1. What is Data Science?**&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Data Science&amp;rdquo; is a vague term. One might mean by &amp;ldquo;data science&amp;rdquo;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is the Science of data. This definition is more frequently used by statistician and machine learning and is more theoretical.&lt;/li&gt;
&lt;li&gt;Doing science with data. This definition is more applied and data intensive.&lt;/li&gt;
&lt;li&gt;Applying science to data. This definition is also heavily applied and data intensive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Data is not the New Oil&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some people are overexcited about having access to huge amount of data as if they have discovered an oil field. Likewise,  they believe that they can simply extract value from  data and this data is a new driver for progress and prosperity. However, even if we &lt;strong&gt;acquire&lt;/strong&gt; data we cannot be certain that it is valuable and we can extract value from it.&lt;/p&gt;
&lt;p&gt;In other words, data in and of itself does not present value:&lt;/p&gt;
&lt;p&gt;data != value but&lt;/p&gt;
&lt;p&gt;But data and knowledge together can result in value. Here knowledge can be an input or an output of the data.&lt;/p&gt;
&lt;p&gt;data + knowledge = value&lt;/p&gt;
&lt;p&gt;Now data science can defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_1.PNG&#34; alt=&#34;1570983727831&#34;&gt;&lt;/p&gt;
&lt;p&gt;It means that Data Science has three main ingredients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Data &lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Knowledge&lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Value &lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The kinds of value that Data Science can generate are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;scientific knowledge and models&lt;/li&gt;
&lt;li&gt;societal value&lt;/li&gt;
&lt;li&gt;economic value&lt;/li&gt;
&lt;li&gt;personal value&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-from-data-mining-to-data-science&#34;&gt;2. From Data Mining to Data Science&lt;/h3&gt;
&lt;p&gt;Many consider data mining to be the father of data science. Others  say that data mining is a subset of data science. While the interest for data mining is declining,  data science gain more popularity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_2.PNG&#34; alt=&#34;test&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2: Data science is getting more popular than data mining&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_3.PNG&#34; alt=&#34;test&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 3: CRISP data mining process&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data mining, we (implicitly) assume that there is some value in the and  our aim is to use data mining techniques to &lt;strong&gt;uncover&lt;/strong&gt; it.  We can see data mining just like the extraction of a valuable metals from an existing mine.&lt;/p&gt;
&lt;p&gt;However, in data science, we first need to make sure that data has some value. In other words, data science can be seen as prospective, which means we are searching for a mine to extract metal material from it. That puts more emphasis on the exploratory aspect (nature) of data science, which includes the following activities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_4.PNG&#34; alt=&#34;1571180675528&#34;&gt;&lt;/p&gt;
&lt;p&gt;These activities do not exist in the data mining space and distinguish data science and data mining.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_5.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Science Trajectory (DST) space&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Data mining is a more sequential and more prescriptive approach where every operation must be implemented in a specific order. All activities in data mining can be a part of a data science project but not the opposite. For instance, not every data science project &lt;em&gt;requires&lt;/em&gt; a modeling phase. On the other hand, the goal of data science  for a specific application can be just data collection or data publication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_6.PNG&#34; alt=&#34;image-20200104194252866&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_7.PNG&#34; alt=&#34;image-20200104194422595&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read more about this in the following paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories&lt;/strong&gt;. Fernando  Martinez-Plumed, Lidia Contreras-Ochando, Cesar Ferri, Jose Hernandez-Orallo, Meelis Kull, NicolasLachiche, Maria Jose Ramirez-Quintana and Peter Flach. (Under review, 2019)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;3-responsible-data-science---the-human-factor-35&#34;&gt;3. Responsible Data Science - The Human Factor (3/5)&lt;/h3&gt;
&lt;p&gt;Data Science is for, about, by and with humans and human factors should be taken into consideration at every stage of a data science project. But it is not always easy to measure, define and ultimately achieve them.&lt;/p&gt;
&lt;p&gt;For example, look at following table which shows the number and the percentage of students who applied and were admitted to a university.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_8.PNG&#34; alt=&#34;1571181577381&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the first glance, this table might suggest a case of  bias toward women in the admission process. However, further examinations show that the low percentage of total admissions for women is due to the fact that female applicants tended to apply to more difficult programs  with an overall lower chance of acceptance while men applied to easier programs with a higher probability of acceptance. In other words, the difficulty of programs was a confounding factor that influenced the outcome not gender bias. It indicates measuring a human factor  such as fairness is not easy because measuring bias is not easy. Furthermore, according to Goodhart&amp;rsquo;s Law, the moment we decide to use these  metrics (e.g. bias) as our target to optimize, they are not good measures anymore.&lt;/p&gt;
&lt;p&gt;in the the rest of talk, Peter Flach discussed the relationship between GDPR and fairness and specifically he touched upon an important issue regarding data ownership and the role of GDPR for personal data protection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_9.PNG&#34; alt=&#34;1571585896393&#34;&gt;&lt;/p&gt;
&lt;p&gt;He provided an example of authorship to demonstrate that solving data ownership is not a simple task. If someone writes a book about someone else (e.g. Clinton), the author has the ownership and the copyright not the the person whom the book is about.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;slideshttpseuadsorgwp-contentuploads201909from-data-mining-processes-to-data-science-trajectories-2pdf&#34;&gt;&lt;a href=&#34;https://euads.org/wp-content/uploads/2019/09/From-Data-Mining-Processes-to-Data-Science-Trajectories-2.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/h5&gt;
&lt;hr&gt;
&lt;h2 id=&#34;model-based-machine-learning&#34;&gt;Model-Based Machine Learning&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This talk was dedicated to Sabine Krolak-Schwerdt who unfortunately passed away recently and was one of the founders of EuADS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;three-factors-have-contributed-to-the-popularity-and-the-recent-success-of-ai&#34;&gt;Three factors have contributed to the popularity and the recent success of AI&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;More computing power&lt;/li&gt;
&lt;li&gt;Large amount of available data (Big data)&lt;/li&gt;
&lt;li&gt;More powerful algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dozens of machine learning algorithms have been developed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_1.PNG&#34; alt=&#34;image-20200104221438797&#34;&gt;&lt;/p&gt;
&lt;p&gt;But the &amp;lsquo;No Free Lunch Theorem&amp;rsquo;  states that no universal machine learning can solve every problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Averaged over all possible data distributions, every classification algorithm has the same error rate when classifying previously unobserved points.
D. Wolpert (1996)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means that the goal of machine learning is to find an algorithm that is well-suited to the problem that being solved.&lt;/p&gt;
&lt;h4 id=&#34;model-based-machine-learning-1&#34;&gt;Model-Based machine learning&lt;/h4&gt;
&lt;p&gt;In the traditional machine learning paradigm, ML algorithms play a centric role. We start by an ML algorithm and we would like to know how we can apply it to our problem.&lt;/p&gt;
&lt;p&gt;However, in model-based machine learning paradigm, we are looking to find a well-matched algorithm for our problem. We can derive a model that best represents our problem by making explicit modeling assumptions.&lt;/p&gt;
&lt;h4 id=&#34;data-and-prior-knowledge&#34;&gt;Data and prior knowledge&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Scenario 1:&lt;/strong&gt; we have collected a handful of voltage and current measurement from an experiment. and we want to determine the relationship between the current and voltage using these measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scenario 2:&lt;/strong&gt;  We have a huge database containing images from 1000 objects and our goal is to develop a model to classify each image correctly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_3.PNG&#34; alt=&#34;1571589636558&#34;&gt;&lt;/p&gt;
&lt;p&gt;But are these datasets &amp;lsquo;big&amp;rsquo; enough for solving their corresponding problems. In the first scenario, although we only have a few  measurements, we know that they are enough for finding the relationship between voltage and current.  On the other hand, even though we have access to a large number of images for each class, these images do not represent the distribution of all images.&lt;/p&gt;
&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;MML_2.PNG&#34; alt=&#34;1571589623467&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The trade-off between prior knowledge and the amount of data needed&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Therefore, we must distinguish between two types of &amp;lsquo;big data&amp;rsquo;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In terms of size&lt;/li&gt;
&lt;li&gt;In terms of being statistically significant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, Chris Bishop argued that we need to incorporate uncertainties into our machine learning models otherwise the consequences would be dire. It means that we should &lt;em&gt;never ever&lt;/em&gt; build direct classifier but we should build probabilistic classifier.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_4.PNG&#34; alt=&#34;1571590428222&#34;&gt;&lt;/p&gt;
&lt;p&gt;Why is that? Because not all misclassification errors are equal and different costs are assigned to different errors. Misclassifying  a patient with cancer may be much worse than misclassifying a healthy patient. So, instead of minimizing the number of misclassified instances, we can minimize the expected (average costs).&lt;/p&gt;
&lt;p&gt;Finally, Chris Bishop presented a demo of a movie recommendation system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://euads.org/wp-content/uploads/2019/09/Chris-Bishop-SabineK-Lecture-2019_2.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;model-based-machine-learninghttpwwwmbmlbookcom&#34;&gt;&lt;a href=&#34;http://www.mbmlbook.com&#34;&gt;Model-Based Machine Learning&lt;/a&gt;&lt;/h5&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Tuesday Submissions</title>
      <link>/project/2019-12-29-tidy-tuesday-submissions/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/2019-12-29-tidy-tuesday-submissions/</guid>
      <description>


&lt;p&gt;For 6 years I had been using python exclusively as the main tool for carrying out my data science tasks and running my experiments. Recently, I have started using Tidyverse packages and tools in R for my data science activities. I am completely facinated by how these tools make it easy for me to perform analysis and create nice visualization. Since then I have tried to participate in the weekly Tidy Tuesday project.
You can find my submissions in this page.&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2020&lt;/h2&gt;
&lt;div id=&#34;week-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 3&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;password_quality.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2019&lt;/h2&gt;
&lt;div id=&#34;week-52---christmas-songs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 52 - Christmas Songs&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;img src=&#34;00001d.png&#34; /&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;week-51--adoptable-dogs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Week 51 -Adoptable dogs&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;tags_Akbar.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ccc.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;week-46---code-in-cran-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Week 46 - Code in CRAN Packages&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;cran_pkg.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;week-36---moores-law&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Week 36 - Moore’s Law&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;CPU.jpg&#34; /&gt;
&lt;img src=&#34;CPU-Plot.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;week-35---simpsons-guest-stars&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Week 35 - Simpsons Guest Stars&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Contrastive Explanations</title>
      <link>/post/2019-11-07-contrastive-explanations/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-11-07-contrastive-explanations/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Data Shapely</title>
      <link>/post/data-shapely/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/data-shapely/</guid>
      <description>


&lt;div id=&#34;what-is-your-data-worth-equitable-data-valuation-in-machine-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is Your Data Worth? Equitable Data Valuation in Machine Learning&lt;/h1&gt;
&lt;p&gt;This paper barely mentions the term explainability although it is closely related to this domain. Actually the motivation behind this paper is not that we want to know the impact individual points on the performance of machine learning models and detect potential bias present in the dataset. On the other hand,the motivat&lt;/p&gt;
&lt;p&gt;The basic idea behind this paper is that we want to measure and quantify the value of individual data points.&lt;/p&gt;
&lt;p&gt;One motivation behind that is&lt;/p&gt;
&lt;p&gt;data value must be computed with respect to three ingridients:&lt;/p&gt;
&lt;p&gt;A fixed training dataset: If we use a different subset of data for training, it will be likely that the results of our machine learning model would change.&lt;/p&gt;
&lt;p&gt;the machine learning model:&lt;/p&gt;
&lt;p&gt;the performance metric: imagin we would like to classify benign and … tumors in a highly imbalanced dataset. If we use accuracy as the performance metric the impact that each data point have on this metric is small. However, if we use a different metric such as percision, the impact of individual points from minority class which were classified incorrectly as … on this metric will be significant.&lt;/p&gt;
&lt;div id=&#34;influence-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;influence functions&lt;/h3&gt;
&lt;p&gt;While influence function do not take into account&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We want to measure the (equitable) value of data (samples) in terms of their contribution to the model training, prediction and decision making.&lt;/p&gt;
&lt;p&gt;{{&amp;lt; figure library=“true” src=“3ingridients.jpg” title=“A caption” lightbox=“true” &amp;gt;}}&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/post/3ingridients.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Supervised ML models consist of three ingredients:&lt;/p&gt;
&lt;p&gt;· Training data&lt;/p&gt;
&lt;p&gt;· Learning model&lt;/p&gt;
&lt;p&gt;· Performance metrics&lt;/p&gt;
&lt;p&gt;Therefore, quantifying Data value should reflect all of these ingredients because in some algorithms the value of a data point might change based on the algorithm or the metric that we use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Research Questions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In this paper the following two research questions were addressed;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;what is an equitable measure of value of a data point to the machine learning model with respect to the performance metric&lt;/li&gt;
&lt;li&gt;How to measure these data values efficiently?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Similar approaches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;· Leave one out (LOO)&lt;/p&gt;
&lt;p&gt;· Influence functions&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;idea&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;They propose Data Shapely as a metric to measure the value (contribution) of individual data points to an algorithm’s performance.&lt;/p&gt;
&lt;p&gt;Note that here data valuation is only defined and measurable in terms of supervised machine learning models.&lt;/p&gt;
&lt;div id=&#34;computing-data-shapely&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Computing Data Shapely&lt;/h5&gt;
&lt;p&gt;Equitable data valuation has three main properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Null element&lt;/strong&gt;: – If adding a sample to any subset of training data never changes the classifier performance, the value of the sample is 0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Symmetry:&lt;/strong&gt; – If adding i and j to any subset of training data always gives the same performance, then value of data i and j are the same&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Decomposability&lt;/strong&gt; – the overall performance score is the sum of individual performance scores&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on the condition above Data Shapely are computed:&lt;/p&gt;
&lt;p&gt;{to do : add formula 1}&lt;/p&gt;
&lt;p&gt;However, computing this formula is computationally expensive as each time that we add a data point to a subset we have to train the machine learning model.&lt;/p&gt;
&lt;p&gt;Therefore, the authors tried to approximate this value by some smart techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/h4&gt;
&lt;div id=&#34;datasets&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;UK Biobank data set (Tabular)&lt;/li&gt;
&lt;li&gt;HAM10000 dataset (image)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;·Removing data points with the &lt;strong&gt;lowest&lt;/strong&gt; Shapely valued improves the model performance&lt;/p&gt;
&lt;p&gt;· Removing data points with the &lt;strong&gt;highest&lt;/strong&gt; Shapely values decreases the model performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Main Applications:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better allocate our resources for collecting data points that are similar to data points with high Shapely values.&lt;/li&gt;
&lt;li&gt;Measure the value of data samples that we already have&lt;/li&gt;
&lt;li&gt;Can be used as a diagnosis tool to identify mislabeled data points.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No condition on it optimality&lt;/li&gt;
&lt;li&gt;It was not compared with other powerful techniques such as influence functions.&lt;/li&gt;
&lt;li&gt;Motivation behind the paper is problematic (Data = new oil)&lt;/li&gt;
&lt;li&gt;Measuring individual data values in the context of legal domains is different from machine learning domains so they will not achieve the goal that they set in the motivation with this technique&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Data value is computed based on all ingredients of machine learning pipeline.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Computing Shapely values is not limited by the type of data, model and performance metrics.
## Resources
Official Repository for the paper:
&lt;a href=&#34;https://github.com/amiratag/DataShapley&#34; class=&#34;uri&#34;&gt;https://github.com/amiratag/DataShapley&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=79pRqMq_-LE&#34;&gt;A talk by one of the authors&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.acolyer.org/2019/07/15/data-shapley/&#34;&gt;A blog post explaining and summarizing the paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://lineardigressions.com/episodes/2018/5/6/game-theory-for-model-interpretability-shapley-values&#34;&gt;A podcast episode about Data Shapely&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GLM in Farsi</title>
      <link>/post/2019-10-03-glm-in-farsi/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-03-glm-in-farsi/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Pandas</title>
      <link>/post/pandas/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/pandas/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;یک راه دیگر ساخت دیتافریم ها، استفاده از دیکشنری های است. در این حالت نام ستون های دیتافریم به صورت key های دیکشنری و مقادیر این ستون ها به صورت یک لیست به عنوان value های این دیکشنری قرار داده می شوند.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;sample_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; { &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;index&amp;#39;&lt;/span&gt;:range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;),
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column 1&amp;#39;&lt;/span&gt;: np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;),
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column 2&amp;#39;&lt;/span&gt;: np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(sample_dict)
df
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rename({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column 1&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rand_noraml_col&amp;#39;&lt;/span&gt;,
          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column 2&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rand_integer_col&amp;#39;&lt;/span&gt;},
         axis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;columns&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;index&amp;#39;&lt;/span&gt;,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rand_noraml_col&amp;#39;&lt;/span&gt;,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rand_integer_col&amp;#39;&lt;/span&gt;]
df
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;str&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;_col&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;)
df
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/house_price.csv&amp;#39;&lt;/span&gt;)
data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;برعکس کردن ترتیب سطرها&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index(drop &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;برعکس-کردن-ترتیب-ستون-ها&#34;&gt;برعکس کردن ترتیب ستون ها&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[:,::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 1460 entries, 0 to 1459
Data columns (total 81 columns):
Id               1460 non-null int64
MSSubClass       1460 non-null int64
MSZoning         1460 non-null object
LotFrontage      1201 non-null float64
LotArea          1460 non-null int64
Street           1460 non-null object
Alley            91 non-null object
LotShape         1460 non-null object
LandContour      1460 non-null object
Utilities        1460 non-null object
LotConfig        1460 non-null object
LandSlope        1460 non-null object
Neighborhood     1460 non-null object
Condition1       1460 non-null object
Condition2       1460 non-null object
BldgType         1460 non-null object
HouseStyle       1460 non-null object
OverallQual      1460 non-null int64
OverallCond      1460 non-null int64
YearBuilt        1460 non-null int64
YearRemodAdd     1460 non-null int64
RoofStyle        1460 non-null object
RoofMatl         1460 non-null object
Exterior1st      1460 non-null object
Exterior2nd      1460 non-null object
MasVnrType       1452 non-null object
MasVnrArea       1452 non-null float64
ExterQual        1460 non-null object
ExterCond        1460 non-null object
Foundation       1460 non-null object
BsmtQual         1423 non-null object
BsmtCond         1423 non-null object
BsmtExposure     1422 non-null object
BsmtFinType1     1423 non-null object
BsmtFinSF1       1460 non-null int64
BsmtFinType2     1422 non-null object
BsmtFinSF2       1460 non-null int64
BsmtUnfSF        1460 non-null int64
TotalBsmtSF      1460 non-null int64
Heating          1460 non-null object
HeatingQC        1460 non-null object
CentralAir       1460 non-null object
Electrical       1459 non-null object
1stFlrSF         1460 non-null int64
2ndFlrSF         1460 non-null int64
LowQualFinSF     1460 non-null int64
GrLivArea        1460 non-null int64
BsmtFullBath     1460 non-null int64
BsmtHalfBath     1460 non-null int64
FullBath         1460 non-null int64
HalfBath         1460 non-null int64
BedroomAbvGr     1460 non-null int64
KitchenAbvGr     1460 non-null int64
KitchenQual      1460 non-null object
TotRmsAbvGrd     1460 non-null int64
Functional       1460 non-null object
Fireplaces       1460 non-null int64
FireplaceQu      770 non-null object
GarageType       1379 non-null object
GarageYrBlt      1379 non-null float64
GarageFinish     1379 non-null object
GarageCars       1460 non-null int64
GarageArea       1460 non-null int64
GarageQual       1379 non-null object
GarageCond       1379 non-null object
PavedDrive       1460 non-null object
WoodDeckSF       1460 non-null int64
OpenPorchSF      1460 non-null int64
EnclosedPorch    1460 non-null int64
3SsnPorch        1460 non-null int64
ScreenPorch      1460 non-null int64
PoolArea         1460 non-null int64
PoolQC           7 non-null object
Fence            281 non-null object
MiscFeature      54 non-null object
MiscVal          1460 non-null int64
MoSold           1460 non-null int64
YrSold           1460 non-null int64
SaleType         1460 non-null object
SaleCondition    1460 non-null object
SalePrice        1460 non-null int64
dtypes: float64(3), int64(35), object(43)
memory usage: 924.0+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;انتخاب ستون ها بر اساس تایپ ستون&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select_dtypes(include &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;int64&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select_dtypes(include &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;number&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select_dtypes(include &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;int64&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;object&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select_dtypes(exclude&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;int64&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;تغییر-تایپ-ستون-ها&#34;&gt;تغییر تایپ ستون ها&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/titanic.csv&amp;#39;&lt;/span&gt;)
titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 1309 entries, 0 to 1308
Data columns (total 14 columns):
pclass       1309 non-null int64
survived     1309 non-null int64
name         1309 non-null object
sex          1309 non-null object
age          1309 non-null object
sibsp        1309 non-null int64
parch        1309 non-null int64
ticket       1309 non-null object
fare         1309 non-null object
cabin        1309 non-null object
embarked     1309 non-null object
boat         1309 non-null object
body         1309 non-null object
home.dest    1309 non-null object
dtypes: int64(4), object(10)
memory usage: 143.2+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value_counts(titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;age)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;?     263
24     47
22     43
21     41
30     40
Name: age, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;نمونه-برداری-تصادفی-از-دیتافریم&#34;&gt;نمونه برداری تصادفی از دیتافریم&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(1460, 81)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sample(frac&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1111&lt;/span&gt;)
train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(1168, 81)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop(train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index)
test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(292, 81)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;فیلتر-کردن-بر-اساس-چند-معیار&#34;&gt;فیلتر کردن بر اساس چند معیار&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic[(titanic[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pclass&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;
       (titanic[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sex&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;
       (titanic[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;survived&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;فیلتر-کردن-بر-اساس-فراوان-ترین-دسته&#34;&gt;فیلتر کردن بر اساس فراوان ترین دسته&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;ubar &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/ubar.csv&amp;#39;&lt;/span&gt;)
ubar&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;source_counts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ubar[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SourceState&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value_counts()
source_counts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;تهران                  5343
اصفهان                 4665
فارس                   4244
خراسان رضوی            3913
آذربایجان شرقی         2995
مرکزی                  2400
مازندران               1919
یزد                    1879
خوزستان                1779
البرز                  1698
قزوین                  1539
آذربایجان غربی         1463
سمنان                  1442
گیلان                  1405
همدان                  1296
کرمان                  1272
قم                     1224
گلستان                 1131
هرمزگان                1056
کرمانشاه               1037
زنجان                   923
لرستان                  843
چهارمحال و بختیاری      709
بوشهر                   693
سیستان و بلوچستان       622
کردستان                 603
خراسان جنوبی            599
اردبیل                  571
خراسان شمالی            416
ایلام                   218
کهگیلویه و بویراحمد     103
Name: SourceState, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;source_counts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nlargest(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;تهران     5343
اصفهان    4665
فارس      4244
Name: SourceState, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;source_counts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nlargest(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Index([&#39;تهران&#39;, &#39;اصفهان&#39;, &#39;فارس&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;ubar[ubar[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SourceState&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isin(source_counts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nlargest(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index)]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;مقادیر-گمشده&#34;&gt;مقادیر گمشده&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/titanic.csv&amp;#39;&lt;/span&gt;,na_values &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;?&amp;#39;&lt;/span&gt;)
titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isna()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pclass          0
survived        0
name            0
sex             0
age           263
sibsp           0
parch           0
ticket          0
fare            1
cabin        1014
embarked        2
boat          823
body         1188
home.dest     564
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;titanic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isna()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pclass       0.000000
survived     0.000000
name         0.000000
sex          0.000000
age          0.200917
sibsp        0.000000
parch        0.000000
ticket       0.000000
fare         0.000764
cabin        0.774637
embarked     0.001528
boat         0.628724
body         0.907563
home.dest    0.430863
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(    )      
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&amp;lt;ipython-input-2-495416214f55&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 pd.read_csv()


TypeError: parser_f() missing 1 required positional argument: &#39;filepath_or_buffer&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;porridge &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; porridge &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;
One 
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;strong&gt;Two&lt;/strong&gt; 
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
Three 
&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;{{% speaker_note %}}
&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Press &lt;span style=&#34;color:#e6db74&#34;&gt;`S`&lt;/span&gt; key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;#34;/img/boards.jpg&amp;#34; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;#34;#0000FF&amp;#34; &amp;gt;}}
{{&amp;lt; slide class=&amp;#34;my-style&amp;#34; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h3&lt;/span&gt; {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;color&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;navy&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Recent Applications of Machine Learning in Rail Track Maintenance A Survey</title>
      <link>/publication/ict-open/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/publication/ict-open/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Recent Applications of Machine Learning in Rail Track Maintenance A Survey</title>
      <link>/publication/rssrail/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/publication/rssrail/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Landing Page</title>
      <link>/contact/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/otherwidget/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/otherwidget/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/otherwidget/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/otherwidget/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FFORT A benchmark set for fault tree analysis</title>
      <link>/publication/ffort/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/ffort/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
