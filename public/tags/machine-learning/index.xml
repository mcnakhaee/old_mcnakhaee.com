<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Muhammad Chenariyan Nakhaee</title>
    <link>/tags/machine-learning/</link>
      <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 31 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpg</url>
      <title>Machine Learning</title>
      <link>/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Explainable Data Science Summer School</title>
      <link>/post/2019-12-31-explainable-data-science-summer-school/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-12-31-explainable-data-science-summer-school/</guid>
      <description>&lt;p&gt;Last September, I had the opportunity to participate in the  &lt;strong&gt;EXPLAINABLE DATA SCIENCE&lt;/strong&gt;  summer school in Kirchberg, Luxembourg. the summer school was organized by the European Association for Data Science (&lt;strong&gt;EuADS&lt;/strong&gt;) and was held during 10-13 September.&lt;/p&gt;
&lt;p&gt;What I specifically liked about this summer school ( of course besides enjoying the the beautiful city of Luxembourg ) was the fact that it covered a vast variety of topics in the explainable machine learning (AI) literature, ranging from visualization, XAI techniques, causality to psychological aspects of explainability.  In addition, the summer school has a special guest, the legendary &lt;strong&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/people/cmbishop/&#34;&gt;Christopher Bishop&lt;/a&gt;&lt;/strong&gt; who gave the &lt;strong&gt;inaugural&lt;/strong&gt; lecture.&lt;/p&gt;
&lt;p&gt;You can find the complete program and the presentations in the &lt;a href=&#34;https://euads.org/summer-school-2019/&#34;&gt;EuADS&#39;s website&lt;/a&gt;. Nevertheless during some presentations in the summer school, I took notes and I summarized them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes it is not easy to keep up with the speaker and take notes. Also, it is possible that what I wrote down is just my interpretation and not what the speaker intened to say.  For this reason, I do not guarantee that all details in this post are accurate or what the speakers wanted to communicate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;from-data-mining-to-data-science---peter-flach-euads-president&#34;&gt;From Data Mining to Data Science - Peter Flach (EuADS President)&lt;/h2&gt;
&lt;h3 id=&#34;1-what-is-data-science&#34;&gt;1. What is Data Science?**&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Data Science&amp;rdquo; is a vague term. One might mean by &amp;ldquo;data science&amp;rdquo;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is the Science of data. This definition is more frequently used by statistician and machine learning and is more theoretical.&lt;/li&gt;
&lt;li&gt;Doing science with data. This definition is more applied and data intensive.&lt;/li&gt;
&lt;li&gt;Applying science to data. This definition is also heavily applied and data intensive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Data is not the New Oil&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some people are overexcited about having access to huge amount of data as if they have discovered an oil field. Likewise,  they believe that they can simply extract value from  data and this data is a new driver for progress and prosperity. However, even if we &lt;strong&gt;acquire&lt;/strong&gt; data we cannot be certain that it is valuable and we can extract value from it.&lt;/p&gt;
&lt;p&gt;In other words, data in and of itself does not present value:&lt;/p&gt;
&lt;p&gt;data != value but&lt;/p&gt;
&lt;p&gt;But data and knowledge together can result in value. Here knowledge can be an input or an output of the data.&lt;/p&gt;
&lt;p&gt;data + knowledge = value&lt;/p&gt;
&lt;p&gt;Now data science can defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_1.PNG&#34; alt=&#34;1570983727831&#34;&gt;&lt;/p&gt;
&lt;p&gt;It means that Data Science has three main ingredients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Data &lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Knowledge&lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;!-- raw HTML omitted --&gt; Value &lt;!-- raw HTML omitted --&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The kinds of value that Data Science can generate are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;scientific knowledge and models&lt;/li&gt;
&lt;li&gt;societal value&lt;/li&gt;
&lt;li&gt;economic value&lt;/li&gt;
&lt;li&gt;personal value&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-from-data-mining-to-data-science&#34;&gt;2. From Data Mining to Data Science&lt;/h3&gt;
&lt;p&gt;Many consider data mining to be the father of data science. Others  say that data mining is a subset of data science. While the interest for data mining is declining,  data science gain more popularity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_2.PNG&#34; alt=&#34;test&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2: Data science is getting more popular than data mining&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_3.PNG&#34; alt=&#34;test&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 3: CRISP data mining process&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data mining, we (implicitly) assume that there is some value in the and  our aim is to use data mining techniques to &lt;strong&gt;uncover&lt;/strong&gt; it.  We can see data mining just like the extraction of a valuable metals from an existing mine.&lt;/p&gt;
&lt;p&gt;However, in data science, we first need to make sure that data has some value. In other words, data science can be seen as prospective, which means we are searching for a mine to extract metal material from it. That puts more emphasis on the exploratory aspect (nature) of data science, which includes the following activities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_4.PNG&#34; alt=&#34;1571180675528&#34;&gt;&lt;/p&gt;
&lt;p&gt;These activities do not exist in the data mining space and distinguish data science and data mining.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_5.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Science Trajectory (DST) space&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Data mining is a more sequential and more prescriptive approach where every operation must be implemented in a specific order. All activities in data mining can be a part of a data science project but not the opposite. For instance, not every data science project &lt;em&gt;requires&lt;/em&gt; a modeling phase. On the other hand, the goal of data science  for a specific application can be just data collection or data publication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_6.PNG&#34; alt=&#34;image-20200104194252866&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_7.PNG&#34; alt=&#34;image-20200104194422595&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read more about this in the following paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories&lt;/strong&gt;. Fernando  Martinez-Plumed, Lidia Contreras-Ochando, Cesar Ferri, Jose Hernandez-Orallo, Meelis Kull, NicolasLachiche, Maria Jose Ramirez-Quintana and Peter Flach. (Under review, 2019)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;3-responsible-data-science---the-human-factor-35&#34;&gt;3. Responsible Data Science - The Human Factor (3/5)&lt;/h3&gt;
&lt;p&gt;Data Science is for, about, by and with humans and human factors should be taken into consideration at every stage of a data science project. But it is not always easy to measure, define and ultimately achieve them.&lt;/p&gt;
&lt;p&gt;For example, look at following table which shows the number and the percentage of students who applied and were admitted to a university.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_8.PNG&#34; alt=&#34;1571181577381&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the first glance, this table might suggest a case of  bias toward women in the admission process. However, further examinations show that the low percentage of total admissions for women is due to the fact that female applicants tended to apply to more difficult programs  with an overall lower chance of acceptance while men applied to easier programs with a higher probability of acceptance. In other words, the difficulty of programs was a confounding factor that influenced the outcome not gender bias. It indicates measuring a human factor  such as fairness is not easy because measuring bias is not easy. Furthermore, according to Goodhart&#39;s Law, the moment we decide to use these  metrics (e.g. bias) as our target to optimize, they are not good measures anymore.&lt;/p&gt;
&lt;p&gt;in the the rest of talk, Peter Flach discussed the relationship between GDPR and fairness and specifically he touched upon an important issue regarding data ownership and the role of GDPR for personal data protection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ds_9.PNG&#34; alt=&#34;1571585896393&#34;&gt;&lt;/p&gt;
&lt;p&gt;He provided an example of authorship to demonstrate that solving data ownership is not a simple task. If someone writes a book about someone else (e.g. Clinton), the author has the ownership and the copyright not the the person whom the book is about.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;slideshttpseuadsorgwp-contentuploads201909from-data-mining-processes-to-data-science-trajectories-2pdf&#34;&gt;&lt;a href=&#34;https://euads.org/wp-content/uploads/2019/09/From-Data-Mining-Processes-to-Data-Science-Trajectories-2.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/h5&gt;
&lt;hr&gt;
&lt;h2 id=&#34;model-based-machine-learning&#34;&gt;Model-Based Machine Learning&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This talk was dedicated to Sabine Krolak-Schwerdt who unfortunately passed away recently and was one of the founders of EuADS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;three-factors-have-contributed-to-the-popularity-and-the-recent-success-of-ai&#34;&gt;Three factors have contributed to the popularity and the recent success of AI&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;More computing power&lt;/li&gt;
&lt;li&gt;Large amount of available data (Big data)&lt;/li&gt;
&lt;li&gt;More powerful algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dozens of machine learning algorithms have been developed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_1.PNG&#34; alt=&#34;image-20200104221438797&#34;&gt;&lt;/p&gt;
&lt;p&gt;But the &amp;lsquo;No Free Lunch Theorem&amp;rsquo;  states that no universal machine learning can solve every problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Averaged over all possible data distributions, every classification algorithm has the same error rate when classifying previously unobserved points.
D. Wolpert (1996)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means that the goal of machine learning is to find an algorithm that is well-suited to the problem that being solved.&lt;/p&gt;
&lt;h4 id=&#34;model-based-machine-learning-1&#34;&gt;Model-Based machine learning&lt;/h4&gt;
&lt;p&gt;In the traditional machine learning paradigm, ML algorithms play a centric role. We start by an ML algorithm and we would like to know how we can apply it to our problem.&lt;/p&gt;
&lt;p&gt;However, in model-based machine learning paradigm, we are looking to find a well-matched algorithm for our problem. We can derive a model that best represents our problem by making explicit modeling assumptions.&lt;/p&gt;
&lt;h4 id=&#34;data-and-prior-knowledge&#34;&gt;Data and prior knowledge&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Scenario 1:&lt;/strong&gt; we have collected a handful of voltage and current measurement from an experiment. and we want to determine the relationship between the current and voltage using these measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scenario 2:&lt;/strong&gt;  We have a huge database containing images from 1000 objects and our goal is to develop a model to classify each image correctly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_3.PNG&#34; alt=&#34;1571589636558&#34;&gt;&lt;/p&gt;
&lt;p&gt;But are these datasets &amp;lsquo;big&amp;rsquo; enough for solving their corresponding problems. In the first scenario, although we only have a few  measurements, we know that they are enough for finding the relationship between voltage and current.  On the other hand, even though we have access to a large number of images for each class, these images do not represent the distribution of all images.&lt;/p&gt;
&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;MML_2.PNG&#34; alt=&#34;1571589623467&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The trade-off between prior knowledge and the amount of data needed&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Therefore, we must distinguish between two types of &amp;lsquo;big data&amp;rsquo;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In terms of size&lt;/li&gt;
&lt;li&gt;In terms of being statistically significant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, Chris Bishop argued that we need to incorporate uncertainties into our machine learning models otherwise the consequences would be dire. It means that we should &lt;em&gt;never ever&lt;/em&gt; build direct classifier but we should build probabilistic classifier.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MML_4.PNG&#34; alt=&#34;1571590428222&#34;&gt;&lt;/p&gt;
&lt;p&gt;Why is that? Because not all misclassification errors are equal and different costs are assigned to different errors. Misclassifying  a patient with cancer may be much worse than misclassifying a healthy patient. So, instead of minimizing the number of misclassified instances, we can minimize the expected (average costs).&lt;/p&gt;
&lt;p&gt;Finally, Chris Bishop presented a demo of a movie recommendation system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://euads.org/wp-content/uploads/2019/09/Chris-Bishop-SabineK-Lecture-2019_2.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;model-based-machine-learninghttpwwwmbmlbookcom&#34;&gt;&lt;a href=&#34;http://www.mbmlbook.com&#34;&gt;Model-Based Machine Learning&lt;/a&gt;&lt;/h5&gt;
</description>
    </item>
    
    <item>
      <title>Contrastive Explanations</title>
      <link>/post/2019-11-07-contrastive-explanations/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-11-07-contrastive-explanations/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)
options(jupyter.display = &amp;#39;rich&amp;#39;)
library(tidyverse)
conda_list()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           name
## 1  r-miniconda
## 2 r-reticulate
## 3   Anaconda37
## 4 r-tensorflow
##                                                                              python
## 1                     C:\\Users\\iMuhammad\\AppData\\Local\\r-miniconda\\python.exe
## 2 C:\\Users\\iMuhammad\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\python.exe
## 3                                           D:\\ProgramData\\Anaconda37\\python.exe
## 4                       D:\\ProgramData\\Anaconda37\\envs\\r-tensorflow\\python.exe&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;py_config()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## python:         D:/ProgramData/Anaconda37/python.exe
## libpython:      D:/ProgramData/Anaconda37/python37.dll
## pythonhome:     D:/ProgramData/Anaconda37
## version:        3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]
## Architecture:   64bit
## numpy:          D:/ProgramData/Anaconda37/Lib/site-packages/numpy
## numpy_version:  1.16.2
## 
## NOTE: Python version was forced by RETICULATE_PYTHON&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId Survived Pclass
## 1           1        0      3
## 2           2        1      1
## 3           3        1      3
## 4           4        1      1
## 5           5        0      3
## 6           6        0      3
##                                                  Name    Sex Age SibSp Parch
## 1                             Braund, Mr. Owen Harris   male  22     1     0
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0
## 3                              Heikkinen, Miss. Laina female  26     0     0
## 4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0
## 5                            Allen, Mr. William Henry   male  35     0     0
## 6                                    Moran, Mr. James   male NaN     0     0
##             Ticket    Fare Cabin Embarked
## 1        A/5 21171  7.2500   NaN        S
## 2         PC 17599 71.2833   C85        C
## 3 STON/O2. 3101282  7.9250   NaN        S
## 4           113803 53.1000  C123        S
## 5           373450  8.0500   NaN        S
## 6           330877  8.4583   NaN        Q&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId Survived Pclass Age SibSp Parch    Fare Embarked_C Embarked_Q
## 0           1        0      3  22     1     0  7.2500          0          0
## 1           2        1      1  38     1     0 71.2833          1          0
## 2           3        1      3  26     0     0  7.9250          0          0
## 3           4        1      1  35     1     0 53.1000          0          0
## 4           5        0      3  35     0     0  8.0500          0          0
## 6           7        0      1  54     0     0 51.8625          0          0
##   Embarked_S Sex_binary
## 0          1          0
## 1          0          1
## 2          1          1
## 3          1          1
## 4          1          0
## 6          1          0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
X = titanic_dummified.drop([&amp;#39;Survived&amp;#39;],axis=1)
y = titanic_dummified[&amp;#39;Survived&amp;#39;]
train, test, labels_train, labels_test = train_test_split(X, y, train_size=0.80)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;rf = RandomForestClassifier(n_estimators=100)
rf.fit(train,labels_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&amp;#39;gini&amp;#39;,
##                        max_depth=None, max_features=&amp;#39;auto&amp;#39;, max_leaf_nodes=None,
##                        min_impurity_decrease=0.0, min_impurity_split=None,
##                        min_samples_leaf=1, min_samples_split=2,
##                        min_weight_fraction_leaf=0.0, n_estimators=100,
##                        n_jobs=None, oob_score=False, random_state=None,
##                        verbose=0, warm_start=False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;rf.score(test,labels_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.8181818181818182&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prd_ds = test.copy()
prd_ds[&amp;#39;actual&amp;#39;]  = labels_test
prd_ds[&amp;#39;prds&amp;#39;] = rf.predict(test)

prd_ds[&amp;#39;wrong&amp;#39;]  =  prd_ds[&amp;#39;prds&amp;#39;] == prd_ds[&amp;#39;actual&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;py$prd_ds %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     PassengerId Pclass Age SibSp Parch     Fare Embarked_C Embarked_Q
## 537         538      1  30     0     0 106.4250          1          0
## 382         383      3  32     0     0   7.9250          0          0
## 345         346      2  24     0     0  13.0000          0          0
## 660         661      1  50     2     0 133.6500          0          0
## 205         206      3   2     0     1  10.4625          0          0
## 603         604      3  44     0     0   8.0500          0          0
##     Embarked_S Sex_binary actual prds wrong
## 537          0          1      1    1  TRUE
## 382          1          0      0    1 FALSE
## 345          1          1      1    1  TRUE
## 660          1          0      1    1  TRUE
## 205          1          1      0    1 FALSE
## 603          1          0      0    0  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import lime
import lime.lime_tabular
import dtreeviz
import shap&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;explainer = lime.lime_tabular.LimeTabularExplainer(
train.to_numpy(),
feature_names = list(train.columns),
class_names = [0,1],
discretize_continuous = True
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;instance = test.iloc[1,:]
exp = explainer.explain_instance(instance,
rf.predict_proba,
num_features = 5,
top_labels = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
from IPython.display import display, HTML
exp.show_in_notebook(show_table = True,show_all = True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;s = exp.as_html()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;rvest&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: xml2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;rvest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     pluck&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     guess_encoding&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#htmltools::&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The Recent Applications of Machine Learning in Rail Track Maintenance A Survey</title>
      <link>/publication/ict-open/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/publication/ict-open/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Recent Applications of Machine Learning in Rail Track Maintenance A Survey</title>
      <link>/publication/rssrail/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/publication/rssrail/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
